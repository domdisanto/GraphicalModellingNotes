
@article{meinshausen_high-dimensional_2006,
	title = {High-dimensional graphs and variable selection with the {Lasso}},
	volume = {34},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-34/issue-3/High-dimensional-graphs-and-variable-selection-with-the-Lasso/10.1214/009053606000000281.full},
	doi = {10.1214/009053606000000281},
	abstract = {The pattern of zero entries in the inverse covariance matrix of a multivariate normal distribution corresponds to conditional independence restrictions between variables. Covariance selection aims at estimating those structural zeros from data. We show that neighborhood selection with the Lasso is a computationally attractive alternative to standard covariance selection for sparse high-dimensional graphs. Neighborhood selection estimates the conditional independence restrictions separately for each node in the graph and is hence equivalent to variable selection for Gaussian linear models. We show that the proposed neighborhood selection scheme is consistent for sparse high-dimensional graphs. Consistency hinges on the choice of the penalty parameter. The oracle value for optimal prediction does not lead to a consistent neighborhood estimate. Controlling instead the probability of falsely joining some distinct connectivity components of the graph, consistent estimation for sparse graphs is achieved (with exponential rates), even when the number of variables grows as the number of observations raised to an arbitrary power.},
	number = {3},
	urldate = {2023-11-07},
	journal = {The Annals of Statistics},
	author = {Meinshausen, Nicolai and Bühlmann, Peter},
	month = jun,
	year = {2006},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62J07, 62F12, 62H20, covariance selection, Gaussian graphical models, Linear regression, penalized regression},
	pages = {1436--1462},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/TAFCKSCW/Meinshausen and Bühlmann - 2006 - High-dimensional graphs and variable selection wit.pdf:application/pdf},
}

@misc{augugliaro_cglasso_2023,
	title = {cglasso: {Conditional} {Graphical} {LASSO} for {Gaussian} {Graphical} {Models} with {Censored} and {Missing} {Values}},
	copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL (≥ 2)]},
	shorttitle = {cglasso},
	url = {https://cran.r-project.org/web/packages/cglasso/index.html},
	abstract = {Conditional graphical lasso estimator is an extension of the graphical lasso proposed to estimate the conditional dependence structure of a set of p response variables given q predictors. This package provides suitable extensions developed to study datasets with censored and/or missing values. Standard conditional graphical lasso is available as a special case. Furthermore, the package provides an integrated set of core routines for visualization, analysis, and simulation of datasets with censored and/or missing values drawn from a Gaussian graphical model. Details about the implemented models can be found in Augugliaro et al. (2023) {\textless}doi:10.18637/jss.v105.i01{\textgreater}, Augugliaro et al. (2020b) {\textless}doi:10.1007/s11222-020-09945-7{\textgreater}, Augugliaro et al. (2020a) {\textless}doi:10.1093/biostatistics/kxy043{\textgreater}, Yin et al. (2001) {\textless}doi:10.1214/11-AOAS494{\textgreater} and Stadler et al. (2012) {\textless}doi:10.1007/s11222-010-9219-7{\textgreater}.},
	urldate = {2023-11-07},
	author = {Augugliaro, Luigi and Sottile, Gianluca and Wit, Ernst C. and Vinciotti, Veronica},
	month = jan,
	year = {2023},
	keywords = {MissingData},
}

@misc{friedman_glasso_2019,
	title = {glasso: {Graphical} {Lasso}: {Estimation} of {Gaussian} {Graphical} {Models}},
	copyright = {GPL-2},
	shorttitle = {glasso},
	url = {https://cran.r-project.org/web/packages/glasso/index.html},
	abstract = {Estimation of a sparse inverse covariance matrix using a lasso (L1) penalty. Facilities are provided for estimates along a path of values for the regularization parameter.},
	urldate = {2023-11-07},
	author = {Friedman, Jerome and Tibshirani, Trevor Hastie {and} Rob},
	month = oct,
	year = {2019},
	keywords = {Psychometrics},
}

@article{stadler_missing_2010,
	title = {Missing values: sparse inverse covariance estimation and an extension to sparse regression},
	volume = {22},
	issn = {0960-3174, 1573-1375},
	shorttitle = {Missing values},
	url = {http://link.springer.com/10.1007/s11222-010-9219-7},
	doi = {10.1007/s11222-010-9219-7},
	abstract = {We propose an 1-regularized likelihood method for estimating the inverse covariance matrix in the highdimensional multivariate normal model in presence of missing data. Our method is based on the assumption that the data are missing at random (MAR) which entails also the completely missing at random case. The implementation of the method is non-trivial as the observed negative loglikelihood generally is a complicated and non-convex function. We propose an efﬁcient EM algorithm for optimization with provable numerical convergence properties. Furthermore, we extend the methodology to handle missing values in a sparse regression context. We demonstrate both methods on simulated and real data.},
	language = {en},
	number = {1},
	urldate = {2023-11-07},
	journal = {Statistics and Computing},
	author = {Städler, Nicolas and Bühlmann, Peter},
	year = {2010},
	pages = {219--235},
	file = {Städler and Bühlmann - 2012 - Missing values sparse inverse covariance estimati.pdf:/Users/jdomi/Zotero/storage/AYSY9F23/Städler and Bühlmann - 2012 - Missing values sparse inverse covariance estimati.pdf:application/pdf},
}

@misc{zheng_gi-joe_2023,
	title = {{GI}-{JOE}: {Graph} {Inference} when {Joint} {Observations} are {Erose}},
	shorttitle = {{GI}-{JOE}},
	url = {https://github.com/Lili-Zheng-stat/GI-JOE},
	urldate = {2023-11-07},
	author = {Zheng, Lili},
	month = mar,
	year = {2023},
}

@misc{zheng_graphical_2023,
	title = {Graphical {Model} {Inference} with {Erosely} {Measured} {Data}},
	url = {http://arxiv.org/abs/2210.11625},
	doi = {10.48550/arXiv.2210.11625},
	abstract = {In this paper, we investigate the Gaussian graphical model inference problem in a novel setting that we call erose measurements, referring to irregularly measured or observed data. For graphs, this results in different node pairs having vastly different sample sizes which frequently arises in data integration, genomics, neuroscience, and sensor networks. Existing works characterize the graph selection performance using the minimum pairwise sample size, which provides little insights for erosely measured data, and no existing inference method is applicable. We aim to fill in this gap by proposing the first inference method that characterizes the different uncertainty levels over the graph caused by the erose measurements, named GI-JOE (Graph Inference when Joint Observations are Erose). Specifically, we develop an edge-wise inference method and an affiliated FDR control procedure, where the variance of each edge depends on the sample sizes associated with corresponding neighbors. We prove statistical validity under erose measurements, thanks to careful localized edge-wise analysis and disentangling the dependencies across the graph. Finally, through simulation studies and a real neuroscience data example, we demonstrate the advantages of our inference methods for graph selection from erosely measured data.},
	urldate = {2023-11-07},
	publisher = {arXiv},
	author = {Zheng, Lili and Allen, Genevera I.},
	month = may,
	year = {2023},
	note = {arXiv:2210.11625 [math, stat]},
	keywords = {Statistics - Methodology, Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/Users/jdomi/Zotero/storage/8AYTETIS/Zheng and Allen - 2023 - Graphical Model Inference with Erosely Measured Da.pdf:application/pdf;arXiv.org Snapshot:/Users/jdomi/Zotero/storage/HDVEVXQ8/2210.html:text/html},
}

@article{friedman_sparse_2008,
	title = {Sparse inverse covariance estimation with the graphical lasso},
	volume = {9},
	issn = {1465-4644},
	url = {https://doi.org/10.1093/biostatistics/kxm045},
	doi = {10.1093/biostatistics/kxm045},
	abstract = {We consider the problem of estimating sparse graphs by a lasso penalty applied to the inverse covariance matrix. Using a coordinate descent procedure for the lasso, we develop a simple algorithm—the graphical lasso—that is remarkably fast: It solves a 1000-node problem (∼500000 parameters) in at most a minute and is 30–4000 times faster than competing methods. It also provides a conceptual link between the exact problem and the approximation suggested by Meinshausen and Bühlmann (2006). We illustrate the method on some cell-signaling data from proteomics.},
	number = {3},
	urldate = {2023-11-07},
	journal = {Biostatistics},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	month = jul,
	year = {2008},
	pages = {432--441},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/S3B6QTSV/Friedman et al. - 2008 - Sparse inverse covariance estimation with the grap.pdf:application/pdf;Snapshot:/Users/jdomi/Zotero/storage/C82PGNK5/224260.html:text/html},
}

@article{meinshausen_note_2008,
	title = {A note on the {Lasso} for {Gaussian} graphical model selection},
	volume = {78},
	issn = {0167-7152},
	url = {https://www.sciencedirect.com/science/article/pii/S0167715207002945},
	doi = {10.1016/j.spl.2007.09.014},
	abstract = {Inspired by the success of the Lasso for regression analysis, it seems attractive to estimate the graph of a multivariate normal distribution by ℓ1-norm penalized likelihood maximization. We examine some properties of the estimator and show that care has to be taken with interpretation of results as the estimator is not consistent for some graphs.},
	number = {7},
	urldate = {2023-11-07},
	journal = {Statistics \& Probability Letters},
	author = {Meinshausen, Nicolai},
	month = may,
	year = {2008},
	pages = {880--884},
	file = {ScienceDirect Snapshot:/Users/jdomi/Zotero/storage/L3DSSSJL/S0167715207002945.html:text/html},
}

@article{park_estimating_2021,
	title = {Estimating high-dimensional covariance and precision matrices under general missing dependence},
	volume = {15},
	issn = {1935-7524, 1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-15/issue-2/Estimating-high-dimensional-covariance-and-precision-matrices-under-general-missing/10.1214/21-EJS1892.full},
	doi = {10.1214/21-EJS1892},
	abstract = {A sample covariance matrix S of completely observed data is the key statistic in a large variety of multivariate statistical procedures, such as structured covariance/precision matrix estimation, principal component analysis, and testing of equality of mean vectors. However, when the data are partially observed, the sample covariance matrix from the available data is biased and does not provide valid multivariate procedures. To correct the bias, a simple adjustment method called inverse probability weighting (IPW) has been used in previous research, yielding the IPW estimator. The estimator can play the role of S in the missing data context, thus replacing S in off-the-shelf multivariate procedures such as the graphical lasso algorithm. However, theoretical properties (e.g. concentration) of the IPW estimator have been only established in earlier work under very simple missing structures; every variable of each sample is independently subject to missingness with equal probability. We investigate the deviation of the IPW estimator when observations are partially observed under general missing dependency. We prove the optimal convergence rate Op( logp∕n) of the IPW estimator based on the element-wise maximum norm, even when two unrealistic assumptions (known mean and/or missing probabilities) frequently assumed to be known in the past work are relaxed. The optimal rate is especially crucial in estimating a precision matrix, because of the “meta-theorem” [26] that claims the rate of the IPW estimator governs that of the resulting precision matrix estimator. In the simulation study, we discuss one of practically important issues, non-positive semi-definiteness of the IPW estimator, and compare the estimator with imputation methods.},
	number = {2},
	urldate = {2023-11-07},
	journal = {Electronic Journal of Statistics},
	author = {Park, Seongoh and Wang, Xinlei and Lim, Johan},
	month = jan,
	year = {2021},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	keywords = {60E15, 62H12, convergence rate, Covariance matrix, dependent missing structure, element-wise maximum norm, inverse probability weighting},
	pages = {4868--4915},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/MFKXKUJH/Park et al. - 2021 - Estimating high-dimensional covariance and precisi.pdf:application/pdf},
}

@article{kolar_estimating_2012,
	title = {Estimating {Sparse} {Precision} {Matrices} from {Data} with {Missing} {Values}},
	abstract = {We study a simple two step procedure for estimating sparse precision matrices from data with missing values, which is tractable in high-dimensions and does not require imputation of the missing values. We provide rates of convergence for this estimator in the spectral norm, Frobenius norm and element-wise ∞ norm. Simulation studies show that this estimator compares favorably with the EM algorithm. Our results have important practical consequences as they show that standard tools for estimating sparse precision matrices can be used when data contains missing values, without resorting to the iterative EM algorithm that can be slow to converge in practice for large problems.},
	language = {en},
	journal = {Proceedings of the 29 th International Conference on Machine Learning},
	author = {Kolar, Mladen and Xing, Eric P},
	year = {2012},
	file = {Kolar and Xing - Estimating Sparse Precision Matrices from Data wit.pdf:/Users/jdomi/Zotero/storage/883M4WGW/Kolar and Xing - Estimating Sparse Precision Matrices from Data wit.pdf:application/pdf},
}

@article{wainwright_graphical_2007,
	title = {Graphical {Models}, {Exponential} {Families}, and {Variational} {Inference}},
	volume = {1},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com/article/Details/MAL-001},
	doi = {10.1561/2200000001},
	language = {en},
	number = {1–2},
	urldate = {2023-11-24},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Wainwright, Martin J. and Jordan, Michael I.},
	year = {2007},
	pages = {1--305},
	file = {Wainwright and Jordan - 2007 - Graphical Models, Exponential Families, and Variat.pdf:/Users/jdomi/Zotero/storage/3VB5MMVS/Wainwright and Jordan - 2007 - Graphical Models, Exponential Families, and Variat.pdf:application/pdf},
}

@book{imperatorskaia_akademia_nauk_russia_commentarii_1726,
	title = {Commentarii {Academiae} scientiarum imperialis {Petropolitanae}},
	url = {http://archive.org/details/commentariiacade08impe},
	abstract = {Engraved title vignette},
	language = {lat},
	urldate = {2023-11-24},
	publisher = {Petropolis, Typis Academiae},
	author = {{Imperatorskaia akademia nauk (Russia)}},
	collaborator = {{American Museum of Natural History Library}},
	year = {1726},
}

@article{shields_cultural_2012,
	title = {Cultural {Topology}: {The} {Seven} {Bridges} of {Königsburg}, 1736},
	volume = {29},
	issn = {0263-2764},
	shorttitle = {Cultural {Topology}},
	url = {https://doi.org/10.1177/0263276412451161},
	doi = {10.1177/0263276412451161},
	abstract = {In an example of Enlightenment ‘engaged research' and public intellectual practice, Euler established the basis of topology and graph theory through his solution to the puzzle of whether a stroll around the seven bridges of 18th-century Königsberg (Kaliningrad) was possible without having to cross any given bridge twice. This ‘Manifesto' argues that, born in a form of cultural studies, topology offers 21st-century researchers a model for mapping the dynamics of time as well as space, allowing the rigorous description of events, situations, changing cultural formations and social spatializations. Law and Mol's network spaces, Serre's folded time, Massey’s ‘power geometries’, Lefebvre's ‘production of space’ and ‘rhythmanalysis’ can be developed through a cultural topological sensitivity that allows time to be understood as not only progressive but cyclical, relationships and the ‘reach’ of power can be understood through ‘knots', and a topology of experience to model the ‘plushness of the Real' via extra- and over-dimensioned time-spaces that capture nuance while drawing on systematic conceptual resources.},
	language = {en},
	number = {4-5},
	urldate = {2023-11-24},
	journal = {Theory, Culture \& Society},
	author = {Shields, Rob},
	month = jul,
	year = {2012},
	note = {Publisher: SAGE Publications Ltd},
	pages = {43--57},
}

@article{eichler_granger_2007,
	title = {Granger causality and path diagrams for multivariate time series},
	volume = {137},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407606000480},
	doi = {10.1016/j.jeconom.2005.06.032},
	abstract = {In this paper, we introduce path diagrams for multivariate time series which visualize the dynamic relationships among the variables. In these path diagrams, the vertices represent the components of the time series and are connected by arrows or lines according to the nonvanishing parameters in the autoregressive representation of the time series. We show that these path diagrams provide a framework for the analysis of the dependence structure of the time series. In particular, we give sufficient graphical conditions for Granger-noncausality and Granger-noncausality up to a certain horizon.},
	number = {2},
	urldate = {2023-11-24},
	journal = {Journal of Econometrics},
	author = {Eichler, Michael},
	month = apr,
	year = {2007},
	keywords = {Causal inference, Granger causality, Graphical models, Multivariate time series, Spurious causality},
	pages = {334--353},
	file = {ScienceDirect Snapshot:/Users/jdomi/Zotero/storage/GFJ7MYRW/S0304407606000480.html:text/html},
}

@article{dempster_covariance_1972,
	title = {Covariance {Selection}},
	volume = {28},
	issn = {0006-341X},
	url = {https://www.jstor.org/stable/2528966},
	doi = {10.2307/2528966},
	abstract = {The covariance structure of a multivariate normal population can be simplified by setting elements of the inverse of the covariance matrix to zero. Reasons for adopting such a model and a rule for estimating its parameters are given in section 2. It is also proposed to select the zeros in the inverse from sample data. A numerical illustration of the proposed technique is given in section 3. Appendix A sketches the general theory of exponential families which underlies the special results of section 2, and Appendix B describes two approaches to computation of the proposed estimator.},
	number = {1},
	urldate = {2023-11-24},
	journal = {Biometrics},
	author = {Dempster, A. P.},
	year = {1972},
	note = {Publisher: [Wiley, International Biometric Society]},
	pages = {157--175},
}

@article{dong_nonparametric_2022,
	title = {Nonparametric {Neighborhood} {Selection} in {Graphical} {Models}},
	volume = {23},
	abstract = {The neighborhood selection method directly explores the conditional dependence structure and has been widely used to construct undirected graphical models. However, except for some special cases with discrete data, there is little research on nonparametric methods for neighborhood selection with mixed data. This paper develops a fully nonparametric neighborhood selection method under a consolidated smoothing spline ANOVA (SS ANOVA) decomposition framework. The proposed model is ﬂexible and contains many existing models as special cases. The proposed method provides a uniﬁed framework for mixed data without any restrictions on the type of each random variable. We detect edges by applying an L1 regularization to interactions in the SS ANOVA decomposition. We propose an iterative procedure to compute the estimates and establish the convergence rates for conditional density and interactions. Simulations indicate that the proposed methods perform well under Gaussian and non-Gaussian settings. We illustrate the proposed methods using two real data examples.},
	language = {en},
	journal = {Journal of Machine Learning Research},
	author = {Dong, Hao and Wang, Yuedong},
	year = {2022},
	pages = {1--26},
	file = {Dong and Wang - Nonparametric Neighborhood Selection in Graphical .pdf:/Users/jdomi/Zotero/storage/W3W98JKU/Dong and Wang - Nonparametric Neighborhood Selection in Graphical .pdf:application/pdf},
}

@article{chen_selection_2015,
	title = {Selection and estimation for mixed graphical models},
	volume = {102},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/asu051},
	doi = {10.1093/biomet/asu051},
	abstract = {We consider the problem of estimating the parameters in a pairwise graphical model in which the distribution of each node, conditioned on the others, may have a different exponential family form. We identify restrictions on the parameter space required for the existence of a well-defined joint density, and establish the consistency of the neighbourhood selection approach for graph reconstruction in high dimensions when the true underlying graph is sparse. Motivated by our theoretical results, we investigate the selection of edges between nodes whose conditional distributions take different parametric forms, and show that efficiency can be gained if edge estimates obtained from the regressions of particular nodes are used to reconstruct the graph. These results are illustrated with examples of Gaussian, Bernoulli, Poisson and exponential distributions. Our theoretical findings are corroborated by evidence from simulation studies.},
	number = {1},
	urldate = {2023-11-24},
	journal = {Biometrika},
	author = {Chen, Shizhe and Witten, Daniela M. and Shojaie, Ali},
	month = mar,
	year = {2015},
	pages = {47--64},
	file = {Accepted Version:/Users/jdomi/Zotero/storage/FI94WPGL/Chen et al. - 2015 - Selection and estimation for mixed graphical model.pdf:application/pdf;Snapshot:/Users/jdomi/Zotero/storage/AKI6PR9L/228946.html:text/html},
}

@article{banerjee_model_2008,
	title = {Model {Selection} {Through} {Sparse} {Maximum} {Likelihood} {Estimation} for {Multivariate} {Gaussian} or {Binary} {Data}},
	volume = {9},
	abstract = {We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added 1-norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our ﬁrst algorithm uses block coordinate descent, and can be interpreted as recursive 1-norm penalized regression. Our second algorithm, based on Nesterov’s ﬁrst order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright and Jordan, 2006), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data.},
	language = {en},
	journal = {Journal of Machine Learning Research},
	author = {Banerjee, Onureena and Ghaoui, Laurent El},
	year = {2008},
	pages = {485--516},
	file = {Banerjee and Ghaoui - Model Selection Through Sparse Maximum Likelihood .pdf:/Users/jdomi/Zotero/storage/I2X7G553/Banerjee and Ghaoui - Model Selection Through Sparse Maximum Likelihood .pdf:application/pdf},
}

@article{koike_-biased_2020,
	title = {De-biased graphical {Lasso} for high-frequency data},
	volume = {22},
	issn = {1099-4300},
	url = {http://arxiv.org/abs/1905.01494},
	doi = {10.3390/e22040456},
	abstract = {This paper develops a new statistical inference theory for the precision matrix of high-frequency data in a high-dimensional setting. The focus is not only on point estimation but also on interval estimation and hypothesis testing for entries of the precision matrix. To accomplish this purpose, we establish an abstract asymptotic theory for the weighted graphical Lasso and its de-biased version without specifying the form of the initial covariance estimator. We also extend the scope of the theory to the case that a known factor structure is present in the data. The developed theory is applied to the concrete situation where we can use the realized covariance matrix as the initial covariance estimator, and we obtain a feasible asymptotic distribution theory to construct (simultaneous) confidence intervals and (multiple) testing procedures for entries of the precision matrix.},
	number = {4},
	urldate = {2023-11-24},
	journal = {Entropy},
	author = {Koike, Yuta},
	month = apr,
	year = {2020},
	note = {arXiv:1905.01494 [math, stat]},
	keywords = {Mathematics - Statistics Theory, 62J07, 62E20, 60H07},
	pages = {456},
	annote = {Comment: 38 pages},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/PZNH9AZA/1905.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/VZLRRJZG/Koike - 2020 - De-biased graphical Lasso for high-frequency data.pdf:application/pdf},
}

@article{zhou_time_2010,
	title = {Time varying undirected graphs},
	volume = {80},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-010-5180-0},
	doi = {10.1007/s10994-010-5180-0},
	abstract = {Undirected graphs are often used to describe high dimensional distributions. Under sparsity conditions, the graph can be estimated using ℓ1 penalization methods. However, current methods assume that the data are independent and identically distributed. If the distribution, and hence the graph, evolves over time then the data are not longer identically distributed. In this paper we develop a nonparametric method for estimating time varying graphical structure for multivariate Gaussian distributions using an ℓ1 regularization method, and show that, as long as the covariances change smoothly over time, we can estimate the covariance matrix well (in predictive risk) even when p is large.},
	language = {en},
	number = {2},
	urldate = {2023-11-30},
	journal = {Machine Learning},
	author = {Zhou, Shuheng and Lafferty, John and Wasserman, Larry},
	month = sep,
	year = {2010},
	keywords = {Graph selection, High dimensional asymptotics, ℓ 1 regularization, Risk consistency},
	pages = {295--319},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/LLBDRZEK/Zhou et al. - 2010 - Time varying undirected graphs.pdf:application/pdf},
}

@misc{mazumder_graphical_2012,
	title = {The {Graphical} {Lasso}: {New} {Insights} and {Alternatives}},
	shorttitle = {The {Graphical} {Lasso}},
	url = {http://arxiv.org/abs/1111.5479},
	abstract = {The graphical lasso {\textbackslash}citep\{FHT2007a\} is an algorithm for learning the structure in an undirected Gaussian graphical model, using \${\textbackslash}ell\_1\$ regularization to control the number of zeros in the precision matrix \$\{{\textbackslash}B{\textbackslash}Theta\}=\{{\textbackslash}B{\textbackslash}Sigma\}{\textasciicircum}\{-1\}\$ {\textbackslash}citep\{BGA2008,yuan\_lin\_07\}. The \{{\textbackslash}texttt R\} package {\textbackslash}GL{\textbackslash} {\textbackslash}citep\{FHT2007a\} is popular, fast, and allows one to efficiently build a path of models for different values of the tuning parameter. Convergence of {\textbackslash}GL{\textbackslash} can be tricky; the converged precision matrix might not be the inverse of the estimated covariance, and occasionally it fails to converge with warm starts. In this paper we explain this behavior, and propose new algorithms that appear to outperform {\textbackslash}GL. By studying the "normal equations" we see that, {\textbackslash}GL{\textbackslash} is solving the \{{\textbackslash}em dual\} of the graphical lasso penalized likelihood, by block coordinate ascent; a result which can also be found in {\textbackslash}cite\{BGA2008\}. In this dual, the target of estimation is \${\textbackslash}B{\textbackslash}Sigma\$, the covariance matrix, rather than the precision matrix \${\textbackslash}B{\textbackslash}Theta\$. We propose similar primal algorithms {\textbackslash}PGL{\textbackslash} and {\textbackslash}DPGL, that also operate by block-coordinate descent, where \${\textbackslash}B{\textbackslash}Theta\$ is the optimization target. We study all of these algorithms, and in particular different approaches to solving their coordinate sub-problems. We conclude that {\textbackslash}DPGL{\textbackslash} is superior from several points of view.},
	urldate = {2023-11-30},
	publisher = {arXiv},
	author = {Mazumder, Rahul and Hastie, Trevor},
	month = aug,
	year = {2012},
	note = {arXiv:1111.5479 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: This is a revised version of our previous manuscript with the same name ArXiv id: http://arxiv.org/abs/1111.5479},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/NR3ABS4B/1111.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/EBS5D36M/Mazumder and Hastie - 2012 - The Graphical Lasso New Insights and Alternatives.pdf:application/pdf},
}

@article{mazumder_graphical_2012-1,
	title = {The graphical lasso: {New} insights and alternatives},
	volume = {6},
	issn = {1935-7524},
	shorttitle = {The graphical lasso},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-6/issue-none/The-graphical-lasso-New-insights-and-alternatives/10.1214/12-EJS740.full},
	doi = {10.1214/12-EJS740},
	abstract = {The graphical lasso [5] is an algorithm for learning the structure in an undirected Gaussian graphical model, using ℓ1 regularization to control the number of zeros in the precision matrix Θ = Σ−1 [2, 11]. The R package glasso [5] is popular, fast, and allows one to eﬃciently build a path of models for diﬀerent values of the tuning parameter. Convergence of glasso can be tricky; the converged precision matrix might not be the inverse of the estimated covariance, and occasionally it fails to converge with warm starts. In this paper we explain this behavior, and propose new algorithms that appear to outperform glasso.},
	language = {en},
	number = {none},
	urldate = {2023-12-01},
	journal = {Electronic Journal of Statistics},
	author = {Mazumder, Rahul and Hastie, Trevor},
	month = jan,
	year = {2012},
	file = {Mazumder and Hastie - 2012 - The graphical lasso New insights and alternatives.pdf:/Users/jdomi/Zotero/storage/CMCYBS5L/Mazumder and Hastie - 2012 - The graphical lasso New insights and alternatives.pdf:application/pdf},
}

@article{zhao_huge_nodate,
	title = {The huge {Package} for {High}-dimensional {Undirected} {Graph} {Estimation} in {R}},
	abstract = {We describe an R package named huge which provides easy-to-use functions for estimating high dimensional undirected graphs from data. This package implements recent results in the literature, including Friedman et al. (2007), Liu et al. (2009, 2012) and Liu et al. (2010). Compared with the existing graph estimation package glasso, the huge package provides extra features: (1) instead of using Fortan, it is written in C, which makes the code more portable and easier to modify; (2) besides ﬁtting Gaussian graphical models, it also provides functions for ﬁtting high dimensional semiparametric Gaussian copula models; (3) more functions like data-dependent model selection, data generation and graph visualization; (4) a minor convergence problem of the graphical lasso algorithm is corrected; (5) the package allows the user to apply both lossless and lossy screening rules to scale up large-scale problems, making a tradeoff between computational and statistical efﬁciency.},
	language = {en},
	author = {Zhao, Tuo and Liu, Han and Roeder, Kathryn and Lafferty, John and Wasserman, Larry},
	file = {Zhao et al. - The huge Package for High-dimensional Undirected G.pdf:/Users/jdomi/Zotero/storage/XJ3QXJZM/Zhao et al. - The huge Package for High-dimensional Undirected G.pdf:application/pdf},
}

@article{ravikumar_high-dimensional_2010,
	title = {High-dimensional {Ising} model selection using ℓ1-regularized logistic regression},
	volume = {38},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-38/issue-3/High-dimensional-Ising-model-selection-using-%e2%84%931-regularized-logistic-regression/10.1214/09-AOS691.full},
	doi = {10.1214/09-AOS691},
	abstract = {We consider the problem of estimating the graph associated with a binary Ising Markov random field. We describe a method based on ℓ1-regularized logistic regression, in which the neighborhood of any given node is estimated by performing logistic regression subject to an ℓ1-constraint. The method is analyzed under high-dimensional scaling in which both the number of nodes p and maximum neighborhood size d are allowed to grow as a function of the number of observations n. Our main results provide sufficient conditions on the triple (n, p, d) and the model parameters for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously. With coherence conditions imposed on the population Fisher information matrix, we prove that consistent neighborhood selection can be obtained for sample sizes n=Ω(d3log p) with exponentially decaying error. When these same conditions are imposed directly on the sample matrices, we show that a reduced sample size of n=Ω(d2log p) suffices for the method to estimate neighborhoods consistently. Although this paper focuses on the binary graphical models, we indicate how a generalization of the method of the paper would apply to general discrete Markov random fields.},
	number = {3},
	urldate = {2023-12-03},
	journal = {The Annals of Statistics},
	author = {Ravikumar, Pradeep and Wainwright, Martin J. and Lafferty, John D.},
	month = jun,
	year = {2010},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62F12, 68T99, convex risk minimization, graphical models, high-dimensional asymptotics, ℓ\_1-regularization, Markov random fields, Model selection, structure learning},
	pages = {1287--1319},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/88ZK5VS2/Ravikumar et al. - 2010 - High-dimensional Ising model selection using ℓ1-re.pdf:application/pdf},
}

@article{mohan_graphical_2021,
	title = {Graphical {Models} for {Processing} {Missing} {Data}},
	volume = {116},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2021.1874961},
	doi = {10.1080/01621459.2021.1874961},
	abstract = {This article reviews recent advances in missing data research using graphical models to represent multivariate dependencies. We first examine the limitations of traditional frameworks from three different perspectives: transparency, estimability, and testability. We then show how procedures based on graphical models can overcome these limitations and provide meaningful performance guarantees even when data are missing not at random (MNAR). In particular, we identify conditions that guarantee consistent estimation in broad categories of missing data problems, and derive procedures for implementing this estimation. Finally, we derive testable implications for missing data models in both missing at random and MNAR categories.},
	number = {534},
	urldate = {2023-12-04},
	journal = {Journal of the American Statistical Association},
	author = {Mohan, Karthika and Pearl, Judea},
	month = apr,
	year = {2021},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2021.1874961},
	keywords = {Graphical models, Missing data, Missing not at random, Nonignorable, Recoverability, Testability},
	pages = {1023--1037},
	file = {Submitted Version:/Users/jdomi/Zotero/storage/F3ZQ6WZX/Mohan and Pearl - 2021 - Graphical Models for Processing Missing Data.pdf:application/pdf},
}

@inproceedings{mohan_graphical_2013,
	title = {Graphical {Models} for {Inference} with {Missing} {Data}},
	volume = {26},
	url = {https://papers.nips.cc/paper_files/paper/2013/hash/0ff8033cf9437c213ee13937b1c4c455-Abstract.html},
	urldate = {2023-12-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Mohan, Karthika and Pearl, Judea and Tian, Jin},
	year = {2013},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/WIWNFSS3/Mohan et al. - 2013 - Graphical Models for Inference with Missing Data.pdf:application/pdf},
}

@book{hastie_elements_2009,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning}},
	isbn = {978-0-387-84857-0 978-0-387-84858-7},
	url = {http://link.springer.com/10.1007/978-0-387-84858-7},
	urldate = {2023-12-04},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009},
	doi = {10.1007/978-0-387-84858-7},
	keywords = {Averaging, Boosting, classification, clustering, data mining, machine learning, Projection pursuit, Random Forest, supervised learning, Support Vector Machine, unsupervised learning},
	file = {Full Text:/Users/jdomi/Zotero/storage/FA7G4XEY/Hastie et al. - 2009 - The Elements of Statistical Learning.pdf:application/pdf},
}

@book{hastie_statistical_2016,
	title = {Statistical {Learning} with {Sparsity}: the {Lasso} and {Generalizations}},
	url = {https://hastie.su.domains/StatLearnSparsity/},
	urldate = {2023-12-04},
	author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
	month = dec,
	year = {2016},
	file = {Statistical Learning with Sparsity\: the Lasso and Generalizations:/Users/jdomi/Zotero/storage/RBQYZ7L5/StatLearnSparsity.html:text/html},
}
