% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nty/global//global/global}
    \entry{abdollahi_nodecoder_2023}{misc}{}
      \name{author}{5}{}{%
        {{hash=f8178a67163b97248d735b49dfb10552}{%
           family={Abdollahi},
           familyi={A\bibinitperiod},
           given={Nasim},
           giveni={N\bibinitperiod}}}%
        {{hash=4efd5e9b13d5523c7d40abcd7d9d9cff}{%
           family={Tonekaboni},
           familyi={T\bibinitperiod},
           given={Seyed\bibnamedelimb Ali\bibnamedelima Madani},
           giveni={S\bibinitperiod\bibinitdelim A\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=95dfac5ff6e2ca54a7bae78900cfe2ab}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Jay},
           giveni={J\bibinitperiod}}}%
        {{hash=2938deb5048323c6e1bfdd80975d5b28}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Bo},
           giveni={B\bibinitperiod}}}%
        {{hash=c6a41ed25b284d9abcd2e6c821420882}{%
           family={MacKinnon},
           familyi={M\bibinitperiod},
           given={Stephen},
           giveni={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{0c139d4971cc91af3d4d263abf3ed56d}
      \strng{fullhash}{fe50ac7aa50f64248526d74a7759b24b}
      \strng{bibnamehash}{0c139d4971cc91af3d4d263abf3ed56d}
      \strng{authorbibnamehash}{0c139d4971cc91af3d4d263abf3ed56d}
      \strng{authornamehash}{0c139d4971cc91af3d4d263abf3ed56d}
      \strng{authorfullhash}{fe50ac7aa50f64248526d74a7759b24b}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{While accurate protein structure predictions are now available for nearly every observed protein sequence, predicted structures lack much of the functional context offered by experimental structure determination. We address this gap with NodeCoder, a task-independent platform that maps residue-based datasets onto 3D protein structures, embeds the resulting structural feature into a contact network, and models residue classification tasks with a Graph Convolutional Network (GCN). We demonstrate the versatility of this strategy by modeling six separate tasks, with some labels derived from other experimental structure studies (ligand, peptide, ion, and nucleic acid binding sites) and other labels derived from annotation databases (post-translational modification and transmembrane regions). Moreover, A NodeCoder model trained to identify ligand binding site residues was able to outperform P2Rank, a widely-used software developed specifically for ligand binding site detection. NodeCoder is available as an open-source python package at https://pypi.org/project/NodeCoder/.}
      \field{annotation}{Comment: including supplementary materials 22 pages, 6 figures, 4 tables, presented at NeurIPS 2021 and ACS 2022}
      \field{month}{2}
      \field{note}{arXiv:2302.03590 [q-bio]}
      \field{shorttitle}{{NodeCoder}}
      \field{title}{{NodeCoder}: a graph-based machine learning platform to predict active sites of modeled protein structures}
      \field{urlday}{6}
      \field{urlmonth}{12}
      \field{urlyear}{2023}
      \field{year}{2023}
      \field{urldateera}{ce}
      \verb{file}
      \verb arXiv.org Snapshot:/Users/jdomi/Zotero/storage/YAJI5RE9/2302.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/L5EGQRFI/Abdollahi et al. - 2023 - NodeCoder a graph-based machine learning platform.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2302.03590
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2302.03590
      \endverb
      \keyw{Quantitative Biology - Quantitative Methods}
    \endentry
    \entry{duvenaud_convolutional_2015}{misc}{}
      \name{author}{7}{}{%
        {{hash=834f50f5d5a332c21effd10f07daf79e}{%
           family={Duvenaud},
           familyi={D\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=07e607fb522f06610feb023ecfa712e2}{%
           family={Maclaurin},
           familyi={M\bibinitperiod},
           given={Dougal},
           giveni={D\bibinitperiod}}}%
        {{hash=5debbaca3d1169a9867a72e085c3cefd}{%
           family={Aguilera-Iparraguirre},
           familyi={A\bibinithyphendelim I\bibinitperiod},
           given={Jorge},
           giveni={J\bibinitperiod}}}%
        {{hash=a37688f976e048ffbc85a45699635c91}{%
           family={Gómez-Bombarelli},
           familyi={G\bibinithyphendelim B\bibinitperiod},
           given={Rafael},
           giveni={R\bibinitperiod}}}%
        {{hash=2f565f45e0faadc2ec7c0fbf40e550cc}{%
           family={Hirzel},
           familyi={H\bibinitperiod},
           given={Timothy},
           giveni={T\bibinitperiod}}}%
        {{hash=083dcdd0db2687433b22fabfb2e55c17}{%
           family={Aspuru-Guzik},
           familyi={A\bibinithyphendelim G\bibinitperiod},
           given={Alán},
           giveni={A\bibinitperiod}}}%
        {{hash=c1552be0c6aa9c6e0fd91a130a682853}{%
           family={Adams},
           familyi={A\bibinitperiod},
           given={Ryan\bibnamedelima P.},
           giveni={R\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{fefc8e1deb8de16e193321f2de1eda2f}
      \strng{fullhash}{76efadea36b06eac13c72157e055b771}
      \strng{bibnamehash}{fefc8e1deb8de16e193321f2de1eda2f}
      \strng{authorbibnamehash}{fefc8e1deb8de16e193321f2de1eda2f}
      \strng{authornamehash}{fefc8e1deb8de16e193321f2de1eda2f}
      \strng{authorfullhash}{76efadea36b06eac13c72157e055b771}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.}
      \field{annotation}{Comment: 9 pages, 5 figures. To appear in Neural Information Processing Systems (NIPS)}
      \field{month}{11}
      \field{note}{arXiv:1509.09292 [cs, stat]}
      \field{title}{Convolutional {Networks} on {Graphs} for {Learning} {Molecular} {Fingerprints}}
      \field{urlday}{27}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2015}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1509.09292
      \endverb
      \verb{file}
      \verb arXiv Fulltext PDF:/Users/jdomi/Zotero/storage/25EMKK23/Duvenaud et al. - 2015 - Convolutional Networks on Graphs for Learning Mole.pdf:application/pdf;arXiv.org Snapshot:/Users/jdomi/Zotero/storage/FSXKHGU9/1509.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1509.09292
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1509.09292
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing}
    \endentry
    \entry{ektefaie_multimodal_2023}{article}{}
      \name{author}{5}{}{%
        {{hash=9706737cdd9e6111fbc0ee5fdaa848c7}{%
           family={Ektefaie},
           familyi={E\bibinitperiod},
           given={Yasha},
           giveni={Y\bibinitperiod}}}%
        {{hash=6d38ba04252526c77483e030d609daf6}{%
           family={Dasoulas},
           familyi={D\bibinitperiod},
           given={George},
           giveni={G\bibinitperiod}}}%
        {{hash=1f012affd20ae9c45ab3c0f96b0e84d3}{%
           family={Noori},
           familyi={N\bibinitperiod},
           given={Ayush},
           giveni={A\bibinitperiod}}}%
        {{hash=4388e632550213867abe8f3dc9d8e74a}{%
           family={Farhat},
           familyi={F\bibinitperiod},
           given={Maha},
           giveni={M\bibinitperiod}}}%
        {{hash=5f1df07c5fbd5a91d0e32b8697082857}{%
           family={Zitnik},
           familyi={Z\bibinitperiod},
           given={Marinka},
           giveni={M\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{82993c530f85986f9b34b0dd6f280369}
      \strng{fullhash}{d399ceab17ccbd7084d97a4199d5b87e}
      \strng{bibnamehash}{82993c530f85986f9b34b0dd6f280369}
      \strng{authorbibnamehash}{82993c530f85986f9b34b0dd6f280369}
      \strng{authornamehash}{82993c530f85986f9b34b0dd6f280369}
      \strng{authorfullhash}{d399ceab17ccbd7084d97a4199d5b87e}
      \field{sortinit}{E}
      \field{sortinithash}{8da8a182d344d5b9047633dfc0cc9131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Artificial intelligence for graphs has achieved remarkable success in modelling complex systems, ranging from dynamic networks in biology to interacting particle systems in physics. However, the increasingly heterogeneous graph datasets call for multimodal methods that can combine different inductive biases — assumptions that algorithms use to make predictions for inputs they have not encountered during training. Learning on multimodal datasets is challenging because the inductive biases can vary by data modality and graphs might not be explicitly given in the input. To address these challenges, graph artificial intelligence methods combine different modalities while leveraging cross-modal dependencies through geometric relationships. Diverse datasets are combined using graphs and fed into sophisticated multimodal architectures, specified as image-intensive, knowledge-grounded and language-intensive models. Using this categorization, we introduce a blueprint for multimodal graph learning, use it to study existing methods and provide guidelines to design new models.}
      \field{issn}{2522-5839}
      \field{journaltitle}{Nature Machine Intelligence}
      \field{month}{4}
      \field{note}{Number: 4 Publisher: Nature Publishing Group}
      \field{number}{4}
      \field{title}{Multimodal learning with graphs}
      \field{urlday}{26}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{5}
      \field{year}{2023}
      \field{urldateera}{ce}
      \field{pages}{340\bibrangedash 350}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1038/s42256-023-00624-6
      \endverb
      \verb{file}
      \verb Submitted Version:/Users/jdomi/Zotero/storage/VWY2SA29/Ektefaie et al. - 2023 - Multimodal learning with graphs.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/s42256-023-00624-6
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/s42256-023-00624-6
      \endverb
      \keyw{Machine learning,Computer science,Statistics,Computational science}
    \endentry
    \entry{ferludin_tf-gnn_2023}{misc}{}
      \name{author}{27}{}{%
        {{hash=6a8ede9edca7199b11eb2be6470496c6}{%
           family={Ferludin},
           familyi={F\bibinitperiod},
           given={Oleksandr},
           giveni={O\bibinitperiod}}}%
        {{hash=3d575967a362a573c1cebf8b80813839}{%
           family={Eigenwillig},
           familyi={E\bibinitperiod},
           given={Arno},
           giveni={A\bibinitperiod}}}%
        {{hash=27fdf0c75975816ef453080418d6d003}{%
           family={Blais},
           familyi={B\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=6196f36ea2a0362af89723c6ab8410ea}{%
           family={Zelle},
           familyi={Z\bibinitperiod},
           given={Dustin},
           giveni={D\bibinitperiod}}}%
        {{hash=9593c188424e417a82ba098eb7bf2b98}{%
           family={Pfeifer},
           familyi={P\bibinitperiod},
           given={Jan},
           giveni={J\bibinitperiod}}}%
        {{hash=cfe62c9365c523ba7d4f7fd1147cff41}{%
           family={Sanchez-Gonzalez},
           familyi={S\bibinithyphendelim G\bibinitperiod},
           given={Alvaro},
           giveni={A\bibinitperiod}}}%
        {{hash=777a88f13c4811865edf7848be0dee07}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Wai\bibnamedelimb Lok\bibnamedelima Sibon},
           giveni={W\bibinitperiod\bibinitdelim L\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=a90c626d0bc2b0bc1c7adb2066a4919c}{%
           family={Abu-El-Haija},
           familyi={A\bibinithyphendelim E\bibinithyphendelim H\bibinitperiod},
           given={Sami},
           giveni={S\bibinitperiod}}}%
        {{hash=5a63429a6e1733e8f3f4dc71cbb6eec9}{%
           family={Battaglia},
           familyi={B\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
        {{hash=292165e91a9fd8315550174c8ea7bb84}{%
           family={Bulut},
           familyi={B\bibinitperiod},
           given={Neslihan},
           giveni={N\bibinitperiod}}}%
        {{hash=f1a8b87e0d2f4a688ff24ff81f90656c}{%
           family={Halcrow},
           familyi={H\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod}}}%
        {{hash=18deeb663a9baa29a49f8c944b184f51}{%
           family={Almeida},
           familyi={A\bibinitperiod},
           given={Filipe\bibnamedelimb Miguel\bibnamedelima Gonçalves},
           giveni={F\bibinitperiod\bibinitdelim M\bibinitperiod\bibinitdelim G\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
        {{hash=8d02bf11fafaf93727a4b58fcfcaa8e8}{%
           family={Gonnet},
           familyi={G\bibinitperiod},
           given={Pedro},
           giveni={P\bibinitperiod}}}%
        {{hash=cb6bb2ec9b79c0990abc58585b3ef7ab}{%
           family={Jiang},
           familyi={J\bibinitperiod},
           given={Liangze},
           giveni={L\bibinitperiod}}}%
        {{hash=5322fb9d898092c5dc6a3057b86b2469}{%
           family={Kothari},
           familyi={K\bibinitperiod},
           given={Parth},
           giveni={P\bibinitperiod}}}%
        {{hash=5681ea16e5d63a76143c93b584264e8c}{%
           family={Lattanzi},
           familyi={L\bibinitperiod},
           given={Silvio},
           giveni={S\bibinitperiod}}}%
        {{hash=40abbd6e5cb3f57a91ba99d3e71f4d5a}{%
           family={Linhares},
           familyi={L\bibinitperiod},
           given={André},
           giveni={A\bibinitperiod}}}%
        {{hash=0a6dea0096b19ec2e1a00e0adce494a6}{%
           family={Mayer},
           familyi={M\bibinitperiod},
           given={Brandon},
           giveni={B\bibinitperiod}}}%
        {{hash=2403978c38056af7a74b3ef42192cc46}{%
           family={Mirrokni},
           familyi={M\bibinitperiod},
           given={Vahab},
           giveni={V\bibinitperiod}}}%
        {{hash=63e3da9b99c3b70eb20fa472a73d4904}{%
           family={Palowitch},
           familyi={P\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
        {{hash=65c6e6f2e861b74ec65c27354079d881}{%
           family={Paradkar},
           familyi={P\bibinitperiod},
           given={Mihir},
           giveni={M\bibinitperiod}}}%
        {{hash=f12eef3f89a29bb7c5ee51d10bc71ba0}{%
           family={She},
           familyi={S\bibinitperiod},
           given={Jennifer},
           giveni={J\bibinitperiod}}}%
        {{hash=33a24fcbb1fd295f5542b38203954a4f}{%
           family={Tsitsulin},
           familyi={T\bibinitperiod},
           given={Anton},
           giveni={A\bibinitperiod}}}%
        {{hash=1f309062b709d3579e03c16ee7a0f124}{%
           family={Villela},
           familyi={V\bibinitperiod},
           given={Kevin},
           giveni={K\bibinitperiod}}}%
        {{hash=c9e3554ba89b08176017242506dd3424}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Lisa},
           giveni={L\bibinitperiod}}}%
        {{hash=ccad041ca1fd5cb223b21f7307baccef}{%
           family={Wong},
           familyi={W\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=3bbe3b6d26e1f03feea22728836605cd}{%
           family={Perozzi},
           familyi={P\bibinitperiod},
           given={Bryan},
           giveni={B\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{93704c698499f9da3e731b37fb5e05a3}
      \strng{fullhash}{ac097a82a9dfa44f2771a82eb16ee826}
      \strng{bibnamehash}{93704c698499f9da3e731b37fb5e05a3}
      \strng{authorbibnamehash}{93704c698499f9da3e731b37fb5e05a3}
      \strng{authornamehash}{93704c698499f9da3e731b37fb5e05a3}
      \strng{authorfullhash}{ac097a82a9dfa44f2771a82eb16ee826}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{TensorFlow-GNN (TF-GNN) is a scalable library for Graph Neural Networks in TensorFlow. It is designed from the bottom up to support the kinds of rich heterogeneous graph data that occurs in today's information ecosystems. In addition to enabling machine learning researchers and advanced developers, TF-GNN offers low-code solutions to empower the broader developer community in graph learning. Many production models at Google use TF-GNN, and it has been recently released as an open source project. In this paper we describe the TF-GNN data model, its Keras message passing API, and relevant capabilities such as graph sampling and distributed training.}
      \field{month}{7}
      \field{note}{arXiv:2207.03522 [physics, stat]}
      \field{shorttitle}{{TF}-{GNN}}
      \field{title}{{TF}-{GNN}: {Graph} {Neural} {Networks} in {TensorFlow}}
      \field{urlday}{28}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2023}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2207.03522
      \endverb
      \verb{file}
      \verb arXiv Fulltext PDF:/Users/jdomi/Zotero/storage/5GCCG9V5/Ferludin et al. - 2023 - TF-GNN Graph Neural Networks in TensorFlow.pdf:application/pdf;arXiv.org Snapshot:/Users/jdomi/Zotero/storage/8GXFP7QQ/2207.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2207.03522
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2207.03522
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Social and Information Networks,Physics - Physics and Society}
    \endentry
    \entry{gainza_deciphering_2020}{article}{}
      \name{author}{7}{}{%
        {{hash=1a9fc988a5f77ccee00c5d1f45cbdba5}{%
           family={Gainza},
           familyi={G\bibinitperiod},
           given={P.},
           giveni={P\bibinitperiod}}}%
        {{hash=c2d94a178a59beea25f96b0c7f5b03da}{%
           family={Sverrisson},
           familyi={S\bibinitperiod},
           given={F.},
           giveni={F\bibinitperiod}}}%
        {{hash=088055f86e6b25e7fe98a71c2035ef43}{%
           family={Monti},
           familyi={M\bibinitperiod},
           given={F.},
           giveni={F\bibinitperiod}}}%
        {{hash=0d6ebeea7575ebbe6de481820b17ce79}{%
           family={Rodolà},
           familyi={R\bibinitperiod},
           given={E.},
           giveni={E\bibinitperiod}}}%
        {{hash=373820ed5dc49d4a83e328076a4d061b}{%
           family={Boscaini},
           familyi={B\bibinitperiod},
           given={D.},
           giveni={D\bibinitperiod}}}%
        {{hash=3378643ad337f0831df40a3159bd8e23}{%
           family={Bronstein},
           familyi={B\bibinitperiod},
           given={M.\bibnamedelimi M.},
           giveni={M\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=25be7ad971aac470dd6c5a7105bf55b3}{%
           family={Correia},
           familyi={C\bibinitperiod},
           given={B.\bibnamedelimi E.},
           giveni={B\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{1778a794a97f4497602dd7105df67341}
      \strng{fullhash}{c0b7f7e339c39cbff616cda163525bbf}
      \strng{bibnamehash}{1778a794a97f4497602dd7105df67341}
      \strng{authorbibnamehash}{1778a794a97f4497602dd7105df67341}
      \strng{authornamehash}{1778a794a97f4497602dd7105df67341}
      \strng{authorfullhash}{c0b7f7e339c39cbff616cda163525bbf}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Predicting interactions between proteins and other biomolecules solely based on structure remains a challenge in biology. A high-level representation of protein structure, the molecular surface, displays patterns of chemical and geometric features that fingerprint a protein’s modes of interactions with other biomolecules. We hypothesize that proteins participating in similar interactions may share common fingerprints, independent of their evolutionary history. Fingerprints may be difficult to grasp by visual analysis but could be learned from large-scale datasets. We present MaSIF (molecular surface interaction fingerprinting), a conceptual framework based on a geometric deep learning method to capture fingerprints that are important for specific biomolecular interactions. We showcase MaSIF with three prediction challenges: protein pocket-ligand prediction, protein–protein interaction site prediction and ultrafast scanning of protein surfaces for prediction of protein–protein complexes. We anticipate that our conceptual framework will lead to improvements in our understanding of protein function and design.}
      \field{issn}{1548-7105}
      \field{journaltitle}{Nature Methods}
      \field{month}{2}
      \field{note}{Number: 2 Publisher: Nature Publishing Group}
      \field{number}{2}
      \field{title}{Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning}
      \field{urlday}{29}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{17}
      \field{year}{2020}
      \field{urldateera}{ce}
      \field{pages}{184\bibrangedash 192}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1038/s41592-019-0666-6
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/jdomi/Zotero/storage/MSR9WTZY/Gainza et al. - 2020 - Deciphering interaction fingerprints from protein .pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/s41592-019-0666-6
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/s41592-019-0666-6
      \endverb
      \keyw{Proteins,Machine learning,Protein function predictions,Protein structure predictions}
    \endentry
    \entry{gilmer_neural_2017}{misc}{}
      \name{author}{5}{}{%
        {{hash=4f550339f0337905aa634f39e1ba4833}{%
           family={Gilmer},
           familyi={G\bibinitperiod},
           given={Justin},
           giveni={J\bibinitperiod}}}%
        {{hash=aa3475bbb938eec3f9f0d7c8d44f1e86}{%
           family={Schoenholz},
           familyi={S\bibinitperiod},
           given={Samuel\bibnamedelima S.},
           giveni={S\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=e9632c2c29a574cd09a5a7503be60435}{%
           family={Riley},
           familyi={R\bibinitperiod},
           given={Patrick\bibnamedelima F.},
           giveni={P\bibinitperiod\bibinitdelim F\bibinitperiod}}}%
        {{hash=494b568c5dc85ba8f3f409635f9c5f25}{%
           family={Vinyals},
           familyi={V\bibinitperiod},
           given={Oriol},
           giveni={O\bibinitperiod}}}%
        {{hash=b2367e57c17225a47853346440e87b35}{%
           family={Dahl},
           familyi={D\bibinitperiod},
           given={George\bibnamedelima E.},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{c654290e16b9e71825428d767650ff2a}
      \strng{fullhash}{a97dbeb15b563be460fc056e7a076a4b}
      \strng{bibnamehash}{c654290e16b9e71825428d767650ff2a}
      \strng{authorbibnamehash}{c654290e16b9e71825428d767650ff2a}
      \strng{authornamehash}{c654290e16b9e71825428d767650ff2a}
      \strng{authorfullhash}{a97dbeb15b563be460fc056e7a076a4b}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.}
      \field{annotation}{Comment: 14 pages}
      \field{month}{6}
      \field{note}{arXiv:1704.01212 [cs]}
      \field{title}{Neural {Message} {Passing} for {Quantum} {Chemistry}}
      \field{urlday}{27}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2017}
      \field{urldateera}{ce}
      \verb{file}
      \verb arXiv.org Snapshot:/Users/jdomi/Zotero/storage/BFUQJEMC/1704.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/5SHINSLY/Gilmer et al. - 2017 - Neural Message Passing for Quantum Chemistry.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1704.01212
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1704.01212
      \endverb
      \keyw{Computer Science - Machine Learning,I.2.6}
    \endentry
    \entry{gori_new_2005}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=9ab33529dbd8ef78724f1dc8a425e57e}{%
           family={Gori},
           familyi={G\bibinitperiod},
           given={M.},
           giveni={M\bibinitperiod}}}%
        {{hash=2c9332473b589bf9413b8bf49f7a999c}{%
           family={Monfardini},
           familyi={M\bibinitperiod},
           given={G.},
           giveni={G\bibinitperiod}}}%
        {{hash=08ec959fd97d02517d7be592475bf19c}{%
           family={Scarselli},
           familyi={S\bibinitperiod},
           given={F.},
           giveni={F\bibinitperiod}}}%
      }
      \strng{namehash}{c0364cfb2415e80858c70d5ead3ee348}
      \strng{fullhash}{6a60b97655015a1d5bf7867cfa6a355d}
      \strng{bibnamehash}{6a60b97655015a1d5bf7867cfa6a355d}
      \strng{authorbibnamehash}{6a60b97655015a1d5bf7867cfa6a355d}
      \strng{authornamehash}{c0364cfb2415e80858c70d5ead3ee348}
      \strng{authorfullhash}{6a60b97655015a1d5bf7867cfa6a355d}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In several applications the information is naturally represented by graphs. Traditional approaches cope with graphical data structures using a preprocessing phase which transforms the graphs into a set of flat vectors. However, in this way, important topological information may be lost and the achieved results may heavily depend on the preprocessing stage. This paper presents a new neural model, called graph neural network (GNN), capable of directly processing graphs. GNNs extends recursive neural networks and can be applied on most of the practically useful kinds of graphs, including directed, undirected, labelled and cyclic graphs. A learning algorithm for GNNs is proposed and some experiments are discussed which assess the properties of the model.}
      \field{booktitle}{Proceedings. 2005 {IEEE} {International} {Joint} {Conference} on {Neural} {Networks}, 2005.}
      \field{month}{7}
      \field{note}{ISSN: 2161-4407}
      \field{title}{A new model for learning in graph domains}
      \field{urlday}{28}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{2}
      \field{year}{2005}
      \field{urldateera}{ce}
      \field{pages}{729\bibrangedash 734 vol. 2}
      \range{pages}{-1}
      \verb{doi}
      \verb 10.1109/IJCNN.2005.1555942
      \endverb
      \verb{file}
      \verb IEEE Xplore Abstract Record:/Users/jdomi/Zotero/storage/GTX6ZF9N/1555942.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/1555942
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/1555942
      \endverb
    \endentry
    \entry{isert_structure-based_2023}{article}{}
      \name{author}{3}{}{%
        {{hash=4520c82216d88cdcd6c2164fac8d05b7}{%
           family={Isert},
           familyi={I\bibinitperiod},
           given={Clemens},
           giveni={C\bibinitperiod}}}%
        {{hash=6d75c9970cc3d386624751597761b0e0}{%
           family={Atz},
           familyi={A\bibinitperiod},
           given={Kenneth},
           giveni={K\bibinitperiod}}}%
        {{hash=9477eb09da9d3d7c6dd63f569486d863}{%
           family={Schneider},
           familyi={S\bibinitperiod},
           given={Gisbert},
           giveni={G\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{68e4f385d3ef10708887e547e4c96265}
      \strng{fullhash}{b9b9ef0a542b969b324af1c718a42428}
      \strng{bibnamehash}{b9b9ef0a542b969b324af1c718a42428}
      \strng{authorbibnamehash}{b9b9ef0a542b969b324af1c718a42428}
      \strng{authornamehash}{68e4f385d3ef10708887e547e4c96265}
      \strng{authorfullhash}{b9b9ef0a542b969b324af1c718a42428}
      \field{sortinit}{I}
      \field{sortinithash}{8d291c51ee89b6cd86bf5379f0b151d8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Structure-based drug design uses three-dimensional geometric information of macromolecules, such as proteins or nucleic acids, to identify suitable ligands. Geometric deep learning, an emerging concept of neural-network-based machine learning, has been applied to macromolecular structures. This review provides an overview of the recent applications of geometric deep learning in bioorganic and medicinal chemistry, highlighting its potential for structurebased drug discovery and design. Emphasis is placed on molecular property prediction, ligand binding site and pose prediction, and structure-based de novo molecular design. The current challenges and opportunities are highlighted, and a forecast of the future of geometric deep learning for drug discovery is presented.}
      \field{issn}{0959440X}
      \field{journaltitle}{Current Opinion in Structural Biology}
      \field{month}{4}
      \field{title}{Structure-based drug design with geometric deep learning}
      \field{urlday}{5}
      \field{urlmonth}{12}
      \field{urlyear}{2023}
      \field{volume}{79}
      \field{year}{2023}
      \field{urldateera}{ce}
      \field{pages}{102548}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1016/j.sbi.2023.102548
      \endverb
      \verb{file}
      \verb Isert et al. - 2023 - Structure-based drug design with geometric deep le.pdf:/Users/jdomi/Zotero/storage/X37AEHGN/Isert et al. - 2023 - Structure-based drug design with geometric deep le.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://linkinghub.elsevier.com/retrieve/pii/S0959440X23000222
      \endverb
      \verb{url}
      \verb https://linkinghub.elsevier.com/retrieve/pii/S0959440X23000222
      \endverb
    \endentry
    \entry{javaloy_mitigating_2022}{misc}{}
      \name{author}{3}{}{%
        {{hash=4537f161c24599389d9f72e154a3517a}{%
           family={Javaloy},
           familyi={J\bibinitperiod},
           given={Adrián},
           giveni={A\bibinitperiod}}}%
        {{hash=cba52ba155b0a1fb8d94716f29132586}{%
           family={Meghdadi},
           familyi={M\bibinitperiod},
           given={Maryam},
           giveni={M\bibinitperiod}}}%
        {{hash=86077c699c3bcb4f9fecce395f1e6f41}{%
           family={Valera},
           familyi={V\bibinitperiod},
           given={Isabel},
           giveni={I\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{e748a3cc30bff1cd08084768eadcd306}
      \strng{fullhash}{4553e82d1f76fa79620b1faf19709d8a}
      \strng{bibnamehash}{4553e82d1f76fa79620b1faf19709d8a}
      \strng{authorbibnamehash}{4553e82d1f76fa79620b1faf19709d8a}
      \strng{authornamehash}{e748a3cc30bff1cd08084768eadcd306}
      \strng{authorfullhash}{4553e82d1f76fa79620b1faf19709d8a}
      \field{sortinit}{J}
      \field{sortinithash}{b2f54a9081ace9966a7cb9413811edb4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A number of variational autoencoders (VAEs) have recently emerged with the aim of modeling multimodal data, e.g., to jointly model images and their corresponding captions. Still, multimodal VAEs tend to focus solely on a subset of the modalities, e.g., by fitting the image while neglecting the caption. We refer to this limitation as modality collapse. In this work, we argue that this effect is a consequence of conflicting gradients during multimodal VAE training. We show how to detect the sub-graphs in the computational graphs where gradients conflict (impartiality blocks), as well as how to leverage existing gradient-conflict solutions from multitask learning to mitigate modality collapse. That is, to ensure impartial optimization across modalities. We apply our training framework to several multimodal VAE models, losses and datasets from the literature, and empirically show that our framework significantly improves the reconstruction performance, conditional generation, and coherence of the latent space across modalities.}
      \field{annotation}{Comment: Accepted as a Spotlight paper at ICML 2022. 27 pages, 10 figures}
      \field{month}{6}
      \field{note}{arXiv:2206.04496 [cs]}
      \field{title}{Mitigating {Modality} {Collapse} in {Multimodal} {VAEs} via {Impartial} {Optimization}}
      \field{urlday}{29}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2206.04496
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2206.04496
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{kipf_semi-supervised_2017}{misc}{}
      \name{author}{2}{}{%
        {{hash=26d8bcbda6afaf96334bf47c8fb88a75}{%
           family={Kipf},
           familyi={K\bibinitperiod},
           given={Thomas\bibnamedelima N.},
           giveni={T\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=53d2880ad8047b61cdae2c6b2803e763}{%
           family={Welling},
           familyi={W\bibinitperiod},
           given={Max},
           giveni={M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{8357de6f2714533d08059670ab3ea016}
      \strng{fullhash}{e04e5808dc0d0ec733212b81e1f8c78a}
      \strng{bibnamehash}{e04e5808dc0d0ec733212b81e1f8c78a}
      \strng{authorbibnamehash}{e04e5808dc0d0ec733212b81e1f8c78a}
      \strng{authornamehash}{8357de6f2714533d08059670ab3ea016}
      \strng{authorfullhash}{e04e5808dc0d0ec733212b81e1f8c78a}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.}
      \field{annotation}{Comment: Published as a conference paper at ICLR 2017}
      \field{month}{2}
      \field{note}{arXiv:1609.02907 [cs, stat]}
      \field{title}{Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}}
      \field{urlday}{27}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2017}
      \field{urldateera}{ce}
      \verb{file}
      \verb arXiv.org Snapshot:/Users/jdomi/Zotero/storage/IJ8MTNEG/1609.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/AY9ETCPZ/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1609.02907
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1609.02907
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{mcdermott_structure-inducing_2023}{article}{}
      \name{author}{4}{}{%
        {{hash=320a9eb93080307302d84ae0ed6b7111}{%
           family={McDermott},
           familyi={M\bibinitperiod},
           given={Matthew\bibnamedelimb B.\bibnamedelimi A.},
           giveni={M\bibinitperiod\bibinitdelim B\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=9af2cc990c398e4e7941565aa2df225b}{%
           family={Yap},
           familyi={Y\bibinitperiod},
           given={Brendan},
           giveni={B\bibinitperiod}}}%
        {{hash=e4ec701f3e9b3400f41c7dcb84d2e88a}{%
           family={Szolovits},
           familyi={S\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
        {{hash=5f1df07c5fbd5a91d0e32b8697082857}{%
           family={Zitnik},
           familyi={Z\bibinitperiod},
           given={Marinka},
           giveni={M\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{f3c9eabb2b2c8e80fbdacf0fd6367f70}
      \strng{fullhash}{ce44954852217b01ca962462ddf80594}
      \strng{bibnamehash}{f3c9eabb2b2c8e80fbdacf0fd6367f70}
      \strng{authorbibnamehash}{f3c9eabb2b2c8e80fbdacf0fd6367f70}
      \strng{authornamehash}{f3c9eabb2b2c8e80fbdacf0fd6367f70}
      \strng{authorfullhash}{ce44954852217b01ca962462ddf80594}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Language model pre-training and the derived general-purpose methods have reshaped machine learning research. However, there remains considerable uncertainty regarding why pre-training improves the performance of downstream tasks. This challenge is pronounced when using language model pre-training in domains outside of natural language. Here we investigate this problem by analysing how pre-training methods impose relational structure in induced per-sample latent spaces—that is, what constraints do pre-training methods impose on the distance or geometry between the pre-trained embeddings of samples. A comprehensive review of pre-training methods reveals that this question remains open, despite theoretical analyses showing the importance of understanding this form of induced structure. Based on this review, we introduce a pre-training framework that enables a granular and comprehensive understanding of how relational structure can be induced. We present a theoretical analysis of the framework from the first principles and establish a connection between the relational inductive bias of pre-training and fine-tuning performance. Empirical studies spanning three data modalities and ten fine-tuning tasks confirm theoretical analyses, inform the design of novel pre-training methods and establish consistent improvements over a compelling suite of methods.}
      \field{issn}{2522-5839}
      \field{journaltitle}{Nature Machine Intelligence}
      \field{month}{6}
      \field{note}{Number: 6 Publisher: Nature Publishing Group}
      \field{number}{6}
      \field{title}{Structure-inducing pre-training}
      \field{urlday}{26}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{5}
      \field{year}{2023}
      \field{urldateera}{ce}
      \field{pages}{612\bibrangedash 621}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1038/s42256-023-00647-z
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/jdomi/Zotero/storage/L88JZQ9T/McDermott et al. - 2023 - Structure-inducing pre-training.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/s42256-023-00647-z
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/s42256-023-00647-z
      \endverb
      \keyw{Machine learning,Computational models,Computer science,Statistics}
    \endentry
    \entry{sanchez-lengeling_gentle_2021}{article}{}
      \name{author}{4}{}{%
        {{hash=7c4f5c739dce58401e7c2ed6906ff806}{%
           family={Sanchez-Lengeling},
           familyi={S\bibinithyphendelim L\bibinitperiod},
           given={Benjamin},
           giveni={B\bibinitperiod}}}%
        {{hash=bf36a95184bcf06abe636f74e9bcdc70}{%
           family={Reif},
           familyi={R\bibinitperiod},
           given={Emily},
           giveni={E\bibinitperiod}}}%
        {{hash=3394c53217b55644faae08f1c08d41ca}{%
           family={Pearce},
           familyi={P\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod}}}%
        {{hash=e0d9e9b0e574bb94ab3c67626cca5c17}{%
           family={Wiltschko},
           familyi={W\bibinitperiod},
           given={Alexander\bibnamedelima B.},
           giveni={A\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{87a91bc8ed3b035a4dbf1ec0b979e4e5}
      \strng{fullhash}{2e68e45732f50abe0c84e383e9f51470}
      \strng{bibnamehash}{87a91bc8ed3b035a4dbf1ec0b979e4e5}
      \strng{authorbibnamehash}{87a91bc8ed3b035a4dbf1ec0b979e4e5}
      \strng{authornamehash}{87a91bc8ed3b035a4dbf1ec0b979e4e5}
      \strng{authorfullhash}{2e68e45732f50abe0c84e383e9f51470}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{What components are needed for building learning algorithms that leverage the structure and properties of graphs?}
      \field{issn}{2476-0757}
      \field{journaltitle}{Distill}
      \field{month}{9}
      \field{number}{9}
      \field{title}{A {Gentle} {Introduction} to {Graph} {Neural} {Networks}}
      \field{urlday}{27}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{6}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{e33}
      \range{pages}{-1}
      \verb{doi}
      \verb 10.23915/distill.00033
      \endverb
      \verb{urlraw}
      \verb https://distill.pub/2021/gnn-intro
      \endverb
      \verb{url}
      \verb https://distill.pub/2021/gnn-intro
      \endverb
    \endentry
    \entry{satorras_en_2022}{misc}{}
      \name{author}{3}{}{%
        {{hash=a9c40c6be2a06d47fb58e0bdfd28a1f3}{%
           family={Satorras},
           familyi={S\bibinitperiod},
           given={Victor\bibnamedelima Garcia},
           giveni={V\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=3217062971cd751f410c7479cd9c280a}{%
           family={Hoogeboom},
           familyi={H\bibinitperiod},
           given={Emiel},
           giveni={E\bibinitperiod}}}%
        {{hash=53d2880ad8047b61cdae2c6b2803e763}{%
           family={Welling},
           familyi={W\bibinitperiod},
           given={Max},
           giveni={M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{3d45a0160934cbf19d121cff53d7db31}
      \strng{fullhash}{40192b6345076c7fe7f0e9204ffb25bc}
      \strng{bibnamehash}{40192b6345076c7fe7f0e9204ffb25bc}
      \strng{authorbibnamehash}{40192b6345076c7fe7f0e9204ffb25bc}
      \strng{authornamehash}{3d45a0160934cbf19d121cff53d7db31}
      \strng{authorfullhash}{40192b6345076c7fe7f0e9204ffb25bc}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.}
      \field{month}{2}
      \field{note}{arXiv:2102.09844 [cs, stat]}
      \field{title}{E(n) {Equivariant} {Graph} {Neural} {Networks}}
      \field{urlday}{6}
      \field{urlmonth}{12}
      \field{urlyear}{2023}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{file}
      \verb arXiv.org Snapshot:/Users/jdomi/Zotero/storage/MH4Q8UMH/2102.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/TNLJWDTI/Satorras et al. - 2022 - E(n) Equivariant Graph Neural Networks.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2102.09844
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2102.09844
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{scarselli_graph_2009}{article}{}
      \name{author}{5}{}{%
        {{hash=08ec959fd97d02517d7be592475bf19c}{%
           family={Scarselli},
           familyi={S\bibinitperiod},
           given={F.},
           giveni={F\bibinitperiod}}}%
        {{hash=9ab33529dbd8ef78724f1dc8a425e57e}{%
           family={Gori},
           familyi={G\bibinitperiod},
           given={M.},
           giveni={M\bibinitperiod}}}%
        {{hash=520b01fdf4ea304850fd2941b3e204df}{%
           family={{Ah Chung Tsoi}},
           familyi={A\bibinitperiod}}}%
        {{hash=f9aeb1565769af71b434ced426b91c86}{%
           family={Hagenbuchner},
           familyi={H\bibinitperiod},
           given={M.},
           giveni={M\bibinitperiod}}}%
        {{hash=2c9332473b589bf9413b8bf49f7a999c}{%
           family={Monfardini},
           familyi={M\bibinitperiod},
           given={G.},
           giveni={G\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{058d3d3e9c66e504975aa6380e0414cd}
      \strng{fullhash}{46c944a3987a3dd2d2c244e52016a100}
      \strng{bibnamehash}{058d3d3e9c66e504975aa6380e0414cd}
      \strng{authorbibnamehash}{058d3d3e9c66e504975aa6380e0414cd}
      \strng{authornamehash}{058d3d3e9c66e504975aa6380e0414cd}
      \strng{authorfullhash}{46c944a3987a3dd2d2c244e52016a100}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{1045-9227, 1941-0093}
      \field{journaltitle}{IEEE Transactions on Neural Networks}
      \field{month}{1}
      \field{number}{1}
      \field{title}{The {Graph} {Neural} {Network} {Model}}
      \field{urlday}{27}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{20}
      \field{year}{2009}
      \field{urldateera}{ce}
      \field{pages}{61\bibrangedash 80}
      \range{pages}{20}
      \verb{doi}
      \verb 10.1109/TNN.2008.2005605
      \endverb
      \verb{file}
      \verb Scarselli et al. - 2009 - The Graph Neural Network Model.pdf:/Users/jdomi/Zotero/storage/DLKGJ92Q/Scarselli et al. - 2009 - The Graph Neural Network Model.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://ieeexplore.ieee.org/document/4700287/
      \endverb
      \verb{url}
      \verb http://ieeexplore.ieee.org/document/4700287/
      \endverb
    \endentry
    \entry{xu_how_2019}{misc}{}
      \name{author}{4}{}{%
        {{hash=29a3a6bf823a641230b0258e0a8a0214}{%
           family={Xu},
           familyi={X\bibinitperiod},
           given={Keyulu},
           giveni={K\bibinitperiod}}}%
        {{hash=164c7f22cde069c57bf9b80f76441fa9}{%
           family={Hu},
           familyi={H\bibinitperiod},
           given={Weihua},
           giveni={W\bibinitperiod}}}%
        {{hash=900d107125ff0ca84698cb909e4f6c51}{%
           family={Leskovec},
           familyi={L\bibinitperiod},
           given={Jure},
           giveni={J\bibinitperiod}}}%
        {{hash=83a44c18acfaa38fa9f6ba76cfdc0132}{%
           family={Jegelka},
           familyi={J\bibinitperiod},
           given={Stefanie},
           giveni={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{344f5b90b6e6e45d86fe86fd8fdca38c}
      \strng{fullhash}{3fe1c2e12a449ac359ab2737dfe4ce17}
      \strng{bibnamehash}{344f5b90b6e6e45d86fe86fd8fdca38c}
      \strng{authorbibnamehash}{344f5b90b6e6e45d86fe86fd8fdca38c}
      \strng{authornamehash}{344f5b90b6e6e45d86fe86fd8fdca38c}
      \strng{authorfullhash}{3fe1c2e12a449ac359ab2737dfe4ce17}
      \field{sortinit}{X}
      \field{sortinithash}{1965c258adceecf23ce3d67b05113442}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.}
      \field{month}{2}
      \field{note}{arXiv:1810.00826 [cs, stat]}
      \field{title}{How {Powerful} are {Graph} {Neural} {Networks}?}
      \field{urlday}{27}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1810.00826
      \endverb
      \verb{file}
      \verb arXiv Fulltext PDF:/Users/jdomi/Zotero/storage/MX7SILVL/Xu et al. - 2019 - How Powerful are Graph Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/jdomi/Zotero/storage/9FD66I8S/1810.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1810.00826
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1810.00826
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning}
    \endentry
    \entry{ying_hierarchical_2018}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=5f21207e4436b16d3a0577038244f9b9}{%
           family={Ying},
           familyi={Y\bibinitperiod},
           given={Zhitao},
           giveni={Z\bibinitperiod}}}%
        {{hash=adf64964c8c9d5f144d937e11ccf61ba}{%
           family={You},
           familyi={Y\bibinitperiod},
           given={Jiaxuan},
           giveni={J\bibinitperiod}}}%
        {{hash=85f1d306bd524b1b382661ffd4da38e8}{%
           family={Morris},
           familyi={M\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
        {{hash=62341ebcc57e03720c55a39d93f0b17d}{%
           family={Ren},
           familyi={R\bibinitperiod},
           given={Xiang},
           giveni={X\bibinitperiod}}}%
        {{hash=2b50dc0bf681355a1fe029fdb5c75479}{%
           family={Hamilton},
           familyi={H\bibinitperiod},
           given={Will},
           giveni={W\bibinitperiod}}}%
        {{hash=900d107125ff0ca84698cb909e4f6c51}{%
           family={Leskovec},
           familyi={L\bibinitperiod},
           given={Jure},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{608f8f7abd0c8ec13137d8ec5a6de927}
      \strng{fullhash}{bf470773783968a3ee54f9c66b8715da}
      \strng{bibnamehash}{608f8f7abd0c8ec13137d8ec5a6de927}
      \strng{authorbibnamehash}{608f8f7abd0c8ec13137d8ec5a6de927}
      \strng{authornamehash}{608f8f7abd0c8ec13137d8ec5a6de927}
      \strng{authorfullhash}{bf470773783968a3ee54f9c66b8715da}
      \field{sortinit}{Y}
      \field{sortinithash}{fd67ad5a9ef0f7456bdd9aab10fe1495}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10\% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark datasets.}
      \field{booktitle}{Advances in {Neural} {Information} {Processing} {Systems}}
      \field{title}{Hierarchical {Graph} {Representation} {Learning} with {Differentiable} {Pooling}}
      \field{urlday}{28}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{31}
      \field{year}{2018}
      \field{urldateera}{ce}
      \verb{file}
      \verb Full Text PDF:/Users/jdomi/Zotero/storage/F9ECRZCH/Ying et al. - 2018 - Hierarchical Graph Representation Learning with Di.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://papers.nips.cc/paper_files/paper/2018/hash/e77dbaf6759253c7c6d0efc5690369c7-Abstract.html
      \endverb
      \verb{url}
      \verb https://papers.nips.cc/paper_files/paper/2018/hash/e77dbaf6759253c7c6d0efc5690369c7-Abstract.html
      \endverb
    \endentry
    \entry{zhang_systematic_2023}{misc}{}
      \name{author}{5}{}{%
        {{hash=9ea8bafe63d556c6624b241ead9d14eb}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Zaixi},
           giveni={Z\bibinitperiod}}}%
        {{hash=76b5cf27acf79c365307525d9fc24da6}{%
           family={Yan},
           familyi={Y\bibinitperiod},
           given={Jiaxian},
           giveni={J\bibinitperiod}}}%
        {{hash=0b07af422f6616fac129405a8b4b1de4}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Qi},
           giveni={Q\bibinitperiod}}}%
        {{hash=439e8d5e114670edea6f746db3c18b5b}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Enhong},
           giveni={E\bibinitperiod}}}%
        {{hash=5f1df07c5fbd5a91d0e32b8697082857}{%
           family={Zitnik},
           familyi={Z\bibinitperiod},
           given={Marinka},
           giveni={M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{17176afd1815f979397e0b67d77c8323}
      \strng{fullhash}{dc794b844db2883bc15128c86d335398}
      \strng{bibnamehash}{17176afd1815f979397e0b67d77c8323}
      \strng{authorbibnamehash}{17176afd1815f979397e0b67d77c8323}
      \strng{authornamehash}{17176afd1815f979397e0b67d77c8323}
      \strng{authorfullhash}{dc794b844db2883bc15128c86d335398}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Structure-based drug design (SBDD) utilizes the three-dimensional geometry of proteins to identify potential drug candidates. Traditional methods, grounded in physicochemical modeling and informed by domain expertise, are resource-intensive. Recent developments in geometric deep learning, focusing on the integration and processing of 3D geometric data, coupled with the availability of accurate protein 3D structure predictions from tools like AlphaFold, have greatly advanced the field of structure-based drug design. This paper systematically reviews the current state of geometric deep learning in SBDD. We first outline foundational tasks in SBDD, detail prevalent 3D protein representations, and highlight representative predictive and generative models. We then offer in-depth reviews of each key task, including binding site prediction, binding pose generation, {\textbackslash}emph\{de novo\} molecule generation, linker design, and binding affinity prediction. We provide formal problem definitions and outline each task's representative methods, datasets, evaluation metrics, and performance benchmarks. Finally, we summarize the current challenges and future opportunities: current challenges in SBDD include oversimplified problem formulations, inadequate out-of-distribution generalization, a lack of reliable evaluation metrics and large-scale benchmarks, and the need for experimental verification and enhanced model understanding; opportunities include leveraging multimodal datasets, integrating domain knowledge, building comprehensive benchmarks, designing criteria based on clinical endpoints, and developing foundation models that broaden the range of design tasks. We also curate {\textbackslash}url\{https://github.com/zaixizhang/Awesome-SBDD\}, reflecting ongoing contributions and new datasets in SBDD.}
      \field{annotation}{Comment: 20 pages, under review}
      \field{month}{10}
      \field{note}{arXiv:2306.11768 [cs, q-bio]}
      \field{title}{A {Systematic} {Survey} in {Geometric} {Deep} {Learning} for {Structure}-based {Drug} {Design}}
      \field{urlday}{26}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2023}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2306.11768
      \endverb
      \verb{file}
      \verb arXiv Fulltext PDF:/Users/jdomi/Zotero/storage/L6C96VV7/Zhang et al. - 2023 - A Systematic Survey in Geometric Deep Learning for.pdf:application/pdf;arXiv.org Snapshot:/Users/jdomi/Zotero/storage/BN8YHF65/2306.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2306.11768
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2306.11768
      \endverb
      \keyw{Computer Science - Computational Engineering,Finance,and Science,Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods}
    \endentry
  \enddatalist
\endrefsection
\endinput

