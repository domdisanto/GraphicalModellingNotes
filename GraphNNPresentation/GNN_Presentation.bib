
@book{imperatorskaia_akademia_nauk_russia_commentarii_1726,
	title = {Commentarii {Academiae} scientiarum imperialis {Petropolitanae}},
	url = {http://archive.org/details/commentariiacade08impe},
	abstract = {Engraved title vignette},
	language = {lat},
	urldate = {2023-11-24},
	publisher = {Petropolis, Typis Academiae},
	author = {{Imperatorskaia akademia nauk (Russia)}},
	collaborator = {{American Museum of Natural History Library}},
	year = {1726},
}

@article{chandak_building_2023,
	title = {Building a knowledge graph to enable precision medicine},
	volume = {10},
	copyright = {2023 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-023-01960-3},
	doi = {10.1038/s41597-023-01960-3},
	abstract = {Developing personalized diagnostic strategies and targeted treatments requires a deep understanding of disease biology and the ability to dissect the relationship between molecular and genetic factors and their phenotypic consequences. However, such knowledge is fragmented across publications, non-standardized repositories, and evolving ontologies describing various scales of biological organization between genotypes and clinical phenotypes. Here, we present PrimeKG, a multimodal knowledge graph for precision medicine analyses. PrimeKG integrates 20 high-quality resources to describe 17,080 diseases with 4,050,249 relationships representing ten major biological scales, including disease-associated protein perturbations, biological processes and pathways, anatomical and phenotypic scales, and the entire range of approved drugs with their therapeutic action, considerably expanding previous efforts in disease-rooted knowledge graphs. PrimeKG contains an abundance of ‘indications’, ‘contradictions’, and ‘off-label use’ drug-disease edges that lack in other knowledge graphs and can support AI analyses of how drugs affect disease-associated networks. We supplement PrimeKG’s graph structure with language descriptions of clinical guidelines to enable multimodal analyses and provide instructions for continual updates of PrimeKG as new data become available.},
	language = {en},
	number = {1},
	urldate = {2023-11-26},
	journal = {Scientific Data},
	author = {Chandak, Payal and Huang, Kexin and Zitnik, Marinka},
	month = feb,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Data integration, Machine learning, Medical research, Network topology, Predictive medicine},
	pages = {67},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/EDBVQ4KP/Chandak et al. - 2023 - Building a knowledge graph to enable precision med.pdf:application/pdf},
}

@article{mcdermott_structure-inducing_2023,
	title = {Structure-inducing pre-training},
	volume = {5},
	copyright = {2023 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-023-00647-z},
	doi = {10.1038/s42256-023-00647-z},
	abstract = {Language model pre-training and the derived general-purpose methods have reshaped machine learning research. However, there remains considerable uncertainty regarding why pre-training improves the performance of downstream tasks. This challenge is pronounced when using language model pre-training in domains outside of natural language. Here we investigate this problem by analysing how pre-training methods impose relational structure in induced per-sample latent spaces—that is, what constraints do pre-training methods impose on the distance or geometry between the pre-trained embeddings of samples. A comprehensive review of pre-training methods reveals that this question remains open, despite theoretical analyses showing the importance of understanding this form of induced structure. Based on this review, we introduce a pre-training framework that enables a granular and comprehensive understanding of how relational structure can be induced. We present a theoretical analysis of the framework from the first principles and establish a connection between the relational inductive bias of pre-training and fine-tuning performance. Empirical studies spanning three data modalities and ten fine-tuning tasks confirm theoretical analyses, inform the design of novel pre-training methods and establish consistent improvements over a compelling suite of methods.},
	language = {en},
	number = {6},
	urldate = {2023-11-26},
	journal = {Nature Machine Intelligence},
	author = {McDermott, Matthew B. A. and Yap, Brendan and Szolovits, Peter and Zitnik, Marinka},
	month = jun,
	year = {2023},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {Computational models, Computer science, Machine learning, Statistics},
	pages = {612--621},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/L88JZQ9T/McDermott et al. - 2023 - Structure-inducing pre-training.pdf:application/pdf},
}

@article{ektefaie_multimodal_2023,
	title = {Multimodal learning with graphs},
	volume = {5},
	copyright = {2023 Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-023-00624-6},
	doi = {10.1038/s42256-023-00624-6},
	abstract = {Artificial intelligence for graphs has achieved remarkable success in modelling complex systems, ranging from dynamic networks in biology to interacting particle systems in physics. However, the increasingly heterogeneous graph datasets call for multimodal methods that can combine different inductive biases — assumptions that algorithms use to make predictions for inputs they have not encountered during training. Learning on multimodal datasets is challenging because the inductive biases can vary by data modality and graphs might not be explicitly given in the input. To address these challenges, graph artificial intelligence methods combine different modalities while leveraging cross-modal dependencies through geometric relationships. Diverse datasets are combined using graphs and fed into sophisticated multimodal architectures, specified as image-intensive, knowledge-grounded and language-intensive models. Using this categorization, we introduce a blueprint for multimodal graph learning, use it to study existing methods and provide guidelines to design new models.},
	language = {en},
	number = {4},
	urldate = {2023-11-26},
	journal = {Nature Machine Intelligence},
	author = {Ektefaie, Yasha and Dasoulas, George and Noori, Ayush and Farhat, Maha and Zitnik, Marinka},
	month = apr,
	year = {2023},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science, Machine learning, Statistics},
	pages = {340--350},
	file = {Submitted Version:/Users/jdomi/Zotero/storage/VWY2SA29/Ektefaie et al. - 2023 - Multimodal learning with graphs.pdf:application/pdf},
}

@misc{zhang_systematic_2023,
	title = {A {Systematic} {Survey} in {Geometric} {Deep} {Learning} for {Structure}-based {Drug} {Design}},
	url = {http://arxiv.org/abs/2306.11768},
	doi = {10.48550/arXiv.2306.11768},
	abstract = {Structure-based drug design (SBDD) utilizes the three-dimensional geometry of proteins to identify potential drug candidates. Traditional methods, grounded in physicochemical modeling and informed by domain expertise, are resource-intensive. Recent developments in geometric deep learning, focusing on the integration and processing of 3D geometric data, coupled with the availability of accurate protein 3D structure predictions from tools like AlphaFold, have greatly advanced the field of structure-based drug design. This paper systematically reviews the current state of geometric deep learning in SBDD. We first outline foundational tasks in SBDD, detail prevalent 3D protein representations, and highlight representative predictive and generative models. We then offer in-depth reviews of each key task, including binding site prediction, binding pose generation, {\textbackslash}emph\{de novo\} molecule generation, linker design, and binding affinity prediction. We provide formal problem definitions and outline each task's representative methods, datasets, evaluation metrics, and performance benchmarks. Finally, we summarize the current challenges and future opportunities: current challenges in SBDD include oversimplified problem formulations, inadequate out-of-distribution generalization, a lack of reliable evaluation metrics and large-scale benchmarks, and the need for experimental verification and enhanced model understanding; opportunities include leveraging multimodal datasets, integrating domain knowledge, building comprehensive benchmarks, designing criteria based on clinical endpoints, and developing foundation models that broaden the range of design tasks. We also curate {\textbackslash}url\{https://github.com/zaixizhang/Awesome-SBDD\}, reflecting ongoing contributions and new datasets in SBDD.},
	urldate = {2023-11-26},
	publisher = {arXiv},
	author = {Zhang, Zaixi and Yan, Jiaxian and Liu, Qi and Chen, Enhong and Zitnik, Marinka},
	month = oct,
	year = {2023},
	note = {arXiv:2306.11768 [cs, q-bio]},
	keywords = {Computer Science - Computational Engineering, Finance, and Science, Computer Science - Machine Learning, Quantitative Biology - Quantitative Methods},
	annote = {Comment: 20 pages, under review},
	file = {arXiv Fulltext PDF:/Users/jdomi/Zotero/storage/L6C96VV7/Zhang et al. - 2023 - A Systematic Survey in Geometric Deep Learning for.pdf:application/pdf;arXiv.org Snapshot:/Users/jdomi/Zotero/storage/BN8YHF65/2306.html:text/html},
}

@article{li_graph_2022,
	title = {Graph representation learning in biomedicine and healthcare},
	volume = {6},
	issn = {2157-846X},
	doi = {10.1038/s41551-022-00942-x},
	abstract = {Networks-or graphs-are universal descriptors of systems of interacting elements. In biomedicine and healthcare, they can represent, for example, molecular interactions, signalling pathways, disease co-morbidities or healthcare systems. In this Perspective, we posit that representation learning can realize principles of network medicine, discuss successes and current limitations of the use of representation learning on graphs in biomedicine and healthcare, and outline algorithmic strategies that leverage the topology of graphs to embed them into compact vectorial spaces. We argue that graph representation learning will keep pushing forward machine learning for biomedicine and healthcare applications, including the identification of genetic variants underlying complex traits, the disentanglement of single-cell behaviours and their effects on health, the assistance of patients in diagnosis and treatment, and the development of safe and effective medicines.},
	language = {eng},
	number = {12},
	journal = {Nature Biomedical Engineering},
	author = {Li, Michelle M. and Huang, Kexin and Zitnik, Marinka},
	month = dec,
	year = {2022},
	pmid = {36316368},
	keywords = {Delivery of Health Care, Humans, Machine Learning},
	pages = {1353--1369},
}

@article{sanchez-lengeling_gentle_2021,
	title = {A {Gentle} {Introduction} to {Graph} {Neural} {Networks}},
	volume = {6},
	issn = {2476-0757},
	url = {https://distill.pub/2021/gnn-intro},
	doi = {10.23915/distill.00033},
	abstract = {What components are needed for building learning algorithms that leverage the structure and properties of graphs?},
	language = {en},
	number = {9},
	urldate = {2023-11-27},
	journal = {Distill},
	author = {Sanchez-Lengeling, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alexander B.},
	month = sep,
	year = {2021},
	pages = {e33},
}

@misc{xu_how_2019,
	title = {How {Powerful} are {Graph} {Neural} {Networks}?},
	url = {http://arxiv.org/abs/1810.00826},
	doi = {10.48550/arXiv.1810.00826},
	abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
	month = feb,
	year = {2019},
	note = {arXiv:1810.00826 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jdomi/Zotero/storage/MX7SILVL/Xu et al. - 2019 - How Powerful are Graph Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/jdomi/Zotero/storage/9FD66I8S/1810.html:text/html},
}

@misc{kipf_semi-supervised_2017,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = feb,
	year = {2017},
	note = {arXiv:1609.02907 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published as a conference paper at ICLR 2017},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/IJ8MTNEG/1609.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/AY9ETCPZ/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf:application/pdf},
}
