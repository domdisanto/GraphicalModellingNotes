
@book{imperatorskaia_akademia_nauk_russia_commentarii_1726,
	title = {Commentarii {Academiae} scientiarum imperialis {Petropolitanae}},
	url = {http://archive.org/details/commentariiacade08impe},
	abstract = {Engraved title vignette},
	language = {lat},
	urldate = {2023-11-24},
	publisher = {Petropolis, Typis Academiae},
	author = {{Imperatorskaia akademia nauk (Russia)}},
	collaborator = {{American Museum of Natural History Library}},
	year = {1726},
}

@article{chandak_building_2023,
	title = {Building a knowledge graph to enable precision medicine},
	volume = {10},
	copyright = {2023 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-023-01960-3},
	doi = {10.1038/s41597-023-01960-3},
	abstract = {Developing personalized diagnostic strategies and targeted treatments requires a deep understanding of disease biology and the ability to dissect the relationship between molecular and genetic factors and their phenotypic consequences. However, such knowledge is fragmented across publications, non-standardized repositories, and evolving ontologies describing various scales of biological organization between genotypes and clinical phenotypes. Here, we present PrimeKG, a multimodal knowledge graph for precision medicine analyses. PrimeKG integrates 20 high-quality resources to describe 17,080 diseases with 4,050,249 relationships representing ten major biological scales, including disease-associated protein perturbations, biological processes and pathways, anatomical and phenotypic scales, and the entire range of approved drugs with their therapeutic action, considerably expanding previous efforts in disease-rooted knowledge graphs. PrimeKG contains an abundance of ‘indications’, ‘contradictions’, and ‘off-label use’ drug-disease edges that lack in other knowledge graphs and can support AI analyses of how drugs affect disease-associated networks. We supplement PrimeKG’s graph structure with language descriptions of clinical guidelines to enable multimodal analyses and provide instructions for continual updates of PrimeKG as new data become available.},
	language = {en},
	number = {1},
	urldate = {2023-11-26},
	journal = {Scientific Data},
	author = {Chandak, Payal and Huang, Kexin and Zitnik, Marinka},
	month = feb,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Data integration, Medical research, Network topology, Predictive medicine},
	pages = {67},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/EDBVQ4KP/Chandak et al. - 2023 - Building a knowledge graph to enable precision med.pdf:application/pdf},
}

@article{mcdermott_structure-inducing_2023,
	title = {Structure-inducing pre-training},
	volume = {5},
	copyright = {2023 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-023-00647-z},
	doi = {10.1038/s42256-023-00647-z},
	abstract = {Language model pre-training and the derived general-purpose methods have reshaped machine learning research. However, there remains considerable uncertainty regarding why pre-training improves the performance of downstream tasks. This challenge is pronounced when using language model pre-training in domains outside of natural language. Here we investigate this problem by analysing how pre-training methods impose relational structure in induced per-sample latent spaces—that is, what constraints do pre-training methods impose on the distance or geometry between the pre-trained embeddings of samples. A comprehensive review of pre-training methods reveals that this question remains open, despite theoretical analyses showing the importance of understanding this form of induced structure. Based on this review, we introduce a pre-training framework that enables a granular and comprehensive understanding of how relational structure can be induced. We present a theoretical analysis of the framework from the first principles and establish a connection between the relational inductive bias of pre-training and fine-tuning performance. Empirical studies spanning three data modalities and ten fine-tuning tasks confirm theoretical analyses, inform the design of novel pre-training methods and establish consistent improvements over a compelling suite of methods.},
	language = {en},
	number = {6},
	urldate = {2023-11-26},
	journal = {Nature Machine Intelligence},
	author = {McDermott, Matthew B. A. and Yap, Brendan and Szolovits, Peter and Zitnik, Marinka},
	month = jun,
	year = {2023},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Computational models, Computer science, Statistics},
	pages = {612--621},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/L88JZQ9T/McDermott et al. - 2023 - Structure-inducing pre-training.pdf:application/pdf},
}

@article{ektefaie_multimodal_2023,
	title = {Multimodal learning with graphs},
	volume = {5},
	copyright = {2023 Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-023-00624-6},
	doi = {10.1038/s42256-023-00624-6},
	abstract = {Artificial intelligence for graphs has achieved remarkable success in modelling complex systems, ranging from dynamic networks in biology to interacting particle systems in physics. However, the increasingly heterogeneous graph datasets call for multimodal methods that can combine different inductive biases — assumptions that algorithms use to make predictions for inputs they have not encountered during training. Learning on multimodal datasets is challenging because the inductive biases can vary by data modality and graphs might not be explicitly given in the input. To address these challenges, graph artificial intelligence methods combine different modalities while leveraging cross-modal dependencies through geometric relationships. Diverse datasets are combined using graphs and fed into sophisticated multimodal architectures, specified as image-intensive, knowledge-grounded and language-intensive models. Using this categorization, we introduce a blueprint for multimodal graph learning, use it to study existing methods and provide guidelines to design new models.},
	language = {en},
	number = {4},
	urldate = {2023-11-26},
	journal = {Nature Machine Intelligence},
	author = {Ektefaie, Yasha and Dasoulas, George and Noori, Ayush and Farhat, Maha and Zitnik, Marinka},
	month = apr,
	year = {2023},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Computer science, Statistics, Computational science},
	pages = {340--350},
	file = {Submitted Version:/Users/jdomi/Zotero/storage/VWY2SA29/Ektefaie et al. - 2023 - Multimodal learning with graphs.pdf:application/pdf},
}

@misc{zhang_systematic_2023,
	title = {A {Systematic} {Survey} in {Geometric} {Deep} {Learning} for {Structure}-based {Drug} {Design}},
	url = {http://arxiv.org/abs/2306.11768},
	doi = {10.48550/arXiv.2306.11768},
	abstract = {Structure-based drug design (SBDD) utilizes the three-dimensional geometry of proteins to identify potential drug candidates. Traditional methods, grounded in physicochemical modeling and informed by domain expertise, are resource-intensive. Recent developments in geometric deep learning, focusing on the integration and processing of 3D geometric data, coupled with the availability of accurate protein 3D structure predictions from tools like AlphaFold, have greatly advanced the field of structure-based drug design. This paper systematically reviews the current state of geometric deep learning in SBDD. We first outline foundational tasks in SBDD, detail prevalent 3D protein representations, and highlight representative predictive and generative models. We then offer in-depth reviews of each key task, including binding site prediction, binding pose generation, {\textbackslash}emph\{de novo\} molecule generation, linker design, and binding affinity prediction. We provide formal problem definitions and outline each task's representative methods, datasets, evaluation metrics, and performance benchmarks. Finally, we summarize the current challenges and future opportunities: current challenges in SBDD include oversimplified problem formulations, inadequate out-of-distribution generalization, a lack of reliable evaluation metrics and large-scale benchmarks, and the need for experimental verification and enhanced model understanding; opportunities include leveraging multimodal datasets, integrating domain knowledge, building comprehensive benchmarks, designing criteria based on clinical endpoints, and developing foundation models that broaden the range of design tasks. We also curate {\textbackslash}url\{https://github.com/zaixizhang/Awesome-SBDD\}, reflecting ongoing contributions and new datasets in SBDD.},
	urldate = {2023-11-26},
	publisher = {arXiv},
	author = {Zhang, Zaixi and Yan, Jiaxian and Liu, Qi and Chen, Enhong and Zitnik, Marinka},
	month = oct,
	year = {2023},
	note = {arXiv:2306.11768 [cs, q-bio]},
	keywords = {Computer Science - Computational Engineering, Finance, and Science, Computer Science - Machine Learning, Quantitative Biology - Quantitative Methods},
	annote = {Comment: 20 pages, under review},
	file = {arXiv Fulltext PDF:/Users/jdomi/Zotero/storage/L6C96VV7/Zhang et al. - 2023 - A Systematic Survey in Geometric Deep Learning for.pdf:application/pdf;arXiv.org Snapshot:/Users/jdomi/Zotero/storage/BN8YHF65/2306.html:text/html},
}

@article{li_graph_2022,
	title = {Graph representation learning in biomedicine and healthcare},
	volume = {6},
	issn = {2157-846X},
	doi = {10.1038/s41551-022-00942-x},
	abstract = {Networks-or graphs-are universal descriptors of systems of interacting elements. In biomedicine and healthcare, they can represent, for example, molecular interactions, signalling pathways, disease co-morbidities or healthcare systems. In this Perspective, we posit that representation learning can realize principles of network medicine, discuss successes and current limitations of the use of representation learning on graphs in biomedicine and healthcare, and outline algorithmic strategies that leverage the topology of graphs to embed them into compact vectorial spaces. We argue that graph representation learning will keep pushing forward machine learning for biomedicine and healthcare applications, including the identification of genetic variants underlying complex traits, the disentanglement of single-cell behaviours and their effects on health, the assistance of patients in diagnosis and treatment, and the development of safe and effective medicines.},
	language = {eng},
	number = {12},
	journal = {Nature Biomedical Engineering},
	author = {Li, Michelle M. and Huang, Kexin and Zitnik, Marinka},
	month = dec,
	year = {2022},
	pmid = {36316368},
	keywords = {Humans, Delivery of Health Care, Machine Learning},
	pages = {1353--1369},
}

@article{sanchez-lengeling_gentle_2021,
	title = {A {Gentle} {Introduction} to {Graph} {Neural} {Networks}},
	volume = {6},
	issn = {2476-0757},
	url = {https://distill.pub/2021/gnn-intro},
	doi = {10.23915/distill.00033},
	abstract = {What components are needed for building learning algorithms that leverage the structure and properties of graphs?},
	language = {en},
	number = {9},
	urldate = {2023-11-27},
	journal = {Distill},
	author = {Sanchez-Lengeling, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alexander B.},
	month = sep,
	year = {2021},
	pages = {e33},
}

@misc{xu_how_2019,
	title = {How {Powerful} are {Graph} {Neural} {Networks}?},
	url = {http://arxiv.org/abs/1810.00826},
	doi = {10.48550/arXiv.1810.00826},
	abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
	month = feb,
	year = {2019},
	note = {arXiv:1810.00826 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jdomi/Zotero/storage/MX7SILVL/Xu et al. - 2019 - How Powerful are Graph Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/jdomi/Zotero/storage/9FD66I8S/1810.html:text/html},
}

@misc{kipf_semi-supervised_2017,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = feb,
	year = {2017},
	note = {arXiv:1609.02907 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published as a conference paper at ICLR 2017},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/IJ8MTNEG/1609.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/AY9ETCPZ/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf:application/pdf},
}

@article{scarselli_graph_2009,
	title = {The {Graph} {Neural} {Network} {Model}},
	volume = {20},
	issn = {1045-9227, 1941-0093},
	url = {http://ieeexplore.ieee.org/document/4700287/},
	doi = {10.1109/TNN.2008.2005605},
	language = {en},
	number = {1},
	urldate = {2023-11-27},
	journal = {IEEE Transactions on Neural Networks},
	author = {Scarselli, F. and Gori, M. and {Ah Chung Tsoi} and Hagenbuchner, M. and Monfardini, G.},
	month = jan,
	year = {2009},
	pages = {61--80},
	file = {Scarselli et al. - 2009 - The Graph Neural Network Model.pdf:/Users/jdomi/Zotero/storage/DLKGJ92Q/Scarselli et al. - 2009 - The Graph Neural Network Model.pdf:application/pdf},
}

@misc{gilmer_neural_2017,
	title = {Neural {Message} {Passing} for {Quantum} {Chemistry}},
	url = {http://arxiv.org/abs/1704.01212},
	abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
	month = jun,
	year = {2017},
	note = {arXiv:1704.01212 [cs]},
	keywords = {Computer Science - Machine Learning, I.2.6},
	annote = {Comment: 14 pages},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/BFUQJEMC/1704.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/5SHINSLY/Gilmer et al. - 2017 - Neural Message Passing for Quantum Chemistry.pdf:application/pdf},
}

@misc{duvenaud_convolutional_2015,
	title = {Convolutional {Networks} on {Graphs} for {Learning} {Molecular} {Fingerprints}},
	url = {http://arxiv.org/abs/1509.09292},
	doi = {10.48550/arXiv.1509.09292},
	abstract = {We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Duvenaud, David and Maclaurin, Dougal and Aguilera-Iparraguirre, Jorge and Gómez-Bombarelli, Rafael and Hirzel, Timothy and Aspuru-Guzik, Alán and Adams, Ryan P.},
	month = nov,
	year = {2015},
	note = {arXiv:1509.09292 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 9 pages, 5 figures. To appear in Neural Information Processing Systems (NIPS)},
	file = {arXiv Fulltext PDF:/Users/jdomi/Zotero/storage/25EMKK23/Duvenaud et al. - 2015 - Convolutional Networks on Graphs for Learning Mole.pdf:application/pdf;arXiv.org Snapshot:/Users/jdomi/Zotero/storage/FSXKHGU9/1509.html:text/html},
}

@misc{bronstein_geometric_2021,
	title = {Geometric {Deep} {Learning}: {Grids}, {Groups}, {Graphs}, {Geodesics}, and {Gauges}},
	shorttitle = {Geometric {Deep} {Learning}},
	url = {http://arxiv.org/abs/2104.13478},
	abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
	month = may,
	year = {2021},
	note = {arXiv:2104.13478 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computational Geometry},
	annote = {Comment: 156 pages. Work in progress -- comments welcome!},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/DXD5RI9V/2104.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/EZNMD8TF/Bronstein et al. - 2021 - Geometric Deep Learning Grids, Groups, Graphs, Ge.pdf:application/pdf},
}

@misc{buterez_graph_2022,
	title = {Graph {Neural} {Networks} with {Adaptive} {Readouts}},
	url = {http://arxiv.org/abs/2211.04952},
	abstract = {An effective aggregation of node features into a graph-level representation via readout functions is an essential step in numerous learning tasks involving graph neural networks. Typically, readouts are simple and non-adaptive functions designed such that the resulting hypothesis space is permutation invariant. Prior work on deep sets indicates that such readouts might require complex node embeddings that can be difficult to learn via standard neighborhood aggregation schemes. Motivated by this, we investigate the potential of adaptive readouts given by neural networks that do not necessarily give rise to permutation invariant hypothesis spaces. We argue that in some problems such as binding affinity prediction where molecules are typically presented in a canonical form it might be possible to relax the constraints on permutation invariance of the hypothesis space and learn a more effective model of the affinity by employing an adaptive readout function. Our empirical results demonstrate the effectiveness of neural readouts on more than 40 datasets spanning different domains and graph characteristics. Moreover, we observe a consistent improvement over standard readouts (i.e., sum, max, and mean) relative to the number of neighborhood aggregation iterations and different convolutional operators.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Buterez, David and Janet, Jon Paul and Kiddle, Steven J. and Oglic, Dino and Liò, Pietro},
	month = nov,
	year = {2022},
	note = {arXiv:2211.04952 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Published at NeurIPS 2022. 10 pages, 5 figures, 1 table},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/B8D23KP8/2211.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/JWLWRIMK/Buterez et al. - 2022 - Graph Neural Networks with Adaptive Readouts.pdf:application/pdf},
}

@misc{ferludin_tf-gnn_2023,
	title = {{TF}-{GNN}: {Graph} {Neural} {Networks} in {TensorFlow}},
	shorttitle = {{TF}-{GNN}},
	url = {http://arxiv.org/abs/2207.03522},
	doi = {10.48550/arXiv.2207.03522},
	abstract = {TensorFlow-GNN (TF-GNN) is a scalable library for Graph Neural Networks in TensorFlow. It is designed from the bottom up to support the kinds of rich heterogeneous graph data that occurs in today's information ecosystems. In addition to enabling machine learning researchers and advanced developers, TF-GNN offers low-code solutions to empower the broader developer community in graph learning. Many production models at Google use TF-GNN, and it has been recently released as an open source project. In this paper we describe the TF-GNN data model, its Keras message passing API, and relevant capabilities such as graph sampling and distributed training.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Ferludin, Oleksandr and Eigenwillig, Arno and Blais, Martin and Zelle, Dustin and Pfeifer, Jan and Sanchez-Gonzalez, Alvaro and Li, Wai Lok Sibon and Abu-El-Haija, Sami and Battaglia, Peter and Bulut, Neslihan and Halcrow, Jonathan and de Almeida, Filipe Miguel Gonçalves and Gonnet, Pedro and Jiang, Liangze and Kothari, Parth and Lattanzi, Silvio and Linhares, André and Mayer, Brandon and Mirrokni, Vahab and Palowitch, John and Paradkar, Mihir and She, Jennifer and Tsitsulin, Anton and Villela, Kevin and Wang, Lisa and Wong, David and Perozzi, Bryan},
	month = jul,
	year = {2023},
	note = {arXiv:2207.03522 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Social and Information Networks, Physics - Physics and Society},
	file = {arXiv Fulltext PDF:/Users/jdomi/Zotero/storage/5GCCG9V5/Ferludin et al. - 2023 - TF-GNN Graph Neural Networks in TensorFlow.pdf:application/pdf;arXiv.org Snapshot:/Users/jdomi/Zotero/storage/8GXFP7QQ/2207.html:text/html},
}

@inproceedings{gori_new_2005,
	title = {A new model for learning in graph domains},
	volume = {2},
	url = {https://ieeexplore.ieee.org/document/1555942},
	doi = {10.1109/IJCNN.2005.1555942},
	abstract = {In several applications the information is naturally represented by graphs. Traditional approaches cope with graphical data structures using a preprocessing phase which transforms the graphs into a set of flat vectors. However, in this way, important topological information may be lost and the achieved results may heavily depend on the preprocessing stage. This paper presents a new neural model, called graph neural network (GNN), capable of directly processing graphs. GNNs extends recursive neural networks and can be applied on most of the practically useful kinds of graphs, including directed, undirected, labelled and cyclic graphs. A learning algorithm for GNNs is proposed and some experiments are discussed which assess the properties of the model.},
	urldate = {2023-11-28},
	booktitle = {Proceedings. 2005 {IEEE} {International} {Joint} {Conference} on {Neural} {Networks}, 2005.},
	author = {Gori, M. and Monfardini, G. and Scarselli, F.},
	month = jul,
	year = {2005},
	note = {ISSN: 2161-4407},
	pages = {729--734 vol. 2},
	file = {IEEE Xplore Abstract Record:/Users/jdomi/Zotero/storage/GTX6ZF9N/1555942.html:text/html},
}

@inproceedings{ying_hierarchical_2018,
	title = {Hierarchical {Graph} {Representation} {Learning} with {Differentiable} {Pooling}},
	volume = {31},
	url = {https://papers.nips.cc/paper_files/paper/2018/hash/e77dbaf6759253c7c6d0efc5690369c7-Abstract.html},
	abstract = {Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10\% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark datasets.},
	urldate = {2023-11-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ying, Zhitao and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, Will and Leskovec, Jure},
	year = {2018},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/F9ECRZCH/Ying et al. - 2018 - Hierarchical Graph Representation Learning with Di.pdf:application/pdf},
}

@misc{hamilton_inductive_2018,
	title = {Inductive {Representation} {Learning} on {Large} {Graphs}},
	url = {http://arxiv.org/abs/1706.02216},
	abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	month = sep,
	year = {2018},
	note = {arXiv:1706.02216 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks},
	annote = {Comment: Published in NIPS 2017; version with full appendix and minor corrections},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/A7VN75B4/1706.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/5R6EYCZA/Hamilton et al. - 2018 - Inductive Representation Learning on Large Graphs.pdf:application/pdf},
}

@inproceedings{ying_hierarchical_2018-1,
	title = {Hierarchical {Graph} {Representation} {Learning} with {Differentiable} {Pooling}},
	volume = {31},
	url = {https://papers.nips.cc/paper_files/paper/2018/hash/e77dbaf6759253c7c6d0efc5690369c7-Abstract.html},
	abstract = {Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10\% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark datasets.},
	urldate = {2023-11-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ying, Zhitao and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, Will and Leskovec, Jure},
	year = {2018},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/MS3YB3G9/Ying et al. - 2018 - Hierarchical Graph Representation Learning with Di.pdf:application/pdf},
}

@misc{he_pytorch_2023,
	title = {{PyTorch} {Geometric} {Signed} {Directed}: {A} {Software} {Package} on {Graph} {Neural} {Networks} for {Signed} and {Directed} {Graphs}},
	shorttitle = {{PyTorch} {Geometric} {Signed} {Directed}},
	url = {http://arxiv.org/abs/2202.10793},
	abstract = {Networks are ubiquitous in many real-world applications (e.g., social networks encoding trust/distrust relationships, correlation networks arising from time series data). While many networks are signed or directed, or both, there is a lack of unified software packages on graph neural networks (GNNs) specially designed for signed and directed networks. In this paper, we present PyTorch Geometric Signed Directed (PyGSD), a software package which fills this gap. Along the way, we evaluate the implemented methods with experiments with a view to providing insights into which method to choose for a given task. The deep learning framework consists of easy-to-use GNN models, synthetic and real-world data, as well as task-specific evaluation metrics and loss functions for signed and directed networks. As an extension library for PyG, our proposed software is maintained with open-source releases, detailed documentation, continuous integration, unit tests and code coverage checks. The GitHub repository of the library is https://github.com/SherylHYX/pytorch\_geometric\_signed\_directed.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {He, Yixuan and Zhang, Xitong and Huang, Junjie and Rozemberczki, Benedek and Cucuringu, Mihai and Reinert, Gesine},
	month = nov,
	year = {2023},
	note = {arXiv:2202.10793 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Social and Information Networks},
	annote = {Comment: Accepted by LoG 2023. 27 pages in total},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/RGAM5P3H/2202.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/H3CSXYAZ/He et al. - 2023 - PyTorch Geometric Signed Directed A Software Pack.pdf:application/pdf},
}

@inproceedings{duvenaud_convolutional_2015-1,
	title = {Convolutional {Networks} on {Graphs} for {Learning} {Molecular} {Fingerprints}},
	volume = {28},
	url = {https://papers.nips.cc/paper_files/paper/2015/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html},
	abstract = {We introduce a convolutional neural network that operates directly on graphs.These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape.The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints.We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
	urldate = {2023-11-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Duvenaud, David K and Maclaurin, Dougal and Iparraguirre, Jorge and Bombarell, Rafael and Hirzel, Timothy and Aspuru-Guzik, Alan and Adams, Ryan P},
	year = {2015},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/82S4LL9Y/Duvenaud et al. - 2015 - Convolutional Networks on Graphs for Learning Mole.pdf:application/pdf},
}

@inproceedings{defferrard_convolutional_2016,
	title = {Convolutional {Neural} {Networks} on {Graphs} with {Fast} {Localized} {Spectral} {Filtering}},
	volume = {29},
	url = {https://papers.nips.cc/paper_files/paper/2016/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html},
	abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words’ embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
	urldate = {2023-11-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Defferrard, Michaël and Bresson, Xavier and Vandergheynst, Pierre},
	year = {2016},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/NKKSTN22/Defferrard et al. - 2016 - Convolutional Neural Networks on Graphs with Fast .pdf:application/pdf},
}

@misc{bruna_spectral_2014,
	title = {Spectral {Networks} and {Locally} {Connected} {Networks} on {Graphs}},
	url = {http://arxiv.org/abs/1312.6203},
	abstract = {Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Bruna, Joan and Zaremba, Wojciech and Szlam, Arthur and LeCun, Yann},
	month = may,
	year = {2014},
	note = {arXiv:1312.6203 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 14 pages},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/R4IYDNIG/1312.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/UJVTV3QD/Bruna et al. - 2014 - Spectral Networks and Locally Connected Networks o.pdf:application/pdf},
}

@misc{javaloy_mitigating_2022,
	title = {Mitigating {Modality} {Collapse} in {Multimodal} {VAEs} via {Impartial} {Optimization}},
	url = {http://arxiv.org/abs/2206.04496},
	abstract = {A number of variational autoencoders (VAEs) have recently emerged with the aim of modeling multimodal data, e.g., to jointly model images and their corresponding captions. Still, multimodal VAEs tend to focus solely on a subset of the modalities, e.g., by fitting the image while neglecting the caption. We refer to this limitation as modality collapse. In this work, we argue that this effect is a consequence of conflicting gradients during multimodal VAE training. We show how to detect the sub-graphs in the computational graphs where gradients conflict (impartiality blocks), as well as how to leverage existing gradient-conflict solutions from multitask learning to mitigate modality collapse. That is, to ensure impartial optimization across modalities. We apply our training framework to several multimodal VAE models, losses and datasets from the literature, and empirically show that our framework significantly improves the reconstruction performance, conditional generation, and coherence of the latent space across modalities.},
	language = {en},
	urldate = {2023-11-29},
	publisher = {arXiv},
	author = {Javaloy, Adrián and Meghdadi, Maryam and Valera, Isabel},
	month = jun,
	year = {2022},
	note = {arXiv:2206.04496 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Accepted as a Spotlight paper at ICML 2022. 27 pages, 10 figures},
}

@misc{javaloy_mitigating_2022-1,
	title = {Mitigating {Modality} {Collapse} in {Multimodal} {VAEs} via {Impartial} {Optimization}},
	url = {https://arxiv.org/abs/2206.04496v1},
	abstract = {A number of variational autoencoders (VAEs) have recently emerged with the aim of modeling multimodal data, e.g., to jointly model images and their corresponding captions. Still, multimodal VAEs tend to focus solely on a subset of the modalities, e.g., by fitting the image while neglecting the caption. We refer to this limitation as modality collapse. In this work, we argue that this effect is a consequence of conflicting gradients during multimodal VAE training. We show how to detect the sub-graphs in the computational graphs where gradients conflict (impartiality blocks), as well as how to leverage existing gradient-conflict solutions from multitask learning to mitigate modality collapse. That is, to ensure impartial optimization across modalities. We apply our training framework to several multimodal VAE models, losses and datasets from the literature, and empirically show that our framework significantly improves the reconstruction performance, conditional generation, and coherence of the latent space across modalities.},
	language = {en},
	urldate = {2023-11-29},
	journal = {arXiv.org},
	author = {Javaloy, Adrián and Meghdadi, Maryam and Valera, Isabel},
	month = jun,
	year = {2022},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/X3NAU3AQ/Javaloy et al. - 2022 - Mitigating Modality Collapse in Multimodal VAEs vi.pdf:application/pdf},
}

@article{batool_structure-based_2019,
	title = {A {Structure}-{Based} {Drug} {Discovery} {Paradigm}},
	volume = {20},
	issn = {1422-0067},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6601033/},
	doi = {10.3390/ijms20112783},
	abstract = {Structure-based drug design is becoming an essential tool for faster and more cost-efficient lead discovery relative to the traditional method. Genomic, proteomic, and structural studies have provided hundreds of new targets and opportunities for future drug discovery. This situation poses a major problem: the necessity to handle the “big data” generated by combinatorial chemistry. Artificial intelligence (AI) and deep learning play a pivotal role in the analysis and systemization of larger data sets by statistical machine learning methods. Advanced AI-based sophisticated machine learning tools have a significant impact on the drug discovery process including medicinal chemistry. In this review, we focus on the currently available methods and algorithms for structure-based drug design including virtual screening and de novo drug design, with a special emphasis on AI- and deep-learning-based methods used for drug discovery.},
	number = {11},
	urldate = {2023-11-29},
	journal = {International Journal of Molecular Sciences},
	author = {Batool, Maria and Ahmad, Bilal and Choi, Sangdun},
	month = jun,
	year = {2019},
	pmid = {31174387},
	pmcid = {PMC6601033},
	pages = {2783},
	file = {PubMed Central Full Text PDF:/Users/jdomi/Zotero/storage/8DWP6XMK/Batool et al. - 2019 - A Structure-Based Drug Discovery Paradigm.pdf:application/pdf},
}

@article{gainza_deciphering_2020,
	title = {Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning},
	volume = {17},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-019-0666-6},
	doi = {10.1038/s41592-019-0666-6},
	abstract = {Predicting interactions between proteins and other biomolecules solely based on structure remains a challenge in biology. A high-level representation of protein structure, the molecular surface, displays patterns of chemical and geometric features that fingerprint a protein’s modes of interactions with other biomolecules. We hypothesize that proteins participating in similar interactions may share common fingerprints, independent of their evolutionary history. Fingerprints may be difficult to grasp by visual analysis but could be learned from large-scale datasets. We present MaSIF (molecular surface interaction fingerprinting), a conceptual framework based on a geometric deep learning method to capture fingerprints that are important for specific biomolecular interactions. We showcase MaSIF with three prediction challenges: protein pocket-ligand prediction, protein–protein interaction site prediction and ultrafast scanning of protein surfaces for prediction of protein–protein complexes. We anticipate that our conceptual framework will lead to improvements in our understanding of protein function and design.},
	language = {en},
	number = {2},
	urldate = {2023-11-29},
	journal = {Nature Methods},
	author = {Gainza, P. and Sverrisson, F. and Monti, F. and Rodolà, E. and Boscaini, D. and Bronstein, M. M. and Correia, B. E.},
	month = feb,
	year = {2020},
	note = {Number: 2
Publisher: Nature Publishing Group},
	keywords = {Proteins, Machine learning, Protein function predictions, Protein structure predictions},
	pages = {184--192},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/MSR9WTZY/Gainza et al. - 2020 - Deciphering interaction fingerprints from protein .pdf:application/pdf},
}

@article{sanner_reduced_1996,
	title = {Reduced surface: an efficient way to compute molecular surfaces},
	volume = {38},
	issn = {0006-3525},
	shorttitle = {Reduced surface},
	doi = {10.1002/(SICI)1097-0282(199603)38:3%3C305::AID-BIP4%3E3.0.CO;2-Y},
	abstract = {Because of their wide use in molecular modeling, methods to compute molecular surfaces have received a lot of interest in recent years. However, most of the proposed algorithms compute the analytical representation of only the solvent-accessible surface. There are a few programs that compute the analytical representation of the solvent-excluded surface, but they often have problems handling singular cases of self-intersecting surfaces and tend to fail on large molecules (more than 10,000 atoms). We describe here a program called MSMS, which is shown to be fast and reliable in computing molecular surfaces. It relies on the use of the reduced surface that is briefly defined here and from which the solvent-accessible and solvent-excluded surfaces are computed. The four algorithms composing MSMS are described and their complexity is analyzed. Special attention is given to the handling of self-intersecting parts of the solvent-excluded surface called singularities. The program has been compared with Connolly's program PQMS [M.L. Connolly (1993) Journal of Molecular Graphics, Vol. 11, pp. 139-141] on a set of 709 molecules taken from the Brookhaven Data Base. MSMS was able to compute topologically correct surfaces for each molecule in the set. Moreover, the actual time spent to compute surfaces is in agreement with the theoretical complexity of the program, which is shown to be O[n log(n)] for n atoms. On a Hewlett-Packard 9000/735 workstation, MSMS takes 0.73 s to produce a triangulated solvent-excluded surface for crambin (1 crn, 46 residues, 327 atoms, 4772 triangles), 4.6 s for thermolysin (3tln, 316 residues, 2437 atoms, 26462 triangles), and 104.53 s for glutamine synthetase (2gls, 5676 residues, 43632 atoms, 476665 triangles).},
	language = {eng},
	number = {3},
	journal = {Biopolymers},
	author = {Sanner, M. F. and Olson, A. J. and Spehner, J. C.},
	month = mar,
	year = {1996},
	pmid = {8906967},
	keywords = {Software, Chemistry, Physical, Computer Simulation, Models, Chemical, Surface Properties},
	pages = {305--320},
}

@article{isert_structure-based_2023,
	title = {Structure-based drug design with geometric deep learning},
	volume = {79},
	issn = {0959440X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0959440X23000222},
	doi = {10.1016/j.sbi.2023.102548},
	abstract = {Structure-based drug design uses three-dimensional geometric information of macromolecules, such as proteins or nucleic acids, to identify suitable ligands. Geometric deep learning, an emerging concept of neural-network-based machine learning, has been applied to macromolecular structures. This review provides an overview of the recent applications of geometric deep learning in bioorganic and medicinal chemistry, highlighting its potential for structurebased drug discovery and design. Emphasis is placed on molecular property prediction, ligand binding site and pose prediction, and structure-based de novo molecular design. The current challenges and opportunities are highlighted, and a forecast of the future of geometric deep learning for drug discovery is presented.},
	language = {en},
	urldate = {2023-12-05},
	journal = {Current Opinion in Structural Biology},
	author = {Isert, Clemens and Atz, Kenneth and Schneider, Gisbert},
	month = apr,
	year = {2023},
	pages = {102548},
	file = {Isert et al. - 2023 - Structure-based drug design with geometric deep le.pdf:/Users/jdomi/Zotero/storage/X37AEHGN/Isert et al. - 2023 - Structure-based drug design with geometric deep le.pdf:application/pdf},
}

@misc{abdollahi_nodecoder_2023,
	title = {{NodeCoder}: a graph-based machine learning platform to predict active sites of modeled protein structures},
	shorttitle = {{NodeCoder}},
	url = {http://arxiv.org/abs/2302.03590},
	abstract = {While accurate protein structure predictions are now available for nearly every observed protein sequence, predicted structures lack much of the functional context offered by experimental structure determination. We address this gap with NodeCoder, a task-independent platform that maps residue-based datasets onto 3D protein structures, embeds the resulting structural feature into a contact network, and models residue classification tasks with a Graph Convolutional Network (GCN). We demonstrate the versatility of this strategy by modeling six separate tasks, with some labels derived from other experimental structure studies (ligand, peptide, ion, and nucleic acid binding sites) and other labels derived from annotation databases (post-translational modification and transmembrane regions). Moreover, A NodeCoder model trained to identify ligand binding site residues was able to outperform P2Rank, a widely-used software developed specifically for ligand binding site detection. NodeCoder is available as an open-source python package at https://pypi.org/project/NodeCoder/.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Abdollahi, Nasim and Tonekaboni, Seyed Ali Madani and Huang, Jay and Wang, Bo and MacKinnon, Stephen},
	month = feb,
	year = {2023},
	note = {arXiv:2302.03590 [q-bio]},
	keywords = {Quantitative Biology - Quantitative Methods},
	annote = {Comment: including supplementary materials 22 pages, 6 figures, 4 tables, presented at NeurIPS 2021 and ACS 2022},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/YAJI5RE9/2302.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/L5EGQRFI/Abdollahi et al. - 2023 - NodeCoder a graph-based machine learning platform.pdf:application/pdf},
}

@misc{satorras_en_2022,
	title = {E(n) {Equivariant} {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2102.09844},
	abstract = {This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Satorras, Victor Garcia and Hoogeboom, Emiel and Welling, Max},
	month = feb,
	year = {2022},
	note = {arXiv:2102.09844 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/MH4Q8UMH/2102.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/TNLJWDTI/Satorras et al. - 2022 - E(n) Equivariant Graph Neural Networks.pdf:application/pdf},
}

@article{tubiana_scannet_2022,
	title = {{ScanNet}: an interpretable geometric deep learning model for structure-based protein binding site prediction},
	volume = {19},
	copyright = {2022 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {{ScanNet}},
	url = {https://www.nature.com/articles/s41592-022-01490-7},
	doi = {10.1038/s41592-022-01490-7},
	abstract = {Predicting the functional sites of a protein from its structure, such as the binding sites of small molecules, other proteins or antibodies, sheds light on its function in vivo. Currently, two classes of methods prevail: machine learning models built on top of handcrafted features and comparative modeling. They are, respectively, limited by the expressivity of the handcrafted features and the availability of similar proteins. Here, we introduce ScanNet, an end-to-end, interpretable geometric deep learning model that learns features directly from 3D structures. ScanNet builds representations of atoms and amino acids based on the spatio-chemical arrangement of their neighbors. We train ScanNet for detecting protein–protein and protein–antibody binding sites, demonstrate its accuracy—including for unseen protein folds—and interpret the filters learned. Finally, we predict epitopes of the SARS-CoV-2 spike protein, validating known antigenic regions and predicting previously uncharacterized ones. Overall, ScanNet is a versatile, powerful and interpretable model suitable for functional site prediction tasks. A webserver for ScanNet is available from http://bioinfo3d.cs.tau.ac.il/ScanNet/.},
	language = {en},
	number = {6},
	urldate = {2023-12-06},
	journal = {Nature Methods},
	author = {Tubiana, Jérôme and Schneidman-Duhovny, Dina and Wolfson, Haim J.},
	month = jun,
	year = {2022},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Protein function predictions, Software},
	pages = {730--739},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/QVK9UGCX/Tubiana et al. - 2022 - ScanNet an interpretable geometric deep learning .pdf:application/pdf},
}

@misc{huang_zero-shot_2023,
	title = {Zero-shot drug repurposing with geometric deep learning and clinician centered design},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.medrxiv.org/content/10.1101/2023.03.19.23287458v2},
	doi = {10.1101/2023.03.19.23287458},
	abstract = {{\textless}p{\textgreater}Historically, drug repurposing – identifying new therapeutic uses for approved drugs – has been attributed to serendipity. While recent advances have leveraged knowledge graphs and deep learning to identify potential therapeutic candidates, their clinical utility remains limited due to their dependence on existing knowledge about diseases. Here, we introduce TXGNN, a geometric deep learning approach designed for “zero-shot” drug repurposing, enabling therapeutic predictions even for diseases with no existing medicines. Trained on a medical knowledge graph, TXGNN utilizes a graph neural network and metric-learning module to rank therapeutic candidates as potential indications and contraindications across 17,080 diseases. When benchmarked against eight leading methods, TXGNN significantly improves prediction accuracy for indications by 49.2\% and contraindications by 35.1\% under stringent zero-shot evaluation. To facilitate interpretation and analysis of the model’s predictions, TXGNN’s Explainer module offers transparent insights into the multi-hop paths that form TXGNN’s predictive rationale. Clinicians and scientists found TXGNN’s explanations instrumental in contextualizing and validating its predicted therapeutic candidates during our user study. Many of TXGNN’s novel predictions have shown remarkable alignment with off-label prescriptions made by clinicians within a large healthcare system, affirming their real-world utility. TXGNN provides drug repurposing predictions that are more accurate than existing methods, consistent with off-label prescription decisions made by clinicians, and can be investigated through multi-hop interpretable explanations.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2023-12-06},
	publisher = {medRxiv},
	author = {Huang, Kexin and Chandak, Payal and Wang, Qianwen and Havaldar, Shreyas and Vaid, Akhil and Leskovec, Jure and Nadkarni, Girish and Glicksberg, Benjamin S. and Gehlenborg, Nils and Zitnik, Marinka},
	month = sep,
	year = {2023},
	note = {Pages: 2023.03.19.23287458},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/FLISZCBQ/Huang et al. - 2023 - Zero-shot drug repurposing with geometric deep lea.pdf:application/pdf},
}

@article{zheng_multi-modal_2022,
	title = {Multi-{Modal} {Graph} {Learning} for {Disease} {Prediction}},
	volume = {41},
	issn = {1558-254X},
	url = {https://ieeexplore.ieee.org/document/9733917},
	doi = {10.1109/TMI.2022.3159264},
	abstract = {Benefiting from the powerful expressive capability of graphs, graph-based approaches have been popularly applied to handle multi-modal medical data and achieved impressive performance in various biomedical applications. For disease prediction tasks, most existing graph-based methods tend to define the graph manually based on specified modality (e.g., demographic information), and then integrated other modalities to obtain the patient representation by Graph Representation Learning (GRL). However, constructing an appropriate graph in advance is not a simple matter for these methods. Meanwhile, the complex correlation between modalities is ignored. These factors inevitably yield the inadequacy of providing sufficient information about the patient’s condition for a reliable diagnosis. To this end, we propose an end-to-end Multi-modal Graph Learning framework (MMGL) for disease prediction with multi-modality. To effectively exploit the rich information across multi-modality associated with the disease, modality-aware representation learning is proposed to aggregate the features of each modality by leveraging the correlation and complementarity between the modalities. Furthermore, instead of defining the graph manually, the latent graph structure is captured through an effective way of adaptive graph learning. It could be jointly optimized with the prediction model, thus revealing the intrinsic connections among samples. Our model is also applicable to the scenario of inductive learning for those unseen data. An extensive group of experiments on two disease prediction tasks demonstrates that the proposed MMGL achieves more favorable performance. The code of MMGL is available at https://github.com/SsGood/MMGL.},
	number = {9},
	urldate = {2023-12-06},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Zheng, Shuai and Zhu, Zhenfeng and Liu, Zhizhe and Guo, Zhenyu and Liu, Yang and Yang, Yuchen and Zhao, Yao},
	month = sep,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	pages = {2207--2216},
	file = {IEEE Xplore Abstract Record:/Users/jdomi/Zotero/storage/R67J2R6P/9733917.html:text/html;Submitted Version:/Users/jdomi/Zotero/storage/NR39LGF5/Zheng et al. - 2022 - Multi-Modal Graph Learning for Disease Prediction.pdf:application/pdf},
}

@article{moon_pignet_2022,
	title = {{PIGNet}: a physics-informed deep learning model toward generalized drug–target interaction predictions},
	volume = {13},
	shorttitle = {{PIGNet}},
	url = {https://pubs.rsc.org/en/content/articlelanding/2022/sc/d1sc06946b},
	doi = {10.1039/D1SC06946B},
	language = {en},
	number = {13},
	urldate = {2023-12-06},
	journal = {Chemical Science},
	author = {Moon, Seokhyun and Zhung, Wonho and Yang, Soojung and Lim, Jaechang and Youn Kim, Woo},
	year = {2022},
	note = {Publisher: Royal Society of Chemistry},
	pages = {3661--3673},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/LQZAZE2K/Moon et al. - 2022 - PIGNet a physics-informed deep learning model tow.pdf:application/pdf},
}

@misc{yao_graph_2018,
	title = {Graph {Convolutional} {Networks} for {Text} {Classification}},
	url = {http://arxiv.org/abs/1809.05679},
	abstract = {Text classification is an important and classical problem in natural language processing. There have been a number of studies that applied convolutional neural networks (convolution on regular grid, e.g., sequence) to classification. However, only a limited number of studies have explored the more flexible graph convolutional neural networks (convolution on non-grid, e.g., arbitrary graph) for the task. In this work, we propose to use graph convolutional networks for text classification. We build a single text graph for a corpus based on word co-occurrence and document word relations, then learn a Text Graph Convolutional Network (Text GCN) for the corpus. Our Text GCN is initialized with one-hot representation for word and document, it then jointly learns the embeddings for both words and documents, as supervised by the known class labels for documents. Our experimental results on multiple benchmark datasets demonstrate that a vanilla Text GCN without any external word embeddings or knowledge outperforms state-of-the-art methods for text classification. On the other hand, Text GCN also learns predictive word and document embeddings. In addition, experimental results show that the improvement of Text GCN over state-of-the-art comparison methods become more prominent as we lower the percentage of training data, suggesting the robustness of Text GCN to less training data in text classification.},
	urldate = {2023-12-07},
	publisher = {arXiv},
	author = {Yao, Liang and Mao, Chengsheng and Luo, Yuan},
	month = nov,
	year = {2018},
	note = {arXiv:1809.05679 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Accepted by 33rd AAAI Conference on Artificial Intelligence (AAAI 2019)},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/TQEULW8F/1809.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/YUZTLX9Q/Yao et al. - 2018 - Graph Convolutional Networks for Text Classificati.pdf:application/pdf},
}
