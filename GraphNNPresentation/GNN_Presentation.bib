
@book{imperatorskaia_akademia_nauk_russia_commentarii_1726,
	title = {Commentarii {Academiae} scientiarum imperialis {Petropolitanae}},
	url = {http://archive.org/details/commentariiacade08impe},
	abstract = {Engraved title vignette},
	language = {lat},
	urldate = {2023-11-24},
	publisher = {Petropolis, Typis Academiae},
	author = {{Imperatorskaia akademia nauk (Russia)}},
	collaborator = {{American Museum of Natural History Library}},
	year = {1726},
}

@article{chandak_building_2023,
	title = {Building a knowledge graph to enable precision medicine},
	volume = {10},
	copyright = {2023 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-023-01960-3},
	doi = {10.1038/s41597-023-01960-3},
	abstract = {Developing personalized diagnostic strategies and targeted treatments requires a deep understanding of disease biology and the ability to dissect the relationship between molecular and genetic factors and their phenotypic consequences. However, such knowledge is fragmented across publications, non-standardized repositories, and evolving ontologies describing various scales of biological organization between genotypes and clinical phenotypes. Here, we present PrimeKG, a multimodal knowledge graph for precision medicine analyses. PrimeKG integrates 20 high-quality resources to describe 17,080 diseases with 4,050,249 relationships representing ten major biological scales, including disease-associated protein perturbations, biological processes and pathways, anatomical and phenotypic scales, and the entire range of approved drugs with their therapeutic action, considerably expanding previous efforts in disease-rooted knowledge graphs. PrimeKG contains an abundance of ‘indications’, ‘contradictions’, and ‘off-label use’ drug-disease edges that lack in other knowledge graphs and can support AI analyses of how drugs affect disease-associated networks. We supplement PrimeKG’s graph structure with language descriptions of clinical guidelines to enable multimodal analyses and provide instructions for continual updates of PrimeKG as new data become available.},
	language = {en},
	number = {1},
	urldate = {2023-11-26},
	journal = {Scientific Data},
	author = {Chandak, Payal and Huang, Kexin and Zitnik, Marinka},
	month = feb,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Data integration, Machine learning, Medical research, Network topology, Predictive medicine},
	pages = {67},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/EDBVQ4KP/Chandak et al. - 2023 - Building a knowledge graph to enable precision med.pdf:application/pdf},
}

@article{mcdermott_structure-inducing_2023,
	title = {Structure-inducing pre-training},
	volume = {5},
	copyright = {2023 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-023-00647-z},
	doi = {10.1038/s42256-023-00647-z},
	abstract = {Language model pre-training and the derived general-purpose methods have reshaped machine learning research. However, there remains considerable uncertainty regarding why pre-training improves the performance of downstream tasks. This challenge is pronounced when using language model pre-training in domains outside of natural language. Here we investigate this problem by analysing how pre-training methods impose relational structure in induced per-sample latent spaces—that is, what constraints do pre-training methods impose on the distance or geometry between the pre-trained embeddings of samples. A comprehensive review of pre-training methods reveals that this question remains open, despite theoretical analyses showing the importance of understanding this form of induced structure. Based on this review, we introduce a pre-training framework that enables a granular and comprehensive understanding of how relational structure can be induced. We present a theoretical analysis of the framework from the first principles and establish a connection between the relational inductive bias of pre-training and fine-tuning performance. Empirical studies spanning three data modalities and ten fine-tuning tasks confirm theoretical analyses, inform the design of novel pre-training methods and establish consistent improvements over a compelling suite of methods.},
	language = {en},
	number = {6},
	urldate = {2023-11-26},
	journal = {Nature Machine Intelligence},
	author = {McDermott, Matthew B. A. and Yap, Brendan and Szolovits, Peter and Zitnik, Marinka},
	month = jun,
	year = {2023},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {Computational models, Computer science, Machine learning, Statistics},
	pages = {612--621},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/L88JZQ9T/McDermott et al. - 2023 - Structure-inducing pre-training.pdf:application/pdf},
}

@article{ektefaie_multimodal_2023,
	title = {Multimodal learning with graphs},
	volume = {5},
	copyright = {2023 Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-023-00624-6},
	doi = {10.1038/s42256-023-00624-6},
	abstract = {Artificial intelligence for graphs has achieved remarkable success in modelling complex systems, ranging from dynamic networks in biology to interacting particle systems in physics. However, the increasingly heterogeneous graph datasets call for multimodal methods that can combine different inductive biases — assumptions that algorithms use to make predictions for inputs they have not encountered during training. Learning on multimodal datasets is challenging because the inductive biases can vary by data modality and graphs might not be explicitly given in the input. To address these challenges, graph artificial intelligence methods combine different modalities while leveraging cross-modal dependencies through geometric relationships. Diverse datasets are combined using graphs and fed into sophisticated multimodal architectures, specified as image-intensive, knowledge-grounded and language-intensive models. Using this categorization, we introduce a blueprint for multimodal graph learning, use it to study existing methods and provide guidelines to design new models.},
	language = {en},
	number = {4},
	urldate = {2023-11-26},
	journal = {Nature Machine Intelligence},
	author = {Ektefaie, Yasha and Dasoulas, George and Noori, Ayush and Farhat, Maha and Zitnik, Marinka},
	month = apr,
	year = {2023},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science, Machine learning, Statistics},
	pages = {340--350},
	file = {Submitted Version:/Users/jdomi/Zotero/storage/VWY2SA29/Ektefaie et al. - 2023 - Multimodal learning with graphs.pdf:application/pdf},
}

@misc{zhang_systematic_2023,
	title = {A {Systematic} {Survey} in {Geometric} {Deep} {Learning} for {Structure}-based {Drug} {Design}},
	url = {http://arxiv.org/abs/2306.11768},
	doi = {10.48550/arXiv.2306.11768},
	abstract = {Structure-based drug design (SBDD) utilizes the three-dimensional geometry of proteins to identify potential drug candidates. Traditional methods, grounded in physicochemical modeling and informed by domain expertise, are resource-intensive. Recent developments in geometric deep learning, focusing on the integration and processing of 3D geometric data, coupled with the availability of accurate protein 3D structure predictions from tools like AlphaFold, have greatly advanced the field of structure-based drug design. This paper systematically reviews the current state of geometric deep learning in SBDD. We first outline foundational tasks in SBDD, detail prevalent 3D protein representations, and highlight representative predictive and generative models. We then offer in-depth reviews of each key task, including binding site prediction, binding pose generation, {\textbackslash}emph\{de novo\} molecule generation, linker design, and binding affinity prediction. We provide formal problem definitions and outline each task's representative methods, datasets, evaluation metrics, and performance benchmarks. Finally, we summarize the current challenges and future opportunities: current challenges in SBDD include oversimplified problem formulations, inadequate out-of-distribution generalization, a lack of reliable evaluation metrics and large-scale benchmarks, and the need for experimental verification and enhanced model understanding; opportunities include leveraging multimodal datasets, integrating domain knowledge, building comprehensive benchmarks, designing criteria based on clinical endpoints, and developing foundation models that broaden the range of design tasks. We also curate {\textbackslash}url\{https://github.com/zaixizhang/Awesome-SBDD\}, reflecting ongoing contributions and new datasets in SBDD.},
	urldate = {2023-11-26},
	publisher = {arXiv},
	author = {Zhang, Zaixi and Yan, Jiaxian and Liu, Qi and Chen, Enhong and Zitnik, Marinka},
	month = oct,
	year = {2023},
	note = {arXiv:2306.11768 [cs, q-bio]},
	keywords = {Computer Science - Computational Engineering, Finance, and Science, Computer Science - Machine Learning, Quantitative Biology - Quantitative Methods},
	annote = {Comment: 20 pages, under review},
	file = {arXiv Fulltext PDF:/Users/jdomi/Zotero/storage/L6C96VV7/Zhang et al. - 2023 - A Systematic Survey in Geometric Deep Learning for.pdf:application/pdf;arXiv.org Snapshot:/Users/jdomi/Zotero/storage/BN8YHF65/2306.html:text/html},
}

@article{li_graph_2022,
	title = {Graph representation learning in biomedicine and healthcare},
	volume = {6},
	issn = {2157-846X},
	doi = {10.1038/s41551-022-00942-x},
	abstract = {Networks-or graphs-are universal descriptors of systems of interacting elements. In biomedicine and healthcare, they can represent, for example, molecular interactions, signalling pathways, disease co-morbidities or healthcare systems. In this Perspective, we posit that representation learning can realize principles of network medicine, discuss successes and current limitations of the use of representation learning on graphs in biomedicine and healthcare, and outline algorithmic strategies that leverage the topology of graphs to embed them into compact vectorial spaces. We argue that graph representation learning will keep pushing forward machine learning for biomedicine and healthcare applications, including the identification of genetic variants underlying complex traits, the disentanglement of single-cell behaviours and their effects on health, the assistance of patients in diagnosis and treatment, and the development of safe and effective medicines.},
	language = {eng},
	number = {12},
	journal = {Nature Biomedical Engineering},
	author = {Li, Michelle M. and Huang, Kexin and Zitnik, Marinka},
	month = dec,
	year = {2022},
	pmid = {36316368},
	keywords = {Delivery of Health Care, Humans, Machine Learning},
	pages = {1353--1369},
}

@article{sanchez-lengeling_gentle_2021,
	title = {A {Gentle} {Introduction} to {Graph} {Neural} {Networks}},
	volume = {6},
	issn = {2476-0757},
	url = {https://distill.pub/2021/gnn-intro},
	doi = {10.23915/distill.00033},
	abstract = {What components are needed for building learning algorithms that leverage the structure and properties of graphs?},
	language = {en},
	number = {9},
	urldate = {2023-11-27},
	journal = {Distill},
	author = {Sanchez-Lengeling, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alexander B.},
	month = sep,
	year = {2021},
	pages = {e33},
}

@misc{xu_how_2019,
	title = {How {Powerful} are {Graph} {Neural} {Networks}?},
	url = {http://arxiv.org/abs/1810.00826},
	doi = {10.48550/arXiv.1810.00826},
	abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
	month = feb,
	year = {2019},
	note = {arXiv:1810.00826 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jdomi/Zotero/storage/MX7SILVL/Xu et al. - 2019 - How Powerful are Graph Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/jdomi/Zotero/storage/9FD66I8S/1810.html:text/html},
}

@misc{kipf_semi-supervised_2017,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = feb,
	year = {2017},
	note = {arXiv:1609.02907 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published as a conference paper at ICLR 2017},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/IJ8MTNEG/1609.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/AY9ETCPZ/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf:application/pdf},
}

@article{scarselli_graph_2009,
	title = {The {Graph} {Neural} {Network} {Model}},
	volume = {20},
	issn = {1045-9227, 1941-0093},
	url = {http://ieeexplore.ieee.org/document/4700287/},
	doi = {10.1109/TNN.2008.2005605},
	language = {en},
	number = {1},
	urldate = {2023-11-27},
	journal = {IEEE Transactions on Neural Networks},
	author = {Scarselli, F. and Gori, M. and {Ah Chung Tsoi} and Hagenbuchner, M. and Monfardini, G.},
	month = jan,
	year = {2009},
	pages = {61--80},
	file = {Scarselli et al. - 2009 - The Graph Neural Network Model.pdf:/Users/jdomi/Zotero/storage/DLKGJ92Q/Scarselli et al. - 2009 - The Graph Neural Network Model.pdf:application/pdf},
}

@misc{gilmer_neural_2017,
	title = {Neural {Message} {Passing} for {Quantum} {Chemistry}},
	url = {http://arxiv.org/abs/1704.01212},
	abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
	month = jun,
	year = {2017},
	note = {arXiv:1704.01212 [cs]},
	keywords = {Computer Science - Machine Learning, I.2.6},
	annote = {Comment: 14 pages},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/BFUQJEMC/1704.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/5SHINSLY/Gilmer et al. - 2017 - Neural Message Passing for Quantum Chemistry.pdf:application/pdf},
}

@misc{duvenaud_convolutional_2015,
	title = {Convolutional {Networks} on {Graphs} for {Learning} {Molecular} {Fingerprints}},
	url = {http://arxiv.org/abs/1509.09292},
	doi = {10.48550/arXiv.1509.09292},
	abstract = {We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Duvenaud, David and Maclaurin, Dougal and Aguilera-Iparraguirre, Jorge and Gómez-Bombarelli, Rafael and Hirzel, Timothy and Aspuru-Guzik, Alán and Adams, Ryan P.},
	month = nov,
	year = {2015},
	note = {arXiv:1509.09292 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: 9 pages, 5 figures. To appear in Neural Information Processing Systems (NIPS)},
	file = {arXiv Fulltext PDF:/Users/jdomi/Zotero/storage/25EMKK23/Duvenaud et al. - 2015 - Convolutional Networks on Graphs for Learning Mole.pdf:application/pdf;arXiv.org Snapshot:/Users/jdomi/Zotero/storage/FSXKHGU9/1509.html:text/html},
}

@misc{bronstein_geometric_2021,
	title = {Geometric {Deep} {Learning}: {Grids}, {Groups}, {Graphs}, {Geodesics}, and {Gauges}},
	shorttitle = {Geometric {Deep} {Learning}},
	url = {http://arxiv.org/abs/2104.13478},
	abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
	month = may,
	year = {2021},
	note = {arXiv:2104.13478 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Geometry, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 156 pages. Work in progress -- comments welcome!},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/DXD5RI9V/2104.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/EZNMD8TF/Bronstein et al. - 2021 - Geometric Deep Learning Grids, Groups, Graphs, Ge.pdf:application/pdf},
}

@misc{buterez_graph_2022,
	title = {Graph {Neural} {Networks} with {Adaptive} {Readouts}},
	url = {http://arxiv.org/abs/2211.04952},
	abstract = {An effective aggregation of node features into a graph-level representation via readout functions is an essential step in numerous learning tasks involving graph neural networks. Typically, readouts are simple and non-adaptive functions designed such that the resulting hypothesis space is permutation invariant. Prior work on deep sets indicates that such readouts might require complex node embeddings that can be difficult to learn via standard neighborhood aggregation schemes. Motivated by this, we investigate the potential of adaptive readouts given by neural networks that do not necessarily give rise to permutation invariant hypothesis spaces. We argue that in some problems such as binding affinity prediction where molecules are typically presented in a canonical form it might be possible to relax the constraints on permutation invariance of the hypothesis space and learn a more effective model of the affinity by employing an adaptive readout function. Our empirical results demonstrate the effectiveness of neural readouts on more than 40 datasets spanning different domains and graph characteristics. Moreover, we observe a consistent improvement over standard readouts (i.e., sum, max, and mean) relative to the number of neighborhood aggregation iterations and different convolutional operators.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Buterez, David and Janet, Jon Paul and Kiddle, Steven J. and Oglic, Dino and Liò, Pietro},
	month = nov,
	year = {2022},
	note = {arXiv:2211.04952 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: Published at NeurIPS 2022. 10 pages, 5 figures, 1 table},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/B8D23KP8/2211.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/JWLWRIMK/Buterez et al. - 2022 - Graph Neural Networks with Adaptive Readouts.pdf:application/pdf},
}

@misc{ferludin_tf-gnn_2023,
	title = {{TF}-{GNN}: {Graph} {Neural} {Networks} in {TensorFlow}},
	shorttitle = {{TF}-{GNN}},
	url = {http://arxiv.org/abs/2207.03522},
	doi = {10.48550/arXiv.2207.03522},
	abstract = {TensorFlow-GNN (TF-GNN) is a scalable library for Graph Neural Networks in TensorFlow. It is designed from the bottom up to support the kinds of rich heterogeneous graph data that occurs in today's information ecosystems. In addition to enabling machine learning researchers and advanced developers, TF-GNN offers low-code solutions to empower the broader developer community in graph learning. Many production models at Google use TF-GNN, and it has been recently released as an open source project. In this paper we describe the TF-GNN data model, its Keras message passing API, and relevant capabilities such as graph sampling and distributed training.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Ferludin, Oleksandr and Eigenwillig, Arno and Blais, Martin and Zelle, Dustin and Pfeifer, Jan and Sanchez-Gonzalez, Alvaro and Li, Wai Lok Sibon and Abu-El-Haija, Sami and Battaglia, Peter and Bulut, Neslihan and Halcrow, Jonathan and de Almeida, Filipe Miguel Gonçalves and Gonnet, Pedro and Jiang, Liangze and Kothari, Parth and Lattanzi, Silvio and Linhares, André and Mayer, Brandon and Mirrokni, Vahab and Palowitch, John and Paradkar, Mihir and She, Jennifer and Tsitsulin, Anton and Villela, Kevin and Wang, Lisa and Wong, David and Perozzi, Bryan},
	month = jul,
	year = {2023},
	note = {arXiv:2207.03522 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Social and Information Networks, Physics - Physics and Society, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jdomi/Zotero/storage/5GCCG9V5/Ferludin et al. - 2023 - TF-GNN Graph Neural Networks in TensorFlow.pdf:application/pdf;arXiv.org Snapshot:/Users/jdomi/Zotero/storage/8GXFP7QQ/2207.html:text/html},
}

@inproceedings{gori_new_2005,
	title = {A new model for learning in graph domains},
	volume = {2},
	url = {https://ieeexplore.ieee.org/document/1555942},
	doi = {10.1109/IJCNN.2005.1555942},
	abstract = {In several applications the information is naturally represented by graphs. Traditional approaches cope with graphical data structures using a preprocessing phase which transforms the graphs into a set of flat vectors. However, in this way, important topological information may be lost and the achieved results may heavily depend on the preprocessing stage. This paper presents a new neural model, called graph neural network (GNN), capable of directly processing graphs. GNNs extends recursive neural networks and can be applied on most of the practically useful kinds of graphs, including directed, undirected, labelled and cyclic graphs. A learning algorithm for GNNs is proposed and some experiments are discussed which assess the properties of the model.},
	urldate = {2023-11-28},
	booktitle = {Proceedings. 2005 {IEEE} {International} {Joint} {Conference} on {Neural} {Networks}, 2005.},
	author = {Gori, M. and Monfardini, G. and Scarselli, F.},
	month = jul,
	year = {2005},
	note = {ISSN: 2161-4407},
	pages = {729--734 vol. 2},
	file = {IEEE Xplore Abstract Record:/Users/jdomi/Zotero/storage/GTX6ZF9N/1555942.html:text/html},
}

@inproceedings{ying_hierarchical_2018,
	title = {Hierarchical {Graph} {Representation} {Learning} with {Differentiable} {Pooling}},
	volume = {31},
	url = {https://papers.nips.cc/paper_files/paper/2018/hash/e77dbaf6759253c7c6d0efc5690369c7-Abstract.html},
	abstract = {Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10\% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark datasets.},
	urldate = {2023-11-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ying, Zhitao and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, Will and Leskovec, Jure},
	year = {2018},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/F9ECRZCH/Ying et al. - 2018 - Hierarchical Graph Representation Learning with Di.pdf:application/pdf},
}

@misc{hamilton_inductive_2018,
	title = {Inductive {Representation} {Learning} on {Large} {Graphs}},
	url = {http://arxiv.org/abs/1706.02216},
	abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	month = sep,
	year = {2018},
	note = {arXiv:1706.02216 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
	annote = {Comment: Published in NIPS 2017; version with full appendix and minor corrections},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/A7VN75B4/1706.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/5R6EYCZA/Hamilton et al. - 2018 - Inductive Representation Learning on Large Graphs.pdf:application/pdf},
}

@inproceedings{ying_hierarchical_2018-1,
	title = {Hierarchical {Graph} {Representation} {Learning} with {Differentiable} {Pooling}},
	volume = {31},
	url = {https://papers.nips.cc/paper_files/paper/2018/hash/e77dbaf6759253c7c6d0efc5690369c7-Abstract.html},
	abstract = {Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10\% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark datasets.},
	urldate = {2023-11-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ying, Zhitao and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, Will and Leskovec, Jure},
	year = {2018},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/MS3YB3G9/Ying et al. - 2018 - Hierarchical Graph Representation Learning with Di.pdf:application/pdf},
}

@misc{he_pytorch_2023,
	title = {{PyTorch} {Geometric} {Signed} {Directed}: {A} {Software} {Package} on {Graph} {Neural} {Networks} for {Signed} and {Directed} {Graphs}},
	shorttitle = {{PyTorch} {Geometric} {Signed} {Directed}},
	url = {http://arxiv.org/abs/2202.10793},
	abstract = {Networks are ubiquitous in many real-world applications (e.g., social networks encoding trust/distrust relationships, correlation networks arising from time series data). While many networks are signed or directed, or both, there is a lack of unified software packages on graph neural networks (GNNs) specially designed for signed and directed networks. In this paper, we present PyTorch Geometric Signed Directed (PyGSD), a software package which fills this gap. Along the way, we evaluate the implemented methods with experiments with a view to providing insights into which method to choose for a given task. The deep learning framework consists of easy-to-use GNN models, synthetic and real-world data, as well as task-specific evaluation metrics and loss functions for signed and directed networks. As an extension library for PyG, our proposed software is maintained with open-source releases, detailed documentation, continuous integration, unit tests and code coverage checks. The GitHub repository of the library is https://github.com/SherylHYX/pytorch\_geometric\_signed\_directed.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {He, Yixuan and Zhang, Xitong and Huang, Junjie and Rozemberczki, Benedek and Cucuringu, Mihai and Reinert, Gesine},
	month = nov,
	year = {2023},
	note = {arXiv:2202.10793 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
	annote = {Comment: Accepted by LoG 2023. 27 pages in total},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/RGAM5P3H/2202.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/H3CSXYAZ/He et al. - 2023 - PyTorch Geometric Signed Directed A Software Pack.pdf:application/pdf},
}

@inproceedings{duvenaud_convolutional_2015-1,
	title = {Convolutional {Networks} on {Graphs} for {Learning} {Molecular} {Fingerprints}},
	volume = {28},
	url = {https://papers.nips.cc/paper_files/paper/2015/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html},
	abstract = {We introduce a convolutional neural network that operates directly on graphs.These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape.The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints.We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
	urldate = {2023-11-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Duvenaud, David K and Maclaurin, Dougal and Iparraguirre, Jorge and Bombarell, Rafael and Hirzel, Timothy and Aspuru-Guzik, Alan and Adams, Ryan P},
	year = {2015},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/82S4LL9Y/Duvenaud et al. - 2015 - Convolutional Networks on Graphs for Learning Mole.pdf:application/pdf},
}

@inproceedings{defferrard_convolutional_2016,
	title = {Convolutional {Neural} {Networks} on {Graphs} with {Fast} {Localized} {Spectral} {Filtering}},
	volume = {29},
	url = {https://papers.nips.cc/paper_files/paper/2016/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html},
	abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words’ embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
	urldate = {2023-11-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Defferrard, Michaël and Bresson, Xavier and Vandergheynst, Pierre},
	year = {2016},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/NKKSTN22/Defferrard et al. - 2016 - Convolutional Neural Networks on Graphs with Fast .pdf:application/pdf},
}

@misc{bruna_spectral_2014,
	title = {Spectral {Networks} and {Locally} {Connected} {Networks} on {Graphs}},
	url = {http://arxiv.org/abs/1312.6203},
	abstract = {Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Bruna, Joan and Zaremba, Wojciech and Szlam, Arthur and LeCun, Yann},
	month = may,
	year = {2014},
	note = {arXiv:1312.6203 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 14 pages},
	file = {arXiv.org Snapshot:/Users/jdomi/Zotero/storage/R4IYDNIG/1312.html:text/html;Full Text PDF:/Users/jdomi/Zotero/storage/UJVTV3QD/Bruna et al. - 2014 - Spectral Networks and Locally Connected Networks o.pdf:application/pdf},
}

@misc{javaloy_mitigating_2022,
	title = {Mitigating {Modality} {Collapse} in {Multimodal} {VAEs} via {Impartial} {Optimization}},
	url = {http://arxiv.org/abs/2206.04496},
	abstract = {A number of variational autoencoders (VAEs) have recently emerged with the aim of modeling multimodal data, e.g., to jointly model images and their corresponding captions. Still, multimodal VAEs tend to focus solely on a subset of the modalities, e.g., by fitting the image while neglecting the caption. We refer to this limitation as modality collapse. In this work, we argue that this effect is a consequence of conflicting gradients during multimodal VAE training. We show how to detect the sub-graphs in the computational graphs where gradients conflict (impartiality blocks), as well as how to leverage existing gradient-conflict solutions from multitask learning to mitigate modality collapse. That is, to ensure impartial optimization across modalities. We apply our training framework to several multimodal VAE models, losses and datasets from the literature, and empirically show that our framework significantly improves the reconstruction performance, conditional generation, and coherence of the latent space across modalities.},
	language = {en},
	urldate = {2023-11-29},
	publisher = {arXiv},
	author = {Javaloy, Adrián and Meghdadi, Maryam and Valera, Isabel},
	month = jun,
	year = {2022},
	note = {arXiv:2206.04496 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Accepted as a Spotlight paper at ICML 2022. 27 pages, 10 figures},
}

@misc{javaloy_mitigating_2022-1,
	title = {Mitigating {Modality} {Collapse} in {Multimodal} {VAEs} via {Impartial} {Optimization}},
	url = {https://arxiv.org/abs/2206.04496v1},
	abstract = {A number of variational autoencoders (VAEs) have recently emerged with the aim of modeling multimodal data, e.g., to jointly model images and their corresponding captions. Still, multimodal VAEs tend to focus solely on a subset of the modalities, e.g., by fitting the image while neglecting the caption. We refer to this limitation as modality collapse. In this work, we argue that this effect is a consequence of conflicting gradients during multimodal VAE training. We show how to detect the sub-graphs in the computational graphs where gradients conflict (impartiality blocks), as well as how to leverage existing gradient-conflict solutions from multitask learning to mitigate modality collapse. That is, to ensure impartial optimization across modalities. We apply our training framework to several multimodal VAE models, losses and datasets from the literature, and empirically show that our framework significantly improves the reconstruction performance, conditional generation, and coherence of the latent space across modalities.},
	language = {en},
	urldate = {2023-11-29},
	journal = {arXiv.org},
	author = {Javaloy, Adrián and Meghdadi, Maryam and Valera, Isabel},
	month = jun,
	year = {2022},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/X3NAU3AQ/Javaloy et al. - 2022 - Mitigating Modality Collapse in Multimodal VAEs vi.pdf:application/pdf},
}

@article{batool_structure-based_2019,
	title = {A {Structure}-{Based} {Drug} {Discovery} {Paradigm}},
	volume = {20},
	issn = {1422-0067},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6601033/},
	doi = {10.3390/ijms20112783},
	abstract = {Structure-based drug design is becoming an essential tool for faster and more cost-efficient lead discovery relative to the traditional method. Genomic, proteomic, and structural studies have provided hundreds of new targets and opportunities for future drug discovery. This situation poses a major problem: the necessity to handle the “big data” generated by combinatorial chemistry. Artificial intelligence (AI) and deep learning play a pivotal role in the analysis and systemization of larger data sets by statistical machine learning methods. Advanced AI-based sophisticated machine learning tools have a significant impact on the drug discovery process including medicinal chemistry. In this review, we focus on the currently available methods and algorithms for structure-based drug design including virtual screening and de novo drug design, with a special emphasis on AI- and deep-learning-based methods used for drug discovery.},
	number = {11},
	urldate = {2023-11-29},
	journal = {International Journal of Molecular Sciences},
	author = {Batool, Maria and Ahmad, Bilal and Choi, Sangdun},
	month = jun,
	year = {2019},
	pmid = {31174387},
	pmcid = {PMC6601033},
	pages = {2783},
	file = {PubMed Central Full Text PDF:/Users/jdomi/Zotero/storage/8DWP6XMK/Batool et al. - 2019 - A Structure-Based Drug Discovery Paradigm.pdf:application/pdf},
}

@article{gainza_deciphering_2020,
	title = {Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning},
	volume = {17},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-019-0666-6},
	doi = {10.1038/s41592-019-0666-6},
	abstract = {Predicting interactions between proteins and other biomolecules solely based on structure remains a challenge in biology. A high-level representation of protein structure, the molecular surface, displays patterns of chemical and geometric features that fingerprint a protein’s modes of interactions with other biomolecules. We hypothesize that proteins participating in similar interactions may share common fingerprints, independent of their evolutionary history. Fingerprints may be difficult to grasp by visual analysis but could be learned from large-scale datasets. We present MaSIF (molecular surface interaction fingerprinting), a conceptual framework based on a geometric deep learning method to capture fingerprints that are important for specific biomolecular interactions. We showcase MaSIF with three prediction challenges: protein pocket-ligand prediction, protein–protein interaction site prediction and ultrafast scanning of protein surfaces for prediction of protein–protein complexes. We anticipate that our conceptual framework will lead to improvements in our understanding of protein function and design.},
	language = {en},
	number = {2},
	urldate = {2023-11-29},
	journal = {Nature Methods},
	author = {Gainza, P. and Sverrisson, F. and Monti, F. and Rodolà, E. and Boscaini, D. and Bronstein, M. M. and Correia, B. E.},
	month = feb,
	year = {2020},
	note = {Number: 2
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Protein function predictions, Protein structure predictions, Proteins},
	pages = {184--192},
	file = {Full Text PDF:/Users/jdomi/Zotero/storage/MSR9WTZY/Gainza et al. - 2020 - Deciphering interaction fingerprints from protein .pdf:application/pdf},
}
