
\documentclass[11pt]{article}

\usepackage{verbose_mdframed}

\makeatletter
\newcommand{\unchapter}[1]{%
  \begingroup
  \let\@makechapterhead\@gobble % make \@makechapterhead do nothing
  \chapter{#1}
  \endgroup
}
\makeatother


\begin{document}

\roundhomework{Graphical Modelling Guided Study}{}{}{ {\bf Dominic DiSanto working with Junwei Lu}}
%\boxsection*[\LARGE Preface]
\section*{Preface}

\begin{itemize}
    \item Exercises are presented with omissions, solutions are unverified 
    \item Proofs are similarly good-faith efforts but unverified 
    \item \textcolor{red}{Red text} indicates my personal questions, lapses in understanding, or otherwise shakey areas 
    \item Other nice treatments of similar material include:
    \begin{itemize}
            \item Frederic Koehler's lecture on Common Gaussian Graphical Models \url{https://www.youtube.com/watch?v=V6NMDZB6LI4}
        	\item -- Illinois lecture note on graphical models class: \url{http://swoh.web.engr.illinois.edu/courses/IE598/info.html}
    \end{itemize}
    \item Useful\footnote{Not a pre-req (or any area of experience for me) but relevant optimization tools do crop up in Ch. 17 of ESL} references/notes on convex optimization and sub-gradient notation 
    \begin{itemize}
            \item Ryan Tibshirani's Convex Optimization Notes at \url{https://www.stat.cmu.edu/~ryantibs/convexopt/}
            \item Boyd and Vandenberghe's 2008 Textbook on Convex Optimization \url{https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf}
    \end{itemize}
\end{itemize}

\section*{Non-Urgent/Of-Interest Review}

\begin{itemize}
    \item Community-Based Group Graphical Lasso (Pircalabelu, 2020) \url{https://www.jmlr.org/papers/volume21/19-181/19-181.pdf}
    \item Applications of lasso/grouped lasso (Friedman 2010) \url{https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2015/2010-Friedman-Hastie-Tibshirani.pdf}
    \item Elastic net model selection in undirected graphical models (Cucuringu 2011) \url{https://arxiv.org/abs/1111.0559}
    \item Review Wasserman Ch 19 (log-linear models)
    \item Review Junction-Tree Algo (ESL Ch 17 references) 
    \item Original group lasso paper (Yuan, 2006) \url{http://www.columbia.edu/~my2550/papers/glasso.final.pdf}
\end{itemize}


\newpage 

\section*{Notation}

{\it Abbreviated, contains only notation that is ambiguous or otherwise necessitates explicit definition}
 
\begin{itemize}
    \item $0 \in \mathbb{N}, 0 \not\in \mathbb{N}^+$
    \item We abbreviate integer sets as $[k] = \{1, 2, ..., k\}$ (implicitly assertes $k \in \mathbb{N}^+$)
    \item For any $f: \mathbb{R}^n \mapsto \mathbb{R}$, $\nabla f, \nabla^2 f$ represent the gradient and Hessian respectively. $H_f, \mathcal{H}_f$ may similarly represent the Hessian but I try to minimize use of this notation in favor of $\nabla^2f$
\end{itemize}

\newpage  
%\boxsection*[\LARGE To Do ]
\section*{To Do}

\subsection*{Important}

\begin{itemize}
    \item Review Lemma 2 proof 
    \item Review proposition 2 proof (v1 doc) 
    \item Understand theorem(s)/cited papers used to determine that Gaussian/subGaussian design matrices in linear setting
        \subitem Exact sparsity, Gaussian-ensemble matrices follows directly from Raskutti 2010 
        \subitem \textcolor{red}{Exact sparsity, sub-Gaussian} Seems like it follows from sub-Gaussian ensemble satisfying the UUP via \href{https://arxiv.org/pdf/math/0608665.pdf}{Mendelson 2006, Uniform uncertainty principle for Bernoulli and subgaussian ensembles} but may also need \href{https://arxiv.org/pdf/0912.4045.pdf}{Zhou 2009, Restricted Eigenvalue Conditions on Subgaussian Random Matrices} (although this may just be an extension beyond independence)
        \subitem \textcolor{red}{Weak sparsity, Gaussian ensemble}
        \subitem \textcolor{red}{Weak sparsity, sub-Gaussian ensemble}
    \item  Find/Prove sub-Gaussian definition (pg. 13): \newline 
        $w\sim subG(\sigma^2)$ if $\mathbb{P}(|\iprod{w, v}|\geq t) \leq 2\exp{-\frac{t^2}{2\sigma^2}}, \forall ||v||_2 \leq 1$
    \item 
\end{itemize}


\subsection*{Supplementary}

\begin{itemize}
    \item Implement Graphical Lasso (ESL Exercise 17.8)
    \item TeX up/finish ESL Ch. 17 notes 
    \item Review proofs:
    \begin{itemize}
    	\item Hammersley-Clifford theorem
	    \item Markov Properties  
        \begin{itemize}
            \item Global $\Leftrightarrow$ Pairwise $\Leftrightarrow$ Local (for positive distributions)
            \item Global $\Rightarrow$ Local $\Rightarrow$ Pairwise (generally)
        \end{itemize}
    \end{itemize}
\end{itemize}

\newpage 
\section*{Misc. Proofs}

\subsection*{Hammersley-Clifford Theorem}

\subsection*{Equivalence of Pairwise and Global Markov Factorizations of Graph}


\newpage 

\begin{tcolorbox}
\vspace{4mm}
\bf \LARGE \chapter{Publications}
\vspace{4mm}
\end{tcolorbox}

\section*{Chen (2014): {\large \it Selection and Estimation for Mixed Graphical Models}}

\subsection*{Takeaways/High-Level Notes}
	\begin{itemize}
		\item This paper extends previous work to allow for estimation of conditional dependencies/associations within graphs of mixed distributions within the exponential family. Previous work focused on Gaussian graphs and more recently within 
		\begin{itemize}
			\item One related/contemporaneous work allowed for a mixed model of two distributions, this work allows for any(?) combination of the specified exponential family distributions
        \end{itemize}
	\end{itemize}

\subsection*{Further Review}
	
	\begin{itemize}
		\item Work through derivations of Ex. 1 to 4 (parameterization of conditional densities $p(x_s | x_{-s})$ in the proposed conditional density form (3))
        \begin{itemize}
            \item -- See \url{http://www.cs.cmu.edu/~epxing/Class/10708-16/note/10708_scribe_lecture10.pdf}
        \end{itemize}
        \item Revisit equation (3) (pg 3), how was this derived or arrived at? Is this a known extension of the exponential family definition in the graphical setting? 
	\end{itemize}

\subsection*{Notes/Questions}
	
	\begin{itemize}
	
		\item \textcolor{red}{The Introduction mentioned papers from \(\sim\)2009-13 that proposed semi-/non-parametric methods for conditional dependence estimation (Graphical Random Forest; Joint Additive Models) but criticizes these methods' efficiency. Is non-parametric estimation still an open research area?}
		
		\item \textcolor{red}{What are these node potential functions, $f(x_s)$? Are they present to capture/account for the marginal "densities"(?) of a given random vector/node $x_s$? I want to understand because they seem to define the importance of $\alpha_s$, which in turn are parameters that we assume are known in the algorithm proposed in Section 3. Just trying to understand 1) what we are estimating in estimating $\alpha_{s1}$ and how strict (or just what the) assumption is when we say $\alpha_{sk}$ are known for $k\geq 2$}
		\begin{itemize}
            \item \textcolor{red}{The $\alpha_s$ are defined as $f(x_s) = \alpha_{s1}x_s + \alpha_{s2}x_s^2/2 + \sum \alpha_{sk}B_{sk}(x_k)$. That is, the $\alpha_s$ vector are coefficients for some linear combination that defines the node potential function}
            \item To this coefficient point, most often $\alpha_{s1}$/linear term coefficient is the only (or most important) parameter of interest. This expression of $f(x_s)$ allows for generalizations to include higher order terms, but as the authors note, $\alpha_{sk}=0$ or known$, k\geq 2$ is a common assumption that works in most applications. This linear combination of functions of $x_k$ is most general 
        \end{itemize}

		\item \textcolor{red}{Why allow for different penalty $\lambda$ by node type? This was counterintuitive to me. We've assumed that our graph is undirected, or $\theta_{st}=\theta)_{ts}$. However if $x_s, x_t$ are distributed differently (e.g. one Poisson and one binomial), $\not \Rightarrow \lambda_s \theta_{st}= \lambda_t \theta_{ts}$ (we could apply different optimal $\lambda$ values}
		
		\item \textcolor{red}{General question: Neighbourhood selection (and Graphical Lasso) are defined on $\ell_1$ penalty alone. Are there extensions (and if so, are they popular/open reserach) on $\ell_2$ or combined penalties?}
			\begin{itemize}
                \item Found a 2011 treatment of an elastic net model for undirected Gaussian undirected graphs out of Princeton at \url{https://arxiv.org/abs/1111.0559}
            \end{itemize} 			
	\end{itemize}
	
	
\newpage 

\section*{Negahban (2012) {\large \it Unified Framework for High-Dimensional Analyais of $M$-Estimators with Decomposable Regulators}}

\subsection*{Takeaways/Outline}


Definitions in section 2 as set-up for Theorem 1:
\begin{enumerate}
	\item Identifies a suitable bound on our penalty $\lambda_n$ related to the norm $\mathcal{R}(\grad \mathcal{L}(\theta^*))$ such that our errors $\delta$ for any optimal solution $\hat{\theta}$ are contained in a bounded cone (if $\theta^* \in \mathcal{M}$, a star-shaped set otherwise)
	\item Strong convexity of our loss function $\mathcal{L}$ is necessary to ensure that $|\mathcal{L}(\theta^*) - \mathcal{L}(\hat{\theta}_{\lambda_n}|$ approaching 0 implies that $\delta$ (or $\hat{\delta}$ is also small. We can be less strict in this assumption and only require strong convexity in a neighborhood about $\theta^*$, and in fact can focus on the cone (or star-shaped set) identified in definition 1, leading us to this RSC definition/consideration 
	\item \textcolor{red}{It seems intuitive to want some subspace compatibility measure, as we can construct any subspace $M$ (and norm, with decomposability as in Dfn 1) and $\overline{M}^\perp$ that we deem appropriate, but what is the idea behind this specific measure of compatibility? } My intuition is that this captures the interplay between the subspace $M$ (by suping over $u \in M$) and balancing the norm $\mathcal{R}$ with the 
\end{enumerate}

\begin{itemize}
	\item namely, a decomposability property for the regularizer and a notion of restricted strong convexity that depends on the inter- action between the regularizer and the loss function.
	\item Decomposability (want to ensure I understand this perturbation intuition): 
		\begin{itemize}
            \item Consider a model $M$ and our estimation $\overline{M}$ (for simplicity consider $M = \overline{M}$ but general result offered for $\overline{M} \subseteq M$. For $\theta \in M$ and $\gamma \in \overline{M}$, $\theta+\gamma$ (and specifically $\gamma$) can be considered deviations away from the model space 
        \end{itemize}
\end{itemize}



Applications to Lasso (4-5):

\begin{itemize}
	\item 4.1) For least squares LASSO ($\ell_1$ penalty), the Taylor series involved in the RSC definition is exact, and thus independent of $\theta^*$ (the parameter's true value for all intents and purpose). This, combined with the definition of the cone set $\mathcal{C}$ to which $\hat{\Delta}$ must belong simplifies the necessary RSC demonstration into a restricted eigenvalue condition, which can be shown to be met with high probability for Gaussian (and subGaussian) design matrices (even with dependencies)
    \begin{itemize}
        \item But no comments yet on convergence, accuracy, etc. just meeting this RSC definition as necessary for further analysis of this M-estimation problem 
    \end{itemize}
    \item 4.2) Assume RE to assume RSC. We know $\ell_1$ is decomposable. Thus we have the bounds from Corollary 1 on the error vector, we must simply identify the regularization constant $\lambda_n$ and the compatibility function $\Psi(M)$ for $\mathcal{R}(\cdot) = ||\cdot||_1$ and error norm $||\cdot||_2$ (which is the meat of the proof of Corollary 2, besides the implicit $RE \Rightarrow RSC$ argument that I believe 4.1 makes)
		\begin{itemize}
            \item Note the sub-gaussian (and normalizing) assumption allows for the high-probability argument made in the proof of corollary 2. 
        \end{itemize}
	\item 4.3) So assuming now $\theta^*$ is weakly sparse, that is not sparse but adequately-approximated by a sparse vector. We construct a set of sparse vectors for $\mathcal{M}$, and now have that $\theta^* \not\in \mathcal{M}$, leading to 1) this ball-shaped set and 2) the need for a positive tolerance function $\tau(\theta^*)$. This is contradicted however by the statement in Corollary 3 that $\theta^* \in \mathbb{B}_q(R_q)$, that is $\theta^*$ actually does belong to our model/sparsifiable set. 
	\begin{itemize}
        \item Maybe it's that $\theta^*$ belongs to $\mathbb{B}_q(R_q)$ but we are estimating with a truly sparse set (i.e. $\mathbb{B}_0(R_0)$ with at most $R_0$ non-zero features), a stricter condition/subset of the $\mathbb{B}_q$ sparsifiable set 
        \item GLM extension essentially (with exponential family requirement although this is w/in definition of GLM) is just a revisited Taylor Series expansion with analogous work and results and an additional\textcolor{red}{(?)} constraint that sample size scales in $\Omega(s \log p)$ for sparsity $|S|=:s$.
    \end{itemize}
\end{itemize}
\subsection*{Questions/Notes}
\textcolor{red}{
	\begin{itemize}
		\item (pg. 11 (c)) How does the tolerance $\tau_\mathcal{L}$ related to unidentifiable components in a high-dim model (possibly discussed in sections 6-8 of v1 paper)? 
		\item Understand considerations of restricted eigenvalue, restricted isometry property, and partial Riesz conditions in the context of convergence fo $\ell_2$ loss functionin $\ell_1$ (i.e. lasso) settings
            \subitem See \url{https://www.stat.berkeley.edu/~binyu/ps/papers2010/RaskuttiWY10.pdf} Raskutti 2010 paper on correlated Gaussian ensembles section 2.2.2
            \subitem RE is listed as least severe, so curious if the others are just of historical importance or the different analyses of convergence are across these properties 
    \item From Raskutti 2010, $\Sigma$-Gaussian ensemble design matrices have sample covariance matrices $X^TX/n$ that satisfy the RE property if $\Sigma$ satisfies the RE condition (and we have adequate sample size scaling). Does $\Sigma$ necessarily complete the RE condition as a covariance matrix? Seems like a nontrivial assumption 
        \subitem \url{https://www.stat.berkeley.edu/~binyu/ps/papers2010/RaskuttiWY10.pdf}
	\end{itemize}
}
	
\newpage 
\subsection*{Restricted Strong Convexity Proofs}

\subsubsection{Set-Up/Background}

\textcolor{blue}{Currently assuming $\overline{\mathcal{M}}=\mathcal{M}$ until otherwise noted, deferring to $\mathcal{M}$ when referencing the model subspace.}\newline 

Recall $\hat\Delta = \hat{\theta}_{\lambda_n} - \theta^*$. Also recall that the first-order Taylor series of any loss function  $\mathcal{L}: \mathbb{R}^n \mapsto \mathbb{R}$ about $\theta^* + \Delta$\footnote{Noting $\Delta$ is some general perturbation, no explicit relationship to $\hat\Delta$} with some error $\delta \mathcal{L}(\theta^*, \Delta)$ is

\begin{gather*}
    \mathcal{L}(\theta^* + \Delta)
    =
    \mathcal{L}(\theta^*)
    +
    \Delta^T\nabla\mathcal{L}(\theta^*)
    +
    \delta \mathcal{L}(\theta^*, \Delta)
\end{gather*}

%\href{https://math.stackexchange.com/questions/302601/exact-expansion-of-functions}{Also recall generally} that for $f: \mathbb{R}^n \mapsto \mathbb{R}$, 
%
%\begin{gather*}
%    f(y) = f(x) + \nabla f(x)^T(y-x) + \frac{1}{2}(y-x)^T \nabla^2 f(z) (y-x)
%\end{gather*}

{\bf Definition:} A loss function satisfies the restricted storng convexity condition with curvature $\kappa_\mathcal{L}$ and tolerance $\tau_{\mathcal{L}}(\theta^*)$ if its first-order Taylor's series error satisfies the following

\begin{gather*}
    \delta \mathcal{L}(\theta^*, \Delta)
    \geq \kappa_\mathcal{L}||\Delta||_2^2 - \tau^2_\mathcal{L}(\theta^*), \ \ \forall \Delta \in \mathbb{S}
\end{gather*}

for some set $\mathbb{S}$. We see that for reasonable selection of $\lambda_n$, we can identify $\mathbb{S}=\mathbb{C}(\mathcal{M}, \overline{\mathcal{M}}^\perp, \theta^*)$.

\subsubsection{OLS  with exact sparsity ($\theta^* \in \mathcal{M}$), $\ell_1$ regularization penalty}
  
$\mathcal{L}(\theta) = ||y-X\theta||_2^2, \mathcal{R}(w) = ||w||_1$, and $S \subseteq[p]$ indexes the known, non-zero elements of $\theta$ (with $S_C = [p]\backslash S)$. \newline 

In this setting $\mathcal{L}(\theta) := \frac{1}{2n}||y-X\theta||_2^2$ and thus $\nabla\mathcal{L}(\theta) = -\frac{1}{n}X^T(y-X\theta), \nabla^2\mathcal{L}(\theta) = \frac{1}{n}X^TX$. \newline 


\begin{enumerate}
    \item Decomposability satisfied for $\ell_1$ penalty 
    \item Subspace compatibility fully specified by known $\ell_2, \ell_1$ relationship such that
    \begin{gather*} \Psi(M) := \sup_{u \in \mathcal{M}} \frac{\mathcal{R}(u)}{||u||_2} = \sup_{u \in S} \frac{||u||_1}{||u||_2} = \sqrt{|S|} \end{gather*}
    \item First-order Taylor series expansion of OLS loss yields $\delta\mathcal{L}(\theta^*, \Delta) = \frac{1}{n}\Delta^T X^TX\Delta = \frac{1}{n}||X\Delta||_2^2$. Want to uniformly lower-bound this remainder for a sensisble/appropriate set of $\Delta$.  
    \item For properly selected regulation paramter $\lambda_n$ (by Lemma 1, bound to be shown later), we can identify our appropriate set as $\mathbb{C} := \{||\Delta_{S_c}||_1 \leq 3||\Delta_S||_1\}$, as by assumption $\theta^* \in \mathcal{M}_S$, which implies $\Pi_{\mathcal{M}_{S\perp}}(\theta^*) = 0 \Rightarrow ||\theta^*_{\mathcal{M}_{S\perp}}||_1=0$. 
        \subitem Desired bound: $\frac{1}{n} ||X\theta||_2^2 \geq \kappa_{\mathcal{L}} ||\theta||_2^2, \forall \theta \in \mathbb{C}$
    \item Assuming sub-Gaussian$(\sigma^2)$ noise $w$, we apply known results to bound $\frac{||X\theta||_2^2}{n}$, thus meeting the RE/RSC property if 1) $n > 64(\kappa_2 / \kappa_1)^2$ (so that our lower bound is valid on $||X\Delta||_2^2/n$) and 2) $\lambda_n \geq 2||\nabla \mathcal{L}(\theta^*)||_\infty$ (so that our set of $\Delta$, $\mathbb{C}$, is valid)
        \footnote{Note that the sub-Gaussian result truly gives us a bound $\frac{||X\theta||_2}{\sqrt{n}} \geq \kappa_1 ||\theta||_2 - \kappa_2\sqrt{\frac{\log p}{n}}||\theta||_1$. The result that $||\theta||_1 \leq 4\sqrt{s}||\theta_M||_2$ from $\theta^* \in \mathcal{M}$ and clever constant selectionas mentioned allows us to extend this to the necessary bound $||X\theta||_2 \geq \kappa_\mathcal{L} ||\theta||_2$.}

    \item We finish by identifying a satisfactory bound on $\lambda_n$ such that previous analysis, assuming $\hat\delta \in \mathbb{C}$ is valid, namely $\lambda_n \geq 2\mathcal{R}^*(\nabla\mathcal{L}(\theta^*))$. Recall that we assume $w \sim subG(\sigma^2)$, where $w$ is our random noise $w = Y-X\theta^*$. 
        \begin{align*}
            2\mathcal{R}^*(\nabla\mathcal{L}(\theta^*))
            &=
            2||\nabla \mathcal{L}(\theta^*)||_\infty
        \\
            &= 
            \frac{2}{n}|| X^T(y-X\theta^*)||_\infty
        \\
            &= 
            \frac{2}{n}|| X^Tw||_\infty
        \end{align*}
            
        Recall that WLOG $X$ is column-centered such that $\frac{||X_j||_2}{\sqrt{n}} \leq 1$: 

        \begin{align*}
            \Pr\left(|| X^Tw||_\infty \geq t\right)
            &\leq 
            \Pr\left( \sum_{j=1}^p |\iprod{X_j, w}| \geq t \right)
        \\
        &\leq 2p\exp{-\frac{nt^2}{2\sigma^2}}
        =
        2\exp{-\frac{nt^2}{2\sigma^2} + \log p}
        \end{align*}
        Informing $\lambda_n$ selection such that above holds with high probability with respect to $n, \sigma^2$, and constants. 
    \end{enumerate}
    
%{\bf Old Notes} \newline 

%We can define our model subspace more explicitly as $M(S) = \{\theta \in \mathbb{R}^p : \theta_{S^c} = 0\}$ (often using $S$ as shorthand for this subspace, even though truly $S \subseteq [p]$ represents the non-zero indices of $\theta^*$). Note $S_c = [p]\backslash S$. By assumption $\theta^* \in \mathcal{M}_S$, implying $\Pi_{\mathcal{M}_\perp}(\theta^*) = 0 \Rightarrow \mathcal{R}(\theta^*_{\mathcal{M}_\perp})=0$ and $\Delta \in \mathbb{C}:=\{\Delta \in \mathbb{R}^p : ||\Delta_{\mathcal{M}_\perp}||_1 \leq 3||\Delta_\mathcal{M}||_1 \}$ (assuming appropriate selection of $\lambda_n$, which we postpone until the final step.) \newline 

\subsubsection{OLS with partial sparsity, $\ell_1$ penalty ($\mathbb{B}_q$ setting)}

{\it Notation, $s\equiv|S|$.}\newline

We now no longer have (or assume) $\theta^* \in \mathcal{M}$, as we are approximating a weakly sparse vector with true sparsity. Thus $\mathcal{R}(\theta^*_{\mathcal{M}_\perp})\neq0$ (as $\Pi_{\mathcal{M_\perp}}(\theta^*)\neq 0$). Our results from the exact sparisy setting holds that for $\lambda_n = 4\sqrt{\frac{\sigma^2 \log p}{n}}$, $\lambda_n \geq ||\nabla\mathcal{L}(\theta^*)||_1$ with probability $1-c_1 \exp{-c_2n\lambda_n^2}$ (along with the subspace compatibility and decomposability arguments). It then only remains to demonstrate the RSC condition holds. \newline 

Now we cannot conclude $||\theta||_1 \leq 4\sqrt{s}||\theta_\mathcal{M}||_2$, as the space $\mathbb{C}$ is not a cone where $||\Delta_\mathcal{M}|| \leq 3||\Delta_{\mathcal{M}_\perp}||$ but now a star-shaped set (so $||\Delta||_1 \leq 4\sqrt{s}||\Delta||_2$ does not hold).  The essence of this proof is now a control over the radius of the $\mathbb{B}_q(R_q)$ ball that we use to intersect $\mathbb{C}$. 
\newline 


\subsubsection{GLM with Logit Link, $\ell_1$ penalty}

The Taylor Series approximation in the OLS setting is no longer exact such that $\delta\mathcal{L}(\theta^*, \Delta) \neq \Delta^T \nabla \mathcal{L}(\theta^*)$. We will appeal to a similar sub-Gaussian argument but must first more carefully bound $\delta\mathcal{L}$ and parameterize our design matrix (see GLM1 assumptions, bounding $\lambda_{min}\geq \kappa_\ell > 0$ of $cov(x_i)$ and sub-Gaussianity of $v^T x_i, \ \forall v \in \mathbb{R}$ with sub-Guassianity parameter $\sigma^2\leq \kappa_u ||v||_2^2$). 

\newpage  
\subsection*{Further Review}

Pressing/Paper Material 
\begin{itemize}	
    \item Figure 1 (3-dimensional error vector, geometric intuition behind $\mathbb{C}(M, \overline{M}^\perp, \theta)$ when $\theta^* \in M, \theta^*\not\in M$ respectively 
\end{itemize}

(Newly) Conceptual
\begin{itemize}
    \item Review equivalency of lasso and basis pursuit de-noising
    \begin{itemize}
        \item See \url{https://www.cs.cornell.edu/courses/cs6220/2017fa/CS6220_Lecture21_2.pdf}
        \item {\it Possibly only of historical relevance}
    \end{itemize}
    \item Definition/abstraction of a (topological) closure:
    \begin{itemize}
        \item For subspace $M \subseteq \mathbb{R}^p, \left(M^\perp \right)^\perp \equiv \overline{M}$ is a closure of $M$ (more accurately is a closure operator on $M$) 
        \item With the exception of discussion of low-rank matrices and the nuclear norm, we can work with $M = \overline{M}$ and not concern ourselves with the concept of topological closures 
    \end{itemize}
\end{itemize}

\subsubsection{Questions}
\begin{itemize}
    \item Where does equation 18 come from? I am familiar with explicit forms of the remainder in univariate expansions, but I'm not familiar with the multivariate remainder expression (and am unsure how this specific expression is derived)
    \subitem Similarly, why is the Taylor series of the OLS loss function exact such that $\delta\mathcal{L}(\theta^*, \Delta) = \iprod{\Delta, \frac{1}{n}X^TX\Delta}$? How is the hessian form introduced?
\item End of page 12, how does this correspond to a restricted eigenvalue condition? 
\end{itemize}

\newpage 
\subsection*{Misc. Proofs}

\subsubsection{Proof of equivalent dual-norm definitions}

WTS $\sup_{u \in \mathbb{R}^p\backslash\{0\} } \frac{\langle u, v \rangle}{\mathcal{R}(u)} = \sup_{\mathcal{R}(u)\leq 1} \langle u,v \rangle$ (i.e. equivalence of two definitions of dual norm $R^*(u)$)

\begin{gather*}
    \sup_{u \in \mathbb{R}^p\backslash\{0\} } \frac{\langle u, v \rangle}{\mathcal{R}(u)} 
    =
    \sup_{u \in \mathbb{R}^p\backslash\{0\} } \frac{u^Tv}{\mathcal{R}(u)}
    =
    \sup_{\mathcal{R}(w)=1} w^{T}v
    \leq 
    \sup_{\mathcal{R}(w)\leq1} w^{T}v
\end{gather*}

To demonstrate equivalence we prove the inequality in the opposite direction:

\begin{gather*}
%	A := \{u : \mathcal{R}(u) \leq 1\} \subseteq \{u : ||u|| \leq 1, u \neq 0\} =: B 
%	\\
%	\Rightarrow 
 	\sup_{\mathcal{R}(u)\leq 1} \langle u,v \rangle 
	\leq 
	\sup_{\mathcal{R}(u)\leq 1}\frac{\langle u,v \rangle}{\mathcal{R}(u)}
	\leq 
	\sup_{u \in \mathbb{R}^p\backslash\{0\} } \frac{\langle u, v \rangle}{\mathcal{R}(u)}
\end{gather*}


\subsubsection{Prove $\ell_\infty$ is dual-norm of $\ell_1$}

Let $u,v \in \mathbb{R}^n$ and $v_{(n)}$ = $\max_i |v_i|, i \in [n]$

\begin{gather*}
    \sup_{||u||\leq 1} \iprod{u,v}
    \leq 
    \sup_{||u||\leq 1} \sum_i |u_i||v_i|
    \leq 
    \sup_{||u|| \leq 1}  v_{(n)} \sum_i |u_i|
    \leq 
    ||v||_\infty 
\end{gather*}

To demonstrate equivalence we prove the inequality in the opposite direction. Assume $v_j = ||v||_\infty$ (i.e. j indexes the maximum value in $v$), let $s \in \mathbb{R}^n, s_j = \sign(v_j)$ and $0$ elsewhere. Clearly $||s|| \in \{u: ||u||\leq 1\}$ and 

\begin{gather*}
    \sup_{||u||\leq 1} \iprod{u, v}
    \geq 
    \iprod{s, v}
    =
    v_j 
    =
    ||v||_\infty
\end{gather*}

\subsubsection{Dual-norm of group-structured norm}
    \footnote{Forgoing some of Sahand's nicer/more concise notation for clarity/confirmation of understanding on my end}

{\it See 2.2 Example 2 (pg 5) for more details on group-sparsity} \newline 

Consider $\theta \in \mathbb{R}^p$ such that  $[p]$ is partitionable as $\mathcal{G} = \{G_1, G_2, ..., G_{N_\mathcal{G}}\}$ (i.e. $\mathcal{G}$ indexes a specific partinioning of $[p]$ of length $N_{\mathcal{G}}$).
Consider similarly the vector $\overrightarrow{\alpha} = (\alpha_1, ..., \alpha_{N_\mathcal{G}}), \alpha_i \in [1, \infty]$. 

We define the group norm of $\theta$ as 

\begin{gather*}
    ||\theta||_{\mathcal{G}, \overrightarrow{\alpha}}
    := 
    \sum_{t=1}^{N_\mathcal{G}} ||\theta_{G_t}||_{\alpha_t}
\end{gather*}

That is, for each $t$th partition, we take the $\alpha_t$-norm of this partition of $\theta$. \newline 

We want to identify the dual norm of $||\cdot||_{\mathcal{G}, \overrightarrow\alpha}$ (limited to $\overrightarrow{\alpha} \in [2,\infty]^{N_\mathcal{G}})$. For convenience we drop the $\mathcal{G}$ index for the specific partitioning for the group norm. \newline 

Define $\overrightarrow{\alpha^*}$ such that $\frac{1}{\alpha_t} + \frac{1}{\alpha^*_t} = 1$. 

\begin{gather*}
    \sup_{||u||_{\overrightarrow\alpha} \leq 1} 
    \iprod{u,v}
%    \leq 
%    \sup_{||u||_{\overrightarrow\alpha} \leq 1} 
%    \frac{\iprod{u,v}}{ \max_{t} ||u_{G_t}||_{\overrightarrow\alpha_t}}    
    =
    \sup_{||u||_{\overrightarrow\alpha} \leq 1} 
    \sum_t 
    \iprod{u_{G_t}, v_{G_t}}
    \stackrel{\text{H\"{o}lder's}}{\leq}
    \sup_{||u||_{\overrightarrow\alpha} \leq 1} 
    \sum_t 
    ||u_{G_t}||_{\alpha_t}
    ||v_{G_t}||_{\alpha^*_t}
\\
    \leq 
    \sup_{||u||_{\overrightarrow\alpha} \leq 1} 
    \max_w ||v_{G_w}||_{\alpha^*_w}
    \sum_t 
    ||u_{G_t}||_{\alpha_t}
    =
    \max_w ||v_{G_w}||_{\alpha^*_w}
\end{gather*}

Similarly define $k := \argmax_w ||v_{G_w}||_{\alpha^*_w}
$ and $s$ such that for $k \in p$, $||s_{G_s}||_{\alpha_s}=1$ and $s_i=0, \forall i \not\in G_s$. Clearly again $||s||_{\overrightarrow{\alpha}} = 1$:

\begin{gather}
        \sup_{||u||_{\overrightarrow\alpha} \leq 1} 
        \iprod{u, v}
        \geq 
        \iprod{s, v}
        =
        \max_w ||v_{G_w}||_{\alpha^*_w} 
        \qed 
\end{gather}







\newpage
\begin{tcolorbox}
\vspace{4mm}
\bf \LARGE \chapter{Elements of Statistical Learning}
\vspace{4mm}
\end{tcolorbox}

\section*{Chapter 17: Undirected Graphical Models}

%\subsection*{Questions, Notes, Properties, Definitions, etc. }


\subsection*{Overview/Intro}
\begin{enumerate}
    \item \textit{Omitting information that is shared with Wasserman chapters, which are more introductory than ESL's discussion of graphical algorithms}
    \item \textcolor{red}{\sout{We define clique potentials (similar to the Wasserman chapter) as affinities (affine functions?). Since we express the density function as product of clique functions, I assume these must be positive functions defined on each clique. Are there other constraints or definitions about these affinities?}}
    	\begin{itemize}
            \item -- My (still limited, possibly incorrect) understanding is that these are simply positive functions that are context-specific or user-defined. Wainwright/Jordan describes these as {\it compatibility functions} which are defined based on a model. 
    	    \item -- An example of a simple compatibility functions is a binary decision rule that is 0 for any configuration of vertices/values that occurs with probability 0, and 1 otherwise. That is for clique $(x_i, x_j, x_k)$ with known impossible configuration $(z_i, z_j, z_k)$, the compatibility function is equivalent to the boolean $\lnot z_i \lor \lnot z_j \lor \lnot z_k$.
        \end{itemize}
    \item Hammserley-Clifford: From ESL, states that we can equivalently represent the joint density function of a graph ($\mathcal{G} = (V,E)$) $f_V$ as a product of clique affinities {\bf for Markov networks with positive (i.e. non-zero) distributions}
    \begin{itemize}
        \item -- That is for set of maximal cliques\footnote{
            From Wainwright, Jordan (2008), I believe we can also use a set of non-maximal cliques (or any combination of cliques) in this factorization when convenient
            } $\mathcal{C}$ and graph $\mathcal{G} = (V, E)$, $f_\mathcal{G}(x) \propto \prod_{C \in \mathcal{C}}\psi_C (x_C)$
    \end{itemize}

    \item \textcolor{red}{Another question re: HC theorem/clique-factorization of the density function "implies a graph with independence properties defined by the cliques...". So this is true even with overlap in the maximal cliques (or within whatever cliques are used to factorize the density function)? }
        
    
\end{enumerate}

\subsection*{17.3 Continuous Variables}

\begin{itemize}
    \item Assuming a Gaussian distribution describing our nodes allows for some convenient estimation properties to arise in graph structure and/or parameter estimation:
    \begin{itemize}
        \item Generally (or perhaps vaguely), Gaussian graphical models allow estimation problems to be constructed conveniently as linear regression problems (see 17.3.1 for regression estimating equations, 17.3.2 for lasso regression for structure estimation)
        \item For $\mathbf{X} \sim N(\boldsymbol{\mu}, \boldsymbol\Sigma)$ and $\boldsymbol\Theta = \boldsymbol\Sigma^{-1}$, $\theta_{ij}=0 \Leftrightarrow X_i \perp X_j | \text{rest}$
    \end{itemize}
\end{itemize}

\subsubsection{17.3.1 Estimating Equations for Graphs with Known Structure}

\textcolor{red}{See work/question for Exercise 17.5 for question of distinction between $W, S$ matrices in presented algorithm}

\newpage 

\subsection*{Exercises}

\begin{enumerate}
\item 
    \begin{enumerate}[label=(\alph*)]
        \item Maximum Cliques:
        $\{X_1, X_2, X_3\}$
        , $\{X_1, X_4 \}$
        , $\{X_3, X_4 \}$
        , $\{X_5, X_6 \}$
        \item Conditional Independencies:
        Trivially, any $X_i, X_j$ without an edge is independent conditional on all other nodes\\ 
        By separation, a list (with some redundancies): \\  
        $X_1 \perp X_5 | X_6$ \\         
        $X_2 \perp X_{3,4} | X_1$
        $X_2 \perp X_6 | X_5$ \\ 
        $X_3 \perp X_{1, 2,5,6} | X_4$ \\
        $X_4 \And X_3 \perp X_{2,5,6} | X_1$ \\
        $X_5 \perp X_{1,2,3,4} | X_6$        
    \end{enumerate}

    
\item {\it Omitted }

\item 
    \begin{enumerate}[label=(\alph*)]
        \item 
        Note $\Sigma \in \mathbb{R}^{p \times p}$ can be partitioned as 
        
        \begin{align*}
            \Sigma = 
            \begin{bmatrix}
            \Sigma_{aa} & \Sigma_{ab} \\ 
            \Sigma_{ba} & \Sigma_{bb}
            \end{bmatrix}
        \end{align*}
        
        for $\Sigma_{aa} \in \mathbb{R}^{2 \times 2}; \ \Sigma_{ab}, \Sigma_{ba}^T \in \mathbb{R}^{2 \times p-2}; \ \Sigma_{bb} \in \mathbb{R}^{p-2 \times p-2}$. 
        
        We can partition $\Theta \equiv \Sigma^{-1}$ and use known properties of the inverses of partitioned matrices to demonstrate:
        
        \begin{gather*}
                \Theta 
                =
            \begin{bmatrix}
                \Theta_{aa} & \Theta_{ab} \\ 
                \Theta_{ba} & \Theta_{bb}
            \end{bmatrix}
                = 
                \Sigma^{-1}
                =
            \begin{bmatrix}
                \left( \Sigma_{aa} - \Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba} \right)^{-1} & f_2\left(\Sigma \right) \\ 
                f_3\left(\Sigma \right) & f_4\left(\Sigma \right)
            \end{bmatrix}
        \end{gather*}
        
        where we see $\Theta_{aa} = \Sigma_{a,b}^{-1}$.

    \item 
        \begin{align*}
            \Sigma_{a,b}
            &=
            \begin{bmatrix}
                Cov(X_1, X_1 | \text{rest})
                & Cov(X_1, X_2 | \text{rest})
                \\
                Cov(X_2, X_1 | \text{rest})
                & Cov(X_2, X_2 | \text{rest})   
            \end{bmatrix}
            \\
            &\Rightarrow 
            \Sigma_{a,b}^{-1}
            =
            \Theta_{aa}
            \propto 
            \begin{bmatrix}
                Cov(X_2, X_2 | \text{rest})
                & -Cov(X_1, X_2 | \text{rest})
                \\
                -Cov(X_2, X_1 | \text{rest})
                & Cov(X_1, X_1 | \text{rest})   
            \end{bmatrix}
        \end{align*}

        The off-diagonals of $\Theta_{aa}=0 \Rightarrow \rho_{1, 2|\text{rest}}=0$. Noting that we've selected $a,b$ such that $X_a=(X_1, X_2)$ as $j=1, k=2$ WLOG (i.e. the result holds $\forall j\neq k$) completes the argument.


    \item\footnote{Wikipedia actually has a nice  walkthrough of the calculation of the partial conditional correlation formula, at  \url{https://en.wikipedia.org/wiki/Partial_correlation\#Using_matrix_inversion}}
        We can (less lazily compared to 3b) calculate the partition of precision matrix $\Theta_{aa}$, where $\theta_{ij} = Cov(X_i, X_j | \text{rest})$: 

        \begin{align*}
        \Sigma^{-1}_{a,b}
        =
        \Theta_{a,b}
        &=
        \frac{1}{\theta_{ii}\theta_{jj} - \theta_{ij}^2}
        \begin{bmatrix}
            \theta_{jj}
            & -\theta_{ij}
            \\
            -\theta_{ji}
            & \theta_{ii}
        \end{bmatrix}
        \end{align*}

        Then 
        $\text{diag}(\Theta)^{1/2}
        =
        \begin{bmatrix}
        \frac{1}{\sqrt{\theta_{jj}}} & 0 \\
        0 & \frac{1}{\sqrt{\theta_{ii}}}
        \end{bmatrix}$ and: 


    \begin{align*}
        \mathbf{R}
        =
        \text{diag}(\Theta)^{-1/2}
        \cdot \Theta \cdot
        \text{diag}(\Theta)^{-1/2}
        &=
        \begin{bmatrix}
         \sqrt{\theta_{jj}} & -\frac{\theta_{ij}}{\sqrt{\theta_{jj}}} \\ 
         -\frac{\theta_{ji}}{\sqrt{\theta_{ii}}} & \sqrt{\theta_{ii}}
        \end{bmatrix}
        \begin{bmatrix}
        \frac{1}{\sqrt{\theta_{jj}}} & 0 \\
        0 & \frac{1}{\sqrt{\theta_{ii}}}
        \end{bmatrix}
    \\
        &= 
        \begin{bmatrix}
            1 
            & 
            -\frac{\theta_{ij}}{\sqrt{\theta_{jj}\theta_{ii}}} 
            \\ 
            -\frac{\theta_{ji}}{\sqrt{\theta_{jj}\theta_{ii}}} 
            & 
            1 
        \end{bmatrix}
    \end{align*}

    where we see $r_{ij} = \rho_{ij}|\text{rest}$ by definition of $\rho_{ij}|\text{rest} =-\frac{\theta_{ij}}{\sqrt{\theta_{jj}\theta_{ii}}}$. 
    \end{enumerate}

    
    \item 
    Notation: Let $\{X_3, X_4, ..., X_p\} = X^* = X_{3, ..., p}$: 
    
    \begin{align*}
        X_1 \perp X_2 | X^*
        &\Leftrightarrow 
        f_{X_1, X_2 | X^*} = f_{X_1 | X^*} f_{X_2 | X^*}
        \\
        f_{X_1, X_2 | X^*}
        &=
        \frac{
        f_{X_1, X_2, X^*}
        }{
        f_{X^*}
        }
        =
        \frac{
        f_{X_1 | X_2, X^*} f_{X_2|X^*}f_{X^*}
        }{
        f_{X^*}
        }
        \\
        &= f_{X_1 | X_2, X^*} f_{X_2 | X^*}
        \\ 
        &=
        f_{X_1 | X^*} f_{X_2 | X^*}
    \end{align*}


    \item 
    From 17.3.1 (and work below under Misc. Claims), the gradient of the log-likelihood for our Gaussian graphical model is 
    $\nabla \ell(\Theta; \mathbf{X}) = \Theta^{-1} - S$ (with $\Gamma = \mathbf{0}$, as all edges are known and present).  $S = \Theta^{-1}$ and assuming a similar partitioning scheme as in 17.3.1: 

    \begin{align*}
    S 
    = 
    \Theta^{-1}
    &\Rightarrow 
    \begin{bmatrix}
        S_{11} & s_{12} \\ 
        s_{21} & s_{11}
    \end{bmatrix}
        \begin{bmatrix}
        \Theta_{11} & \theta_{12} \\ 
        \theta_{21} & \theta_{22}
    \end{bmatrix}
    =
    \begin{bmatrix}
        \mathbf{I} & 0 \\ 
        0^T & 1
    \end{bmatrix}
    \\
    &\Rightarrow 
    S_{11}\theta_{12} + s_{12}\theta_{22} = 0
    \\
    &\Rightarrow 
    s_{12} 
    = 
    -\frac{
    S_{11}\theta_{12}
    }{
    \theta_{22}
    }
    =
    S_{11}\beta 
    \end{align*}
    
    \begin{align*}
        \Theta - S \stackrel{!}{=}0
        &\Rightarrow
        s_{11} - s_{12} = 0
        \Leftrightarrow 
        S_{12}\beta - s_{12} = 0 \qed 
    \end{align*}
    \textcolor{red}{This problem feels a bit weird. In order to use this substitution, the gradient gives us $s_{12} - s_{12}$, no? Which is trivially true. IN the 17.3.1 derivation, we use $W$. Not sure if I truly understand the distinction of $S, W$. }

    \item Omitted, result follow nearly immediately from 17.16 (or the provided 17.41-2) and a profile-likelihood style argument

    \item {\it Incomplete (Programming)} 

    \item {\it Incomplete (Programming)}

    \item 
\end{enumerate}


\newpage 

\begin{tcolorbox}
    \vspace{4mm}
    \Large \bf Misc. Claims, Proofs, Work
    \vspace{4mm}
\end{tcolorbox}

{\bf Claim:} The log-likelihood of $N$ random samples of a $k-$dimenisonal multivariate Gaussian with mean $\boldsymbol\mu$ and covariance matrix $\boldsymbol\Sigma = \boldsymbol\Theta^{-1}$ can be expressed as (noting $\mathbf{x} \in \mathbb{R}^k; \boldsymbol\Theta, \mathbf{S} \in \mathbb{R}^{k \times k}$) for sample covariance matrix $\mathbf{S} =  (\mathbf{x}-\mu)(\mathbf{x}-\mu)^T$:

\begin{align*}
    \text{WTS }    
    \ell(\boldsymbol\Theta)
    &= \textcolor{red}{(\propto?)}
    \log \det \boldsymbol\Theta
    -
    \trace(\mathbf{S}\boldsymbol\Theta )
    \\
    \\
    \ell(\boldsymbol\Theta)
    &= 
    \sum_{i=1}^N
    \log\left[
    (2\pi)^{-k/2} 
    \det(\boldsymbol\Sigma)^{-1/2}
    \exp{
    -\frac{1}{2}
    (\mathbf{x}_i-\boldsymbol\mu)^T
    \boldsymbol\Sigma^{-1}
    (\mathbf{x}_i-\boldsymbol\mu)
    }
    \right]
\\
    &= 
    \sum_{i=1}^N
    \log\left[
    (2\pi)^{-k/2} 
    \det(\boldsymbol\Theta)^{1/2}
    \exp{
    -\frac{1}{2}
    (\mathbf{x}_i-\boldsymbol\mu)^T
    \boldsymbol\Theta
    (\mathbf{x}_i-\boldsymbol\mu)
    }
    \right]   
\\
    &\textcolor{red}{\propto}
    \log \det \boldsymbol\Theta 
    -\frac{1}{2}
    \sum_{i=1}^N
    (\mathbf{x}_i-\boldsymbol\mu)^T
    \boldsymbol\Theta 
    (\mathbf{x}_i-\boldsymbol\mu)    
\\
    &
    \stackrel{
    \overline{\mathbf{x}} = \hat{\mu}_{MLE}
    }
    {
    \textcolor{red}{\propto}
    }
    \log \det \boldsymbol\Theta 
    -
    \frac{1}{2}
    \sum_{i=1}^N
    (\mathbf{x}_i-\overline{\mathbf{x}})^T
    \boldsymbol\Theta 
    (\mathbf{x}_i-\overline{\mathbf{x}})   
\\
    &\textcolor{red}{\stackrel{?}{=}}
    \log \det \boldsymbol\Theta 
    -
    \trace(\mathbf{S}\boldsymbol\Theta) 
\end{align*}

\newpage 
\begin{tcolorbox}
\vspace{4mm}
\bf \LARGE \chapter{All of Statistics (Wasserman)} 
\vspace{4mm}
\end{tcolorbox}



\section*{Chapter 17: Directed Graphs}

\subsection*{Questions, Definitions, Notes, Properties, etc.}

\begin{enumerate}
    \item An \textbf{unshielded collider} is any collider whose "pointing nodes" are disconnected/non-adjacent:
    \begin{figure}[h]
    \centering 
    \begin{tikzpicture}
        \begin{scope}[every node/.style={circle, thick, draw}]
        \node (A) at (0,3) {A};
        \node (B) at (2.5,4) {B};
        \node (C) at (5,3) {C};
        \end{scope}

        \begin{scope}[>={stealth[black]}]
        \path [->] (A) edge node {} (B);
        \path [->] (C) edge node {} (B);
        \end{scope}
    \end{tikzpicture}
    \ is unshielded, whereas
    \end{figure}
    
    \begin{figure}[h]
    \centering
    \begin{tikzpicture}
    \begin{scope}[every node/.style={circle,thick,draw}]
        \node (A) at (0,3) {A};
        \node (B) at (2.5,4) {B};
        \node (C) at (5,3) {C} ;
    \end{scope}
    
    \begin{scope}[>={stealth[black]}]
        \path [->] (A) edge[bend left=10] node {} (B);
        \path [->] (C) edge[bend right=10] node {} (B);
        \path [->] (A) edge[bend right=10] node {} (C);
    \end{scope}
    \end{tikzpicture}
    \ is shielded 
    \end{figure}

    \item \label{MarkovGraphDef} A distribution $\Psymb$ for nodes $V = \{X_1, ..., X_k\}$ is Markov wrt a graph $\mathcal{G}$ if $f(v) = \prod_{i=1}^k f(x_i | \pi_i)$ for $\pi_i$ parents for node $X_i$. Also written as $\Psymb \in M(\mathcal{G})$

    \item \label{MarkovCondition} The \textbf{Markov Condition} (or Local Markov property) for distribution $\Psymb$ holds if $\forall X_i \in V, \mathcal{G}=(V,E)$ (or for $X_i$ simply as random variables) if $W \perp \tilde{W} | \pi_W$, where $\tilde{W}$ includes all other nodes/variables besides $\pi_W$ and descendants of $W$ 


    \item The following items from this list are equivalent characterizations $\mathcal{G}$: \ref{MarkovGraphDef} $\Leftrightarrow$
    \ref{MarkovCondition} 

    \item For disjoint sets of verticies $A,B,C$: $A,B$ are d-separated by $C \Leftrightarrow A \perp B | C$ 

    \item $\mathcal{G}_1, \mathcal{G}_2$ are \textbf{Markov Equivalent} $\Leftrightarrow \mathcal{I}(\mathcal{G}_1) = \mathcal{I}(\mathcal{G}_2) \Leftrightarrow \text{skeleton}(\mathcal{G}_1) = \text{skeleton}(\mathcal{G}_2) \land $ both graphs have the same unshielded colliders 
\end{enumerate}


\boxsubsection*{Exercises}

\subsubsection{1}

WTS (17.1) and (17.2) are equivalent: $X\perp Y |Z$ indicates
$f_{X,Y|Z}=f_{X|Z}f_{Y|Z} \Leftrightarrow f_{X|Y,Z} = f_{X|Z}$

\begin{gather*}
    f_{X|Y,Z}
    =
    \frac{ f_{X, Y, Z} }{ f_{Y, Z} }
    =
    \frac{ f_{X, Y, Z} }{ f_{Y|Z} f_Z }
    =
    \frac{ f_{X, Y | Z} }{ f_{Y|Z}}    
    \stackrel{X \perp Y |Z}{=}
    \frac{ f_{X| Z} f_{Y|Z} }{ f_{Y|Z}}    
    =
    f_{X|Z}
\end{gather*}

\subsubsection{2}

\begin{gather*}
    \Psymb (U\leq u, Y \leq y | Z)
        =
    \Psymb (X \leq h^{-1}(u), Y \leq y | Z)
    =
    \Psymb (X \leq h^{-1}(u)|Z)
    \Psymb ( Y \leq y | Z)
    =
    \Psymb (U \leq u|Z)
    \Psymb ( Y \leq y | Z)    
\end{gather*}


\begin{enumerate}[label=(\alph*)]
    \item \textit{Trivial} 
    
    \item WTS $X \perp Y|Z \land U=h(X) \Rightarrow U \perp Y | Z$:

    \begin{gather*}
        f_{U,Y|Z}(u,y) 
        =
        f_{X,Y|Z}(h^{-1}(u), y) \left| \frac{\partial h^{-1}(u)}{\partial u} \right|
        =
        f_{X|Z}(h^{-1}(u)) \left| \frac{\partial h^{-1}(u)}{\partial u} \right|
        f_{Y|Z}(y) 
        =
        f_{U|Z}(u)f_{Y|Z}(y)
    \end{gather*}
    

    \item 
    WTS $X\perp Y | Z \land \ U=h(X) \Rightarrow X \perp Y | (Z,U)$: 

    \begin{gather*}
    f_{X, Y | Z, U}
    =
    f_{Y|X, U, Z} f_{X|U, Z}
    \stackrel{U=h(X)}{=}
    f_{Y|U, Z} f_{X|U, Z}     
    \end{gather*}
    
    \item WTS $X \perp Y |Z \land X \perp W | (Y,Z) \Rightarrow X \perp (W,Y) | Z$

    \begin{gather*}
        f_{X, W, Y | Z} 
        =
        f_{W | X, Y, Z} f_{X, Y|Z}
        \stackrel{X \perp W | (Y, Z)}{=}
        f_{W | Y, Z} f_{X | Y, Z} 
        \stackrel{X \perp Y | Z}{=}
        f_{W | Y, Z} f_{X | Z} 
    \end{gather*}

    \item WTS $X \perp Y |Z \land X \perp Z |Y \Rightarrow X \perp (Y,Z)$ (without assumption of positivity for all involved probabilities)

    \begin{gather*}
        f_{X, Y, Z}
        =
        f_{Z, Y| X} f_X
        =
        f_{}
    \end{gather*}

\end{enumerate}

\subsubsection{3}

\textit{Omitted}

\subsubsection{4}

Consider the (re-created) DAG's in 17.6 with no colliders present: 

\begin{figure}[h]
\centering 
\begin{tikzpicture}
    \node (X) at (0,0)  {X};
    \node (Y) at (1,0) {Y};
    \node (Z) at (2,0) {Z};
    \path [->] (X) edge node {} (Y);
    \path [->] (Y) edge node {} (Z);
\end{tikzpicture}
\end{figure}

\begin{figure}[h]
\centering 
\begin{tikzpicture}
    \node (X) at (0,0)  {X};
    \node (Y) at (1,0) {Y};
    \node (Z) at (2,0) {Z};
    \path [<-] (X) edge node {} (Y);
    \path [<-] (Y) edge node {} (Z);
\end{tikzpicture}
\end{figure}

\begin{figure}[h]
\centering 
\begin{tikzpicture}
    \node (X) at (0,0)  {X};
    \node (Y) at (1,0) {Y};
    \node (Z) at (2,0) {Z};
    \path [<-] (X) edge node {} (Y);
    \path [->] (Y) edge node {} (Z);
\end{tikzpicture}
\end{figure}

WTS $X \perp Z|Y$

\begin{enumerate}
    \item The Markov Condition directly implies $Z \perp X | Y$ ($Z$ is independent of all nodes excluding its parents \{$Y$\} and descendants \{$\varnothing$\} conditioned upon its parents)
    \item $X \perp Z |Y$ again by Markov condition (same as above)
    \item Markov Condition again in a similar way wrt either $X,Z$ (both have empty set descendants, $Y$ as parent) 
\end{enumerate}

\subsubsection{5}

\begin{figure}[H]
\centering 
\begin{tikzpicture}
    \node (X) at (0,0)  {X};
    \node (Y) at (1,0) {Y};
    \node (Z) at (2,0) {Z};
    \path [->] (X) edge node {} (Y);
    \path [<-] (Y) edge node {} (Z);
\end{tikzpicture}
\end{figure}

Consider now the above DAG with a collider present, WTS $X \perp Z$ and $X \not\perp Z | Y$:  \newline  
  
$X\perp Z$ follows from Markov Condition ($Z$ is not a descendant of $X$, $X$ has no parental nodes) or noting that $X,Z$ are d-separated (specifically only when \textbf{not} conditioning on $Y$). \newline 

We know that $X \perp Z | Y \Leftrightarrow X, Z$ are d-separated. We note by definition $X,Z$ are d-connected conditioning on $Y$ and thus $X \not\perp Z | Y$. 

\subsubsection{6}

{\textit Simulations omitted}

\begin{gather*}
    f_{X,Y,Z}
    =
    f_{Z|Y}f_{Y|X}f_X
\end{gather*}

\subsubsection{7}

{\it DAG Omitted}

Consider the set of nodes $V = \{Z_j, X, Y_i\}$, $i,j=[4]$: 

\begin{gather*}
    f_{V}
    =
    f_X
    \prod_{k=1}^4
    f_{Z_k}f_{Y_k | Z_k, X}
%    f_{Z_1}f_{Z_2}f_{Z_3}f_{Z_4}
%    f_X
%    f_{Y_1 | Z_1, X}
%    f_{Y_2 | Z_2, X}
%    f_{Y_3 | Z_3, X}
%    f_{Y_4 | Z_4, X}
\end{gather*}

$X \perp Z_j, \forall j \in [4]$ follows directly from the Markov Condition, as no $Z_j$ is a parent or descendent of $X$. We could also note $X, Z_j$ collide at $Y_j$ and are d-separated, thus $X\perp Z_j$ but $X \not\perp Z_j | Y_j$ ($\forall j \in [4]$). 


\subsubsection{8}

\begin{enumerate}[label=(\alph*)]
    \item 
    \begin{gather*}
        \Psymb(Z|Y)
        =
        \frac{ \sum_{x=0}^1 \Psymb(Z, Y, X=x)}
        { \sum_{x=0}^1 \Psymb(Y, X=x) }
        =
        \frac{ \sum_{x=0}^1 \Psymb(Z|Y, X=x)\Psymb(Y|X=x)\Psymb(X=x)}
        { \sum_{x=0}^1 \Psymb(Y, X=x) }
    \end{gather*}
    
    Result omitted, calculation follows from expression above (all information known from given information)
    
    \item Omitted 
    \item {\it Incomplete}
    \item Omitted   
\end{enumerate}



\subsubsection{9}

\begin{enumerate}[label=(\alph*)]
    \item {\it Incomplete}
\end{enumerate}

\boxsection*{Chapter 18: Undirected Graphs}

\boxsubsection*{Questions, Definitions, Notes, Properties, etc.}

\begin{enumerate}
    \item Pairwise Markov property for $\mathcal{G}=(V,E)$,  $X,Y \subseteq V$, and $V\backslash\{X,Y\}$ is all nodes excluding $X,Y$:
    
    {\centering No edge exists between $X,Y \Leftrightarrow X \perp Y | V\backslash\{X,Y\}$ \par}


    \item The Global Markov states for sets of vertices $A, B, C \subseteq V$ in graph $\mathcal{G}$:   

    {\centering 
    $A \perp B | C 
    \Leftrightarrow $
    $C$ separates $A,B$
    \par}

    \item $M_\text{ pair}(\mathcal{G})
    =
    M_\text{global}(\mathcal{G})$
\end{enumerate}

\boxsubsection*{Exercises}

\subsubsection{1}

\begin{figure}[H]
\centering 
\begin{tikzpicture}
    %(a)
    \node (A) at (0.3, 0) {A)};
    \node (X11) at (1, 0) {$X_1$};
    \node (X21) at (2, 0) {$X_2$};
    \node (X31) at (3, 0) {$X_3$};
    \path [-] (X11) edge node {} (X21); 
    \path [-] (X21) edge node {} (X31); 

    %(b)
    \node (B) at (4.3, 0) {B)};
    \node (X12) at (5, 0) {$X_1$};
    \node (X22) at (6, 0) {$X_2$};
    \node (X32) at (7, 0) {$X_3$};
    \path [-] (X22) edge node {} (X32); 

    %(c)
    \node (C) at (8.3, 0) {C)};
    \node (X13) at (9, 0) {$X_1$};
    \node (X23) at (10, 0) {$X_2$};
    \node (X33) at (11, 0) {$X_3$};
\end{tikzpicture}
\end{figure}

All three relationships also hold trivially for the graph in (C). 

\subsubsection{2}

\begin{figure}[H]
\centering
    \begin{tikzpicture}
        \node (A) at (-1, 0) {A)};
        \node (X1A) at (0, 0) {$X_1$};
        \node (X2A) at (1, 0) {$X_2$};
        \node (X3A) at (2, 0) {$X_3$};
        \node (X4A) at (3, 0) {$X_4$};
        \path [-] (X1A) edge node {} (X2A);
        \path [-] (X2A) edge node {} (X3A);
        \path [-] (X3A) edge node {} (X4A);
    \end{tikzpicture}
\end{figure}


\begin{figure}[H]
\centering
    \begin{tikzpicture}
        \node (B) at (-1, 0) {B)};
        \node (X1B) at (0, 0) {$X_1$};
        \node (X2B) at (2, 0) {$X_2$};
        \node (X3B) at (1, -1) {$X_3$};
        \node (X4B) at (1, 0) {$X_4$};
        \path [-] (X2B) edge node {} (X4B);
        \path [-] (X4B) edge node {} (X1B);
        \path [-] (X4B) edge node {} (X3B);
    \end{tikzpicture}
\end{figure}



\begin{figure}[H]
\centering
    \begin{tikzpicture}
        \node (C) at (-1, 0) {C)};
        \node (X1C) at (3, 0) {$X_1$};
        \node (X2C) at (0, 0) {$X_2$};
        \node (X3C) at (1, 0) {$X_3$};
        \node (X4C) at (2, 0) {$X_4$};
        \path [-] (X2C) edge node {} (X3C);
        \path [-] (X4C) edge node {} (X1C);
        \path [-] (X4C) edge node {} (X3C);
    \end{tikzpicture}
\end{figure}

\subsubsection{3}

\begin{enumerate}[label=(\alph*)]
    \item $X_1 \perp \{X_3, X_4\} | X_2; \\
    X_3 \perp X_4 | X_2$
    \item $X_1 \perp \{X_3, X_4\} | X_2$ or $X_1 \perp X_4 | X_3; \\ 
    X_2 \perp X_4 | X_3$
    \item $X_1 \perp X_3 | X_2, X_4; \\ 
    X_2 \perp X_4 | X_1, X_3$
    \item $X_1 \perp \{X_4, X_5, X_6\} | X_2, X_3; \\ 
    X_2 \perp X_6 | X_3, X_5; \ X_3 \perp X_4 | X_2, X_5; \\ 
    X_4 \perp \{X_3,X_6\} | X_2, X_5; \\ 
    X_6 \perp \{X_2, X_5\} | X_3, X_5$
\end{enumerate}

\subsubsection{4}
\textit{Omitted }


    \iffalse 
        Consider $X_i \sim \text{Bernoulli}(p_i), i \in [3]$. Define $\Psymb(X_i=1, X_j=1) = p_{ij}$, noting that $X_i \perp X_j \Rightarrow p_{ij} = p_i p_j$. Under $H_0: p_{12|3} = p_{1|3}p_{2|3}$ and under $H_A: p_{12|3} \not=  p_{1|3}p_{2|3}$. 
        
        \begin{align*}
            \lambda 
            &= 
            \frac{
            \Psymb(X_1 = x_1 | X_3)
            \Psymb(X_2 = x_2 | X_3)
            }{
            \Psymb(X_1 = x_1, X_2 = x_2 | X_3)
            }
            \\
            &=
            \frac{
            p_{1|3}^{x_1} (1-p_{1|3})^{1-x_1}
            p_{2|3}^{x_2} (1-p_{2|3})^{1-x_2}
            }{
            asdf
            }
        \end{align*}
    \fi 

\subsubsection{5}
\textit{Omitted}

\end{document}
